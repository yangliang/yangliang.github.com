
## Graphon based Clustering and Testing of Networks: Algorithms and Theory

### TL;DR

None

### Abstract

Network-valued data are encountered in a wide range of applications, and pose challenges in learning due to their complex structure and absence of vertex correspondence. Typical examples of such problems include classification or grouping of protein structures and social networks. Various methods, ranging from graph kernels to graph neural networks, have been proposed that achieve some success in graph classification problems. However, there is limited theoretical justification for most methods, and their applicability beyond classification remains unexplored.

In this work, we propose methods for clustering multiple graphs, without vertex correspondence, that are inspired by the recent literature on estimating graphons---symmetric functions corresponding to infinite vertex limit of graphs. We propose a novel graph distance based on sorting-and-smoothing graphon estimators. We present two clustering algorithms using the proposed graph distance, and show that they achieve state-of-the-art results. Under Lipschitz assumptions on the graph degrees, we prove the statistical consistency of both algorithms. We further study the applicability of the proposed distance for graph two-sample testing problems. 

## Learning Graphon Autoencoders for Generative Graph Modeling

### TL;DR

None

### Abstract

Graphon is a nonparametric model that generates graphs with arbitrary sizes and can be induced from graphs easily. Based on this model, we propose a novel algorithmic framework called graphon autoencoder to build an interpretable and scalable graph generative model. This framework treats observed graphs as induced graphons in functional space and derives their latent representations by an encoder that aggregates Chebshev graphon filters. A linear graphon factorization model works as a decoder, leveraging the latent representations to reconstruct the induced graphons (and the corresponding observed graphs). We develop an efficient learning algorithm to learn the encoder and the decoder, minimizing the Wasserstein distance between the model and data distributions. This algorithm takes the KL divergence of the graph distributions conditioned on different graphons as the underlying distance and leads to a reward-augmented maximum likelihood estimation. The graphon autoencoder provides a new paradigm to represent and generate graphs, which has good generalizability and transferability.

## Contrastive Laplacian Eigenmaps

### TL;DR

The Laplacian eigenmaps are reformulated with a contrastive objective, which is compatible with various GNN backbones.

### Abstract

Graph contrastive learning attracts/disperses node representations for similar/dissimilar sample pairs under some notion of similarity, resulting from intrinsic properties of data grouping, and it may be combined with a low-dimensional embedding of nodes to preserve intrinsic and structural properties of a graph. In this paper, we extend the celebrated Laplacian Eigenmaps with contrastive learning, and call them  COntrastive Laplacian EigenmapS (COLES). Starting from a GAN-inspired contrastive formulation, we show the Jensen-Shannon divergence, which underlies many contrastive graph embeddings, fails under disjoint positive and negative distributions that naturally emerge during contrastive sampling. In contrast, we demonstrate analytically that COLES
 essentially minimizes a surrogate of Wasserstein distance, and the Wasserstein distance is known to cope well under disjoint distributions. Moreover, we show that the loss of COLES belongs to the family of so-called block-contrastive losses, previously shown to be superior compared to pair-wise losses typically used by contrastive methods. We show on popular benchmarks/backbones that COLES yields  favourable accuracy/scalability compared to DeepWalk, GCN, Graph2Gauss, DGI and GRACE baselines.

## Graph Symbiosis Learning

### TL;DR

We introduce a framework for learning from multiple generated graph views, named graph symbiosis learning (GraphSym).

### Abstract

We introduce a framework for learning from multiple generated graph views, named graph symbiosis learning~(GraphSym). In GraphSym, graph neural networks~(GNN) developed in multiple generated graph views can adaptively exchange parameters with each other and fuse information stored in linkage structures and node features. Specifically, we propose a novel adaptive exchange method to iteratively substitute redundant channels in the weight matrix of one GNN with informative channels of another GNN in a layer-by-layer manner. GraphSym does not rely on specific methods to generate multiple graph views and GNN architectures. Thus, existing GNNs can be seamlessly integrated into our framework. On 3 semi-supervised node classification datasets, GraphSym outperforms previous single-graph and multiple-graph GNNs without knowledge distillation, and achieves new state-of-the-art results. We also conduct a series of experiments on 15 public benchmarks, 8 popular GNN models, and 3 graph tasks---node classification, graph classification, and edge prediction---and show that GraphSym consistently achieves better performance than existing popular GNNs by 1.9\%$\sim$3.9\% on average and their ensembles. Extensive ablation studies and experiments on the few-shot setting also demonstrate the effectiveness of GraphSym.

## How Graph Neural Networks Go Beyond Weisfeiler-Lehman?

### TL;DR

We propose a novel GNN aggregation method, called Graph Structured Network (GraphSN), which introduces a new structural neighborhood aggregation scheme.

### Abstract

We propose a novel graph representation learning framework, called Structured-Message-Passing (SMP) framework, which enables a general solution to inject structural properties of graphs into a message-passing aggregation scheme of Graph Neural Networks (GNNs). We elaborate a theoretical basis for designing GNNs to go beyond the expressive power of the Weisfeiler Lehman test in the SMP framework. To demonstrate an instance of such GNNs, we  propose a novel neural model, called Graph Structured Network (GraphSN), and prove that GraphSN is strictly more expressive than the Weisfeiler Lehman test in distinguishing graph structures. We have evaluated our models using three benchmark tasks: (1)  node classification on graphs with standard and random splits, (2) graph classification, and (3) oversmoothing analysis w.r.t. the model depth. The experimental results show that our proposed models are efficient and can significantly improve the state-of-the-art methods without sacrificing computational simplicity.

## Hypergraph Propagation and Community Selection for Objects Retrieval

### TL;DR

We propose a hypergraph model to settle the ambiguity problem of propagation, and we use the graph information to reduce the spatial verification cost.

### Abstract

Spatial verification is a crucial technique for particular object retrieval. It utilizes spatial information for the accurate detection of true positive images. However, existing query expansion and diffusion methods cannot efficiently propagate the spatial information in an ordinary graph with scalar edge weights, resulting in low recall or precision. To tackle these problems, we propose a novel hypergraph-based framework that efficiently propagates spatial information in query time and retrieves an object in the database accurately. Additionally, we propose using the image graph's structure information through community selection technique, to measure the accuracy of the initial search result and to provide correct starting points for hypergraph propagation without heavy spatial verification computations. Experiment results on ROxford and RParis show that our method  significantly outperforms the existing query expansion and diffusion methods.

## Graph2Graph Learning with Conditional Auto-regressive Models

### TL;DR

None

### Abstract

We present a graph neural network model for solving graph-to-graph learning problems. Most deep learning on graphs considers ``simple'' problems such as graph classification or regressing real-valued graph properties. For such tasks, the main requirement for intermediate representations of the data is to maintain the structure needed for output, i.e.~keeping classes separated or maintaining the order indicated by the regressor. However, a number of learning tasks, such as regressing graph-valued output, generative models, or graph autoencoders, aim to predict a graph-structured output. In order to successfully do this, the learned representations need to preserve far more structure. We present a conditional auto-regressive model for graph-to-graph learning and illustrate its representational capabilities via experiments on challenging subgraph predictions from graph algorithmics as well as a graph autoencoder.


## Value Decomposition in Multi-Agent Reinforcement Learning via Graph Neural Networks

### TL;DR

We present a multi-agent reinforcement learning algorithm, called GraphMIX, with centralized training and decentralized execution, that uses value decomposition leveraging graph neural network architectures.

### Abstract

We propose a novel framework for value function factorization in multi-agent deep reinforcement learning (MARL) using graph neural networks (GNNs). In particular, we consider the team of agents as the set of nodes of a graph, whose edge weights are governed by an attention mechanism. We then introduce a mixing GNN module, which is responsible for factorizing the team state-action value function into individual per-agent observation-action value functions, as well as explicit credit assignment to each agent in terms of fractions of the global team reward. Our approach, which we call GraphMIX, follows the centralized training and decentralized execution paradigm, enabling the agents to make their decisions independently once training is completed. We show the superiority of GraphMIX as compared to the state-of-the-art on several scenarios in the StarCraft II multi-agent challenge (SMAC) benchmark. We further demonstrate how GraphMIX can be used to enable fine-tuning agents on mismatched test scenarios with higher numbers of agents and/or actions as compared to the training scenarios.

## Shift-Robust GNNs: Overcoming the Limitations of Localized Graph Training data

### TL;DR

In semi-supervised graph learning, gathering labels uniformly at random can be a great challenge. We present Shift-Robust GNN to account for distributional differences between biased training data and the graph's true inference distribution.

### Abstract

There has been a recent surge of interest in designing Graph Neural Networks (GNNs) for semi-supervised learning tasks. Unfortunately this work has assumed that the nodes labeled for use in training were selected uniformly at random (i.e. are an IID sample). However in many real world scenarios gathering labels for graph nodes is both expensive and inherently biased -- so this assumption can not be met. GNNs can suffer poor generalization when this occurs, by overfitting to superfluous regularities present in the training data. In this work we present a method, Shift-Robust GNN (SR-GNN), designed to account for distributional differences between biased training data and the graph's true inference distribution. SR-GNN adapts GNN models for the presence of distributional shifts between the nodes which have had labels provided for training and the rest of the dataset. We illustrate the effectiveness of SR-GNN in a variety of experiments with biased training datasets on common GNN benchmark datasets for semi-supervised learning, where we see that SR-GNN outperforms other GNN baselines by accuracy, eliminating at least (~40%) of the negative effects introduced by biased training data. On the largest dataset we consider, ogb-arxiv, we observe an 2% absolute improvement over the baseline and reduce 30% of the negative effects.

## Do Transformers Really Perform Bad for Graph Representation?

### TL;DR

We have explored the direct application of Transformers to graph representation. With three simple, yet effective graph structural encodings, the proposed GraphFormer works surprisingly well on a wide range of popular benchmark datasets.

### Abstract

The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting GraphFormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help GraphFormer better model graph-structured data. Besides, we mathematically characterize the expressive power of GraphFormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of GraphFormer. The code and models of GraphFormer will be made publicly available at \url{https://github.com/anonymous/anonymous}.

## GraphR: Differentiable Learning of Graph-like Rules on Knowledge Graphs

### TL;DR

We take the first step to learn graph-like rules, by designing a model, GraphR, which can learn the graph structure and relation assignments simultaneously, while most existing works can only learn chain-like rules.

### Abstract

Learning rules on a knowledge graph (KG) is essential for reasoning, logic programming, and theorem proving. However, existing works can only learn simple, i.e., chain-like rules, and cannot capture KG’s complex semantics which can be better captured by graph-like rules. Besides, learning graph-like rules is very difficult because they form a huge discrete space, including the graph structure of rule and relation assignments. On one hand, for learning relation assignments, observing that the plausibility of rules can be explained by the occurrence frequency in a KG, we generalize the score from chain-like rules to graph-like ones. On the other hand, considering the uncertainty of the graph structure, we propose to utilize two auxiliary relations to learn the rule structure. Furthermore, to relax the discrete space into a continuous one, we propose a differentiable parameterization for the score, which allows us to learn graph-like rules in an end-to-end differentiable training manner. We conduct extensive experiments on real-world datasets to show that our method outperforms existing works due to the better expressive ability for rules. Furthermore, we demonstrate that our method can learn high-quality graph-like rules.

## Towards building a robust large-scale Bangla text recognition solution using a unique multiple-domain character-based document recognition approach

### TL;DR

Towards robust large-scale Bangla OCR - A unique multiple-domain character-based OCR solution

### Abstract

Bangla is one of the world's top ten popular languages in terms of the number of speakers. It also happens to have a complex script primarily because of the presence of complex characters e.g... graphemes, that are composed of multiple single characters, and the characteristic shorthands e.g. vowel diacritics, and consonant diacritics, making the number of classes of this script recognition quite large, varied, and challenging. In this paper, we present a unique large-scale Bangla document OCR solution based on character-level recognition modules. We have tested our approach on two independent domains: printed and handwritten documents. We also applied our solution to three sub-domains within the printed domain: computer-composed documents, letterpress documents, and typewritten documents. Our extensive experiments show that our approach achieves state-of-the-art performance on handwritten and printed documents.

## Increase and Conquer: Training Graph Neural Networks on Growing Graphs

### TL;DR

In this paper we show that: (i) the expected distance between the learning steps on the graph neural network and on the graphon neural network decreases asymptotically with the size of the graph, allowing to learn the limit object architecture.

### Abstract

Graph neural networks (GNNs) use graph convolutions to exploit network invariances and learn meaningful features from network data. However, on large-scale graphs convolutions incur in high computational cost, leading to scalability limitations. Leveraging the graphon --- the limit object of a graph --- in this paper we consider the problem of learning a graphon neural network (WNN) --- the limit object of a GNN --- by training GNNs on graphs sampled Bernoulli from the graphon. Under smoothness conditions, we show that: (i) the expected distance between the learning steps on the GNN and on the WNN decreases asymptotically with the size of the graph, and (ii) when training on a sequence of growing graphs, gradient descent follows the learning direction of the WNN. Inspired by these results, we propose a novel algorithm to learn GNNs on large-scale graphs that, starting from a moderate number of nodes, successively increases the size of the graph during training. This algorithm is benchmarked on both a recommendation system and a decentralized control problem where it is shown to retain comparable performance, to its large-scale counterpart, at a reduced computational cost.

## Residual2Vec: Debiasing graph embedding with random graphs

### TL;DR

We propose residual2vec, a general graph embedding method that can debias specified structural biases in graphs by using random graphs.

### Abstract

Graph embedding maps a graph into a convenient vector-space representation for graph analysis and machine learning applications. Many graph embedding methods hinge on a sampling of context nodes based on random walks. However, random walks can be a biased sampler due to the structural properties of graphs. Most notably, random walks are biased by the degree of each node, where, at each step, a node is sampled proportionally to its degree. The implication of such biases has not been clear, particularly in the context of graph representation learning. Here, we investigate the impact of the random walks' bias on graph embedding and propose residual2vec, a general graph embedding method that can debias various structural biases in graphs by using random graphs. We demonstrate that this debiasing not only improves link prediction and clustering performance but also allows us to explicitly model salient structural properties in graph embedding.

## You Only Look at Text: A CNN-Free Object Detector for Vector Graphics

### TL;DR

We propose a CNN-Free object detector for Vector Graphics that directly looks at the textual document, without rendering it into pixels. 

### Abstract

In this paper, we consider a different data format for images: vector graphics. In contrast to raster graphics which are widely used in image recognition, vector graphics can be scaled up or down into any resolution without aliasing or information loss, due to the analytic representation of the primitives in the document. Furthermore, vector graphics are able to give extra structural information on how low-level elements group together to form high level shapes or structures. These merits of graphic vectors have not been fully leveraged in existing methods.  To explore this data format, we target on the fundamental recognition tasks: object localization and classification. We propose an efficient CNN-free pipeline that only looks at the textual document of the vector graphics (YOLaT), without rendering the graphic into pixels.  YOLaT builds multi-graphs to model the structural and spatial information in vector graphics, and a dual-stream graph neural network is proposed to detect objects from the graph. Our experiments show that by directly operating on vector graphics, YOLaT outperforms raster-graphic based object detection baselines in terms of both average precision and efficiency.

## Structural Graph Transformers

### TL;DR

Encoding graph structure is a key to improve transformer architectures for graphs

### Abstract

 We show that viewing graphs as sets of node features and incorporating structural and positional information into a transformer architecture is able to outperform representations learned with classical graph neural networks (GNNs). Our model, which we call structural graph transformer, encodes such information by (i) leveraging relative positional encoding strategies in self-attention scores based on positive definite kernels on graphs, and (ii) enumerating and encoding local sub-structures such as paths of short length. We thoroughly evaluate these two ideas on many classification and regression tasks, demonstrating the effectiveness of each of them independently, as well as their combination. In addition to performing well on standard benchmarks, our model also admits natural visualization mechanisms for interpreting graph motifs explaining the predictions, making it a potentially strong candidate for scientific applications where interpretation is important.

## Imagine the Imaginary: Bicomplex Knowledge Graph Embeddings

### TL;DR

Knowledge Graph Embeddings via bicomplex numbers.

### Abstract

In order to encapsulate a Knowledge Graph's structure there has been a plethora of work dealing with an adequate representation in real vector space enabling to capture as much information as possible in a vectorial representation of entities and relations. This work makes a substantial step in encoding the representation in hypercomplex spaces and leaves therefore the realm of traditional real valued vector spaces. We see that the algebraic structures of the set of bicomplex numbers enables to encode information and also opens the door for exploring holomorphic functions within the scope of Knowledge Graph Analysis (KGA).  
Bicomplex numbers have been shown to describe a four-dimensional algebra, containing $\mathbb{C}$ as a subalgebra that preserves commutativity by introducing two imaginary units $i$,$j$ with $k=ij$ (as in the quaternionic case) but additionally imposing the constrain that $ij=ji$ which makes $k$ a hyperbolic imaginary unit. We show that this algebraic structure can be used within a translational-based model. A multiplication of a and entity's representation as a bicomplex number by the trigonometric representation of a second bicomplex number means a rotation by a complex angle around the origin of the hypercomplex plane. Compared to other models this approach enables an expressive rotation in four-dimensional space having more degree of freedom compared to a rotation in complex plane. Experimental results on four real world benchmark datasets demonstrate the capabilities of using bicomplex numbers and we provide an outlook on how this representation forge a bridge to other space like the Minkowsi space.

## Iterative Connecting Probability Estimation for Networks

### TL;DR

None

### Abstract

Estimating the probabilities of connections between vertices in a random network using the observed adjacency matrix is an important task for network data analysis. Many existing estimation methods are based on certain assumptions on network structures, which limits their applicability in practice. Without making strong assumptions, we develop an iterative connecting probability estimation method based on neighborhood averaging. Starting at a random initial point or an existing estimate, our method iteratively updates the pairwise vertex distances, the neighborhood sets, and connecting probabilities to improve the precision of the estimate. We propose a two-stage neighborhood selection procedure to achieve the trade-off between smoothness of the estimate and the ability to discover local structure. The tuning parameters can be selected by cross-validation. We establish desirable theoretical properties for our method, and further justifies its superior performance by comparing with existing methods in simulation and real data analysis.

## Generalization of graph network inferences in higher-order probabilistic graphical models

### TL;DR

None

### Abstract

Probabilistic graphical models provide a powerful tool to describe complex statistical structure, with many real-world applications in science and engineering from controlling robotic arms to understanding neuronal computations. A major challenge for these graphical models is that inferences such as marginalization are intractable for general graphs. These inferences are often approximated by a distributed message-passing algorithm such as Belief Propagation, which does not always perform well on graphs with cycles, nor can it always be easily specified for complex continuous probability distributions. Such difficulties arise frequently in expressive graphical models that include intractable higher-order interactions. In this paper we construct iterative message-passing algorithms using Graph Neural Networks defined on factor graphs to achieve fast approximate inference on graphical models that involve many-variable interactions. Experimental results on several families of graphical models demonstrate the out-of-distribution generalization capability of our method to different sized graphs, and indicate the domain in which our method gains advantage over Belief Propagation.

## Knowledge Graph Embeddings with Switch Spaces

### TL;DR

None

### Abstract

Aligning the geometric inductive bias well with the underlying structure of data is critical for representation learning of knowledge graphs (KGs). To achieve this goal, we propose \textit{switch spaces}, a data-driven representation learning approach, and apply it to KG embeddings learning.  Switch space is a generalization of product space which is composed of multiple euclidean and non-euclidean (e.g., hyperbolic, spherical) spaces.  Given $N$ spaces, our model utilizes a sparse gating mechanism to let each input data point choose $K (K<N)$ spaces and combines them to form a product space. In doing so, ${}_{N}C_K$ product spaces are generated automatically in a single model and each input data point is processed by one of them with greater specialization. As such, the spaces are switchable. In addition, switch space models are efficient and have a constant computational complexity regardless of the model size. We apply switch space to the KG completion task and propose \textit{SwisE} which enables the triples of KGs to specify whatever spaces they favor. Experiments on benchmarking KG datasets demonstrate that SwisE achieves state-of-the-art performances, outperforming a range of recently proposed models by a large margin. 

## Encoding Feature Set Information in Heterogeneous Graph Neural Networks

### TL;DR

Feature set based feature transformation approaches improve the outcomes of SOTA heterogeneous graph neural networks in not-disjoint feature datasets

### Abstract

Graphs are ubiquitous data structures that have become increasingly popular for Representation Learning in recent years. Following the advent of Graph Neural Networks (GNN), recent studies have pointed limitations and proposed solutions on how such architectures leverage structural information during the learning process. On the other hand, even though essential for most tasks, the feature sets associated with each node or edge of the graph are often overlooked. In heterogeneous graphs, where nodes or edges might contain different feature sets, solutions usually rely on simple approaches such as projecting each type of node to the same n-dimensional space. This paper presents a novel strategy for Heterogeneous Graph Representation Learning by tackling node attributes as feature sets and leveraging their intersection across node types. We conduct experiments on two heterogeneous datasets with a diverse set of distinct features, namely the KDD Cup 2014 DonorsChoose and a new dataset gathered from the Smoke Squadron game. Our results point out that encoding feature set information in the representation learning process improves the outcomes of state-of-the-art models in not-disjoint feature datasets.

## MagNet: A Neural Network for Directed Graphs

### TL;DR

We describe how to build graph neural networks for directed graphs using complex Hermitian matrices. 

### Abstract

The prevalence of graph-based data has spurred the rapid development of graph neural networks (GNNs) and related machine learning algorithms. Yet, despite the many datasets naturally modeled as directed graphs, including citation, website, and traffic networks, the vast majority of this research focuses on undirected graphs. In this paper, we propose MagNet, a spectral GNN for directed graphs based on a complex Hermitian matrix known as the magnetic Laplacian. This matrix encodes undirected geometric structure in the magnitude of its entries and directional information in their phase. A "charge" parameter attunes spectral information to variation among directed cycles. We apply our network to a variety of directed graph node classification and link prediction tasks showing that MagNet  performs well on all tasks and that its performance exceeds all other methods on a  majority of such tasks. The underlying principles of MagNet are such that it can be adapted to other spectral GNN architectures.

## Optimizing Graph Transformer Networks with Graph-based Techniques

### TL;DR

We optimize memory and compute costs for graph transformer networks with subpath finding and sampling.

### Abstract

Graph transformer networks (GTN) are a variant of graph convolutional networks
(GCN) that use type information on the nodes and edges of a heterogeneous graph.
GTNs learn important metapaths in the graph, create weighted edges for these
metapaths, and use the resulting graph in a GCN. Currently, the only available
implementation of GTNs uses dense matrix multiplication to find metapaths.
Unfortunately, the space overhead of this approach grows proportionately with
the size of the graph, so it can be used only for small graphs. In addition, the
matrix-based implementation is not fine-grained enough to use random-walk based
methods to optimize metapath finding. In this paper, we present a
graph-based formulation and implementation of the GTN metapath finding
problem. This graph-based formulation has two advantages over the matrix-based
approach.  First, it is more space efficient than the original GTN
implementation and more compute-efficient for metapath sizes of practical
interest. Second, it permits us to implement a sampling method that
reduces the number of metapaths that must be enumerated, allowing the
implementation to be used for larger graphs and larger metapath sizes.
Experimental results show that our implementation is $6.5\times$ faster than the
original GTN implementation on average for a metapath length of 4, and our
sampling implementation is $155\times$ faster on average than this
implementation without compromising on the accuracy of the GTN. 

## ODG-GNN: Out-of-Distribution Generalized Graph Representation Learning

### TL;DR

We propose an Out-of-Distribution Generalized Graph Neural Network (ODG-GNN) for achieving satisfactory performance on unseen testing graphs that have different distributions with training graphs.

### Abstract

Graph neural networks (GNNs) have achieved impressive performance when testing and training graph data come from identical distribution. However, existing GNNs lack out-of-distribution generalization abilities so that their performance substantially degrades when there exist distribution shifts between testing and training graph data. To solve this problem, in this work, we propose an Out-of-Distribution Generalized Graph Neural Network (ODG-GNN) for achieving satisfactory performance on unseen testing graphs that have different distributions with training graphs. Our proposed ODG-GNN employs a novel nonlinear graph representation decorrelation method utilizing random Fourier features, which encourages to eliminate the statistical dependence between relevant and irrelevant graph representations through iteratively optimizing the sample graph weights and graph encoder. We further design a global weight estimator to learn weights for training graphs such that variables in graph representations are forced to be independent. The learned weights help the graph encoder to get rid of spurious correlations and, in turn, concentrate more on the true connection between learned discriminative graph representations and their ground-truth labels. We conduct extensive experiments to validate the out-of-distribution generalization abilities on 2 synthetic and 15 real-world datasets with distribution shifts. The results demonstrate that our proposed ODG-GNN is able to significantly outperform several state-of-the-art baselines.

## Structural balance in sample correlation graphs leads to contraction upon conditioning in graphical models

### TL;DR

A graphical condition is given for checking when a correlation contracts upon conditioning in graphical models

### Abstract

When data are available for all nodes of a Gaussian graphical model, then it is possible to use sample correlations and partial correlations to test to what extent the conditional independencies that encode the structure of the model are indeed verified by the data. In the paper we give a heuristic rule useful in such validation process: when the correlation subgraph involved in a conditional independence is structurally balanced (i.e., all its cycles have an even number of negative edges), then a partial correlation is usually a contraction of the corresponding correlation, which often leads to conditional independence. In particular, the contraction rule can be made rigorous if we look at concentration subgraphs, rather than correlation subgraphs. The rule is applied to synthetic data as well as to real data for elementary gene regulatory motifs. 

## Sublinear-Time Clustering Oracle for Signed Graphs

### TL;DR

We show that short random walks can be used to detect clusters in signed graphs.

### Abstract

Social networks are often modeled using signed graphs, where vertices
correspond to users and edges have a sign that indicates whether an interaction
between users was positive or negative.  The arising signed graphs typically
contain a clear community structure in the sense that the graph can be
partitioned into a small number of polarized communities, each defining a
sparse cut and indivisible into smaller polarized sub-communities.  We provide a
local clustering oracle for signed graphs with such a clear community
structure, that can answer membership queries, i.e.,
	``Given a vertex $v$, which community does $v$ belong to?,
in sublinear time by reading only a small portion of the graph. Formally, with
$\tilde{O}(\sqrt{n}\operatorname{poly}(1/\varepsilon))$ preprocessing time, our oracle can
answer each membership query in $\tilde{O}(\sqrt{n}\operatorname{poly}(1/\varepsilon))$ time,
and it correctly classifies a $(1-\varepsilon)$-fraction of vertices w.r.t. a
set of hidden planted ground-truth communities. Our oracle is desirable in
applications where the clustering information is needed for only a small number
of vertices.  Previously, such local clustering oracles were only known for
*un*signed graphs; our generalization to signed graphs requires a number of
new ideas and gives a novel spectral analysis of the behavior of random
walks with signs.  We evaluate our algorithm for constructing such an oracle
and answering membership queries on both synthetic and real-world datasets,
validating its performance in practice.

## On the Universality of Graph Neural Networks on Large Random Graphs

### TL;DR

We prove universality theorems for "continuous" Graph Neural Networks arising in the large random graphs limit.

### Abstract

We study the approximation power of Graph Neural Networks (GNNs) on latent position random graphs. In the large graph limit, GNNs are known to converge to certain ``continuous'' models known as c-GNNs, which directly enables a study of their approximation power on random graph models. In the absence of input node features however, just as GNNs are limited by the Weisfeiler-Lehman isomorphism test, c-GNNs will be severely limited on simple random graph models. For instance, they will fail to distinguish the communities of a well-separated Stochastic Block Model (SBM) with constant degree function. Thus, we consider recently proposed architectures that augment GNNs with unique node identifiers, referred to as Structural GNNs here (SGNNs). We study the convergence of SGNNs to their continuous counterpart (c-SGNNs) in the large random graph limit, under new conditions on the node identifiers. We then show that c-SGNNs are strictly more powerful than c-GNNs in the continuous limit, and prove their universality on several random graph models of interest, including most SBMs and a large class of random geometric graphs. Our results cover both permutation-invariant and permutation-equivariant architectures.

## Wasserstein diffusion on graphs with missing attributes

### TL;DR

We propose a new graph representation method based on optimal transport for graphs with missing attributes.

### Abstract

Missing node attributes is a common problem in real-world graphs. Graph neural networks have been demonstrated powerful in graph representation learning, however, they rely heavily on the completeness of graph information. Few of them consider the incomplete node attributes, which can bring great damage to the performance in practice. In this paper, we propose an innovative node representation learning framework, Wasserstein graph diffusion (WGD), to mitigate the problem. Instead of feature imputation, our method directly learns node representations from the missing-attribute graphs. Specifically, we extend the message passing schema in general graph neural networks to a Wasserstein space derived from the decomposition of attribute matrices. We test WGD in node classification tasks under two settings: missing whole attributes on some nodes and missing only partial attributes on all nodes. In addition, we find WGD is suitable to recover missing values and adapt it to tackle matrix completion problems with graphs of users and items. Experimental results on both tasks demonstrate the superiority of our method.

## GraphLINT: Graph Local Clustering Based Inductive Learning Method

### TL;DR

We propose GraphLINT, an inductive learning method that utilizes a PPR-based local clustering algorithm to efﬁciently search for small but compact subgraphs for GNN training and inference.

### Abstract

Graph Neural Networks (GNNs), which benefit various real-world problems and applications, have emerged as a powerful technique for learning graph representations. In this work, we establish a theoretical connection between GNNs and local clustering theory, showing that graph propagation in GNNs have a high probability to be stuck at a local cluster. Two common strategies for scalable GNN learning, graph sampling and personalized PageRank (PPR), can be viewed as an implicit version of finding local clusters. Inspired by the above facts, we propose GraphLINT, an inductive learning method that utilizes a PPR-based local clustering algorithm, PPR-Nibble, to efficiently search for small but compact subgraphs for GNN training and inference only on the extracted local clusters. Compared to full-batch GNNs, graph sampling-based GNNs, graph partition-based GNNs and PPR-based GNNs, GraphLINT achieves state-of-the-art results on four inductive datasets with different scales, which cover extensive downstream applications.

## Learning to Infer the Structure of Network Games

### TL;DR

None

### Abstract

Strategic interactions between a group of individuals or organisations can be modelled as games played on networks, where a player's payoff depends not only on their actions but also on those of their neighbors. 
Inferring the network structure from observed game outcomes (equilibrium actions) is an important problem with numerous potential applications in economics and social sciences. 
Currently available methods require the knowledge of the utility function associated with the game, which is often unrealistic to obtain in real-world scenarios. To address this limitation, we propose a novel transformer-like architecture which correctly accounts for the symmetries of the problem and learns a mapping from the equilibrium actions to the network structure of the game without explicit knowledge of the utility function. We test our method on three different types of network games using both synthetic and real-world data, and demonstrate its effectiveness in network structure inference and superior performance over existing methods. 

## Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods

### TL;DR

We propose new datasets and a strong simple method for scalable learning on non-homophilous graphs.

### Abstract

Many widely used datasets for graph machine learning tasks have generally been homophilous, where nodes with similar labels connect to each other. Recently, new Graph Neural Networks (GNNs) have been developed that move beyond the homophily regime; however, their evaluation has often been conducted on small graphs with limited application domains. We collect and introduce diverse non-homophilous datasets from a variety of application areas that have up to 384x more nodes and 1398x more edges than prior datasets. We further show that existing scalable graph learning and graph minibatching techniques lead to performance degradation on these non-homophilous datasets, thus highlighting the need for further work on scalable non-homophilous methods. To address these concerns, we introduce LINKX --- a strong simple method that admits straightforward minibatch training and inference. Extensive experimental results with representative simple methods and GNNs across our proposed datasets show that LINKX achieves state-of-the-art performance for learning on non-homophilous graphs.

## Variational Flow Graphical Model

### TL;DR

None

### Abstract

This paper introduces a novel approach to embed flow-based models with hierarchical structures. The proposed model learns latent representations of high dimensional data via a message-passing scheme by carefully integrating normalizing flows in variational graphs. Meanwhile, the model can generate data representations with reduced latent dimensions, thus overcoming the drawbacks of many flow-based models, usually requiring a high dimensional latent space involving many trivial variables. With aggregation nodes, the model provides a convenient approach for data integration and graphical inference.  Theoretical analysis and numerical experiments on synthetic and real datasets show the benefits and broad potentials of our~proposed~method.

## End-to-End Learning of Probabilistic Hierarchies on Graphs

### TL;DR

End-to-end optimization of hierarchical clustering objectives learns high-quality hierarchies.

### Abstract

Real-world graphs exhibit rich hierarchical structure --- for example, in social networks, we have fine-grained communities in the form of families or friends, up to large-scale communities such as regions or nations. 
In this work, we propose a novel probabilistic model over hierarchies on graphs obtained by continuous relaxation of tree-based hierarchies. We draw connections to Markov chain theory, enabling us to learn a hierarchy by efficient end-to-end optimization of of quality metrics such as Dasgupta cost or Tree-Sampling Divergence (TSD). We show that our model learns rich, high-quality hierarchies present in 8 real world graphs, including a large  graph with 2.3M nodes. Our model consistently outperforms recent as well as strong traditional baselines such as average linkage. Further, we demonstrate that our model also obtains competitive results on link prediction despite not being trained on this task. 

## Knowledge Hypergraph Embedding Meets Relational Algebra

### TL;DR

None

### Abstract

Embedding-based methods for reasoning in knowledge hypergraphs learn a representation for each entity and relation and use them to make predictions. Most of the existing methods do not capture the procedural rules underlying the relations in the graph. We propose an embedding-based model called Relational Algebra Embedding (ReAlE) that performs link prediction in knowledge hypergraphs (generalized knowledge graphs) and can capture high-level abstractions in terms of relational algebra operations. We show theoretically that ReAlE is fully expressive and provide proofs and empirical evidence that it can represent a large subset of the primitive relational algebra operations, namely renaming, projection, set union, selection, and set difference. We also verify experimentally that ReAlE outperforms state-of-the-art models in knowledge hypergraph completion, and in representing each of these primitive relational algebra operations. For the latter experiment, we generate a synthetic knowledge hypergraph, for which we design an algorithm based on the Erdos-R'enyi model for generating random graphs.

## Multi-task Cascaded Model on Heterogeneous Data for End-to-End Character based Chinese Text-to-Speech Synthesis

### TL;DR

Multi-task Cascaded Model on Heterogeneous Data for End-to-End Character based Chinese Text-to-Speech Synthesis

### Abstract

End-to-end text-to-speech (TTS) systems have outperformed conventional TTS on various languages by learning a flexible mapping from linguistic to acoustic space. But for Chinese, it is quite complicated to directly learn such a mapping from characters
because of the complex linguistic structure and ambiguities in word tokenization, part-of-speech tagging, grapheme-to-phoneme (G2P) conversion and prosodic phrasing. Nowadays Chinese TTS systems generally train their models in a multi-stage manner with heterogeneous data that is specific to different sub-modules for the above tasks. However, the conventional scheme leaves every sub-module and its knowledge learned from the corresponding sub-dataset in isolation from each other, which consequently deters the further enhancement on overall performance and robustness of the hybrid system. We propose a multi-task cascaded (MTC) model for character-based Chinese TTS,  which consists of multiple cascaded sub-modules,  including syntactic tree-based linguistic feature learning, G2P with attention-based polyphone disambiguation, phonological changes prediction and speech synthesis. Results show that the proposed model achieves a better synthesis quality than character-based TTS, with all sub-modules outperforming their corresponding state-of-the-art approaches.

## Necessary and sufficient graphical conditions for optimal adjustment sets in causal graphical models with hidden variables

### TL;DR

The paper presents necessary and sufficient graphical conditions and an algorithm for optimal adjustment sets in causal graphical models with hidden variables.

### Abstract

The problem of selecting optimal backdoor adjustment sets to estimate causal effects in graphical models with hidden and conditioned variables is addressed. Previous work has defined optimality as achieving the smallest asymptotic estimation variance and derived an optimal set for the case without hidden variables. For the case with hidden variables there can be settings where no optimal set exists and currently only a sufficient graphical optimality criterion of limited applicability exists. Here optimality is characterized as maximizing a certain adjustment information which allows to derive a necessary and sufficient graphical criterion for the existence of an optimal adjustment set and a definition and algorithm to construct it. Further, the optimal set is valid if and only if a valid adjustment set exists and has higher (or equal) adjustment information than the Adjust-set proposed in Perkovi{\'c} et~al. [Journal of Machine Learning Research, 18: 1--62, 2018] for any graph. The results translate to minimal estimation variance for a class of estimators whose asymptotic variance follows a certain information-theoretic relation. Numerical experiments indicate that the asymptotic results also hold for relatively small sample sizes. Surprisingly, among the randomly created setups more than 80\% fulfill the optimality conditions indicating that also in many real-world scenarios graphical optimality may hold.

## Robustness of Graph Neural Networks at Scale

### TL;DR

We evaluate adversarial robustness/vulnerability of Graph Neural Networks on graphs up to three orders of magnitude larger than before.

### Abstract

Graph Neural Networks (GNNs) are increasingly important given their popularity and the diversity of applications. Yet, existing studies of their vulnerability to adversarial attacks rely on relatively small graphs. We address this gap and study how to attack and defend GNNs at scale. We propose two sparsity-aware first-order optimization attacks that maintain an efficient representation despite optimizing over a number of parameters which is quadratic in the number of nodes. We show that common surrogate losses are not well-suited for global attacks on GNNs. Our alternatives can double the attack strength. Moreover, to improve GNNs' reliability we design a robust aggregation function, Soft Median, resulting in an effective defense at all scales. We evaluate our attacks and defense with standard GNNs on graphs more than 100 times larger compared to previous work. We even scale one order of magnitude further by extending our techniques to a scalable GNN.

## Improved Gibbs Sampling Algorithms for Gaussian Graphical Models

### TL;DR

Exact and approximate inference in Gaussian graphical models

### Abstract

We consider the problems of exact and approximate inference in Gaussian graphical models.
Our approach is based on transforming the graphical model into a new domain by taking the Fourier transform of 
the local factors of the model. In the first part of the paper, 
we propose a method to solve exactly the Gaussian graphical model defined on the ladder graph 
if certain conditions on the local covariance matrices are satisfied. Unlike the conventional approaches,
the applicability of the method depends on the position of the zeros of the local covariance matrices, where 
after transforming the model into a cycle-free graphical models, the exact determinant of the covariance
matrix can be computed via the Gaussian belief propagation algorithm. In the second part, the problem of 
estimating the marginal densities of Gaussian Markov random fields is considered. 
Our numerical experiments show that, in some settings, the Gibbs sampling algorithm
can provide more accurate estimates of the marginal densities in the transformed graphical model.

## A Spectral Approach for Graph Topology Learning

### TL;DR

A spectral method for efficient learning of graph topologies from data

### Abstract

Graph learning plays an important role in many data mining and machine learning tasks, such as manifold learning, data representation and analysis, dimensionality reduction, data clustering, and visualization, etc. In this work, we introduce a highly-scalable spectral graph densification approach (GRASPEL) for graph topology learning from data. By limiting the precision matrix to be a graph-Laplacian-like matrix, our approach aims to learn sparse undirected graphs from potentially high-dimensional input data. A very unique property of the graphs learned by GRASPEL is that the spectral embedding (or approximate effective-resistance) distances on the graph will encode the similarities between the original input data points. By leveraging high-performance spectral methods, sparse yet spectrally-robust graphs can be learned by identifying and including the most spectrally-critical edges into the graph. Compared with prior state-of-the-art graph learning approaches, GRASPEL is more scalable and allows substantially improving computing efficiency and solution quality of a variety of data mining and machine learning applications, such as manifold learning, spectral clustering (SC), and dimensionality reduction (DR).

## Efficient Learning of Discrete-Continuous Computation Graphs

### TL;DR

We propose two new strategies to enable efficient learning of discrete-continuous computation graphs with multiple stochastic nodes where naïve approaches fail.

### Abstract

Numerous models for supervised, unsupervised, and reinforcement learning benefit from combining discrete and continuous components. The resulting discrete-continuous models are end-to-end learnable and are compositional, tend to generalize better, and are more interpretable. A popular approach to build discrete-continuous computation graphs is that of integrating discrete probability distributions into neural networks using stochastic softmax tricks. Prior work has focused on computation graphs with a single discrete component on each of the graph's execution paths. We analyze the behavior of more complex stochastic computations graphs with multiple sequential discrete components. We show that it is challenging to optimize the parameters of these models, mainly due to vanishing gradients. We then propose two new strategies to overcome these challenges. First, we propose dropout residual connections specifically tailored to discrete-continuous computation graphs. Second, we show that matching the temperature parameters of the Gumbel-Softmax and increasing the scale parameter of the Gumbel noise perturbations drastically improves the learning behavior. With an extensive set of experiments, we show that we can train complex discrete-continuous models which one cannot train with standard stochastic softmax tricks. We also show that using complex discrete-stochastic models allows us to generalize better than their continuous counterparts on several benchmark datasets.

## Relevance Proximity Graphs for Efficient Relevance Retrieval

### TL;DR

Propose Relevance Proximity Graphs (RPG) - an efficient method for relevance retrieval.

### Abstract

In many machine learning applications, the most relevant items for a particular query should be efficiently extracted, while the relevance function is based on a highly nonlinear model, e.g., DNN or GBDT. Due to the high computational complexity of such models, exhaustive search is infeasible even for medium-scale problems. To address this issue, we use best-first search over Relevance Proximity Graphs (RPG): an efficient non-exhaustive approach that provides a high-quality approximate solution for maximal relevance retrieval. Namely, we apply a graph-based similarity search framework to the setting when there is no similarity measure defined on item pairs, which is an important practical use case. By design, our approach directly maximizes off-the-shelf relevance functions and does not require any proxy auxiliary models. Via extensive experiments, we show that the developed method provides excellent retrieval accuracy while requiring only a few model computations, outperforming cascade approaches that use faster proxy measures to retrieve a list of candidates to be re-ranked. Importantly, we explain the success of the RPG via theoretical analysis and a series of synthetic experiments. We will open-source our implementation as well as two large-scale datasets to support further research on relevance retrieval.

## Directional Message Passing on Molecular Graphs via Synthetic Coordinates

### TL;DR

We propose synthetic coordinates based on personalized pagerank and atom distance bounds to enhance graph neural networks for molecules

### Abstract

  Graph neural networks that leverage coordinates via directional message passing have recently set the state of the art on multiple molecular property prediction tasks. However, they rely on atom position information that is often unavailable, and obtaining it is usually prohibitively expensive or even impossible. In this paper we propose synthetic coordinates that enable the use of advanced GNNs without requiring the true molecular configuration. We propose two distances as synthetic coordinates: Graph-based distances based on a symmetric variant of personalized PageRank, and distance bounds that specify the rough range of molecular configurations. To leverage both distance and angular information we propose a method of transforming normal graph neural networks into directional MPNNs. We show that with this transformation we can reduce the error of a normal graph neural network by 49% on ZINC. We furthermore set the state of the art on ZINC and coordinate-free QM9 by incorporating synthetic coordinates in the SMP and DimeNet$++ models.

## Capacity and Bias of Learned Geometric Embeddings for Directed Graphs

### TL;DR

We introduce a novel geometric embedding method for capturing graph structure, prove it's ability to represent any DAG, and empirically analyze the representational capacity and bias of a large set of geometric embeddings for graph modeling.

### Abstract

A wide variety of machine learning tasks such as knowledge base completion, ontology alignment, and multi-label classification can benefit from incorporating into learning differentiable representations of graphs or taxonomies.  While vectors in Euclidean space can theoretically represent any graph, much recent work shows that alternatives such as complex, hyperbolic, order, or box embeddings have geometric properties better suited to modeling real-world graphs. Experimentally these gains are seen only in lower dimensions, however, with performance benefits diminishing in higher dimensions. In this work, we introduce a novel variant of box embeddings that uses a learned smoothing parameter to achieve better representational capacity than vector models in low dimensions, while also avoiding performance saturation common to other geometric models in high dimensions. Further, we present theoretical results that prove box embeddings can represent any DAG. We perform rigorous empirical evaluations of vector, hyperbolic, and region-based geometric representations on several families of synthetic and real-world directed graphs. Analysis of these results exposes correlations between different families of graphs, graph characteristics, model size, and embedding geometry, providing useful insights into the inductive biases of various differentiable graph representations.

## VQ-GNN: A Universal Framework to Scale up Graph Neural Networks using Vector Quantization

### TL;DR

We propose a universal framework to scale most state-of-the-art Graph Neural Networks to large graphs using Vector Quantization.

### Abstract

  Most state-of-the-art Graph Neural Networks (GNNs) can be defined as a form of graph convolution which can be realized by message passing between direct neighbors or beyond. To scale such GNNs to large graphs, various neighbor-, layer-, or subgraph-sampling techniques are proposed to alleviate the "neighbor explosion" problem by considering only a small subset of messages passed to the nodes in a mini-batch. However, sampling-based methods are difficult to apply to GNNs that utilize many-hops-away or global context each layer, show unstable performance for different tasks and datasets, and do not speed up model inference. We propose a principled and fundamentally different approach, VQ-GNN, a universal framework to scale up any convolution-based GNNs using Vector Quantization (VQ) without compromising the performance. In contrast to sampling-based techniques, our approach can effectively preserve all the messages passed to a mini-batch of nodes by learning and updating a small number of quantized reference vectors of global node representations, using VQ within each GNN layer. Our framework avoids the "neighbor explosion" problem of GNNs using quantized representations combined with a low-rank version of the graph convolution matrix. We show that such a compact low-rank version of the gigantic convolution matrix is sufficient both theoretically and experimentally. In company with VQ, we design a novel approximated message passing algorithm and a nontrivial back-propagation rule for our framework. Experiments on various types of GNN backbones demonstrate the scalability and competitive performance of our framework on large-graph node classification and link prediction benchmarks.

## Ultrahyperbolic Neural Networks

### TL;DR

A (graph) neural network mapping representations to a pseudo-Riemannian manifold of constant curvature used to classify hierarchical graphs with cycles

### Abstract

Riemannian space forms, such as the Euclidean space, sphere and hyperbolic space, are popular and powerful representation spaces in machine learning. For instance, hyperbolic geometry is appropriate to represent graphs without cycles and has been used to extend Graph Neural Networks. Recently, pseudo-Riemannian space forms that generalize both hyperbolic and spherical geometries have been exploited to learn a specific type of (nonparametric) embedding called ultrahyperbolic. The lack of geodesic between every pair of ultrahyperbolic points makes the task of learning parametric models (e.g., neural networks) difficult. This paper introduces a method to learn parametric models in ultrahyperbolic space. We experimentally show the relevance of our approach in the tasks of graph and node classification. 

## A Topological Framework for Causal Discovery

### TL;DR

We propose a novel topological representation of graphical models, and explore its application to causal discovery

### Abstract

We propose a novel framework for causal discovery, building on the key relationship between  partially ordered sets (posets) and finite Alexandroff topologies. Alexandroff spaces yield a directional topological space: the topology is defined by a unique minimal basis defined by open sets $U_x$ for each variable $x$, specified as the intersection of all open sets containing $x$. Alexandroff spaces induce a (reflexive, transitive) preorder: a variable $x \leq y$ if  $x \in U_y$. Alexandroff spaces satisfying the Kolmogorov $T_0$ separation criterion, where open sets distinguish variables, converts the preordering into a partial ordering. Our approach broadly is to construct a topological representation of posets from data, and then use the poset representation to build a conventional DAG-oriented causal model. We illustrate our framework by showing how it unifies disparate algorithms and case studies proposed previously. Topology plays two key roles in causal discovery. First, topological separability constraints on datasets have been used in several previous approaches to infer causal structure from observations and interventions. Second, a diverse range of graphical models used to represent causal structures can be represented in a unified way in terms of a topological representation of the induced poset structure. We show that the homotopy theory of Alexandroff spaces can be exploited to significantly efficiently reduce the number of possible DAG structures, reducing the search space by several orders of magnitude. 

## SONG: Self-Organizing Neural Graphs

### TL;DR

We introduce Self-Organizing Neural Graphs (SONGs), a new paradigm of end-to-end decision graph training based on Markov processes that simultaneously learn the optimal graph structure and transition probabilities.

### Abstract

Recent years have seen a surge in research on deep interpretable neural networks with decision trees as one of the most commonly incorporated tools. There are at least three advantages of using decision trees over logistic regression classification models: they are easy to interpret since they are based on binary decisions, they can make decisions faster, and they provide a hierarchy of classes. However, one of the well-known drawbacks of decision trees, as compared to decision graphs, is that decision trees cannot reuse the decision nodes. Nevertheless, decision graphs were not commonly used in deep learning due to the lack of efficient gradient-based training techniques. In this paper, we fill this gap and provide a general paradigm based on Markov processes, which allows for efficient training of the special type of decision graphs, which we call Self-Organizing Neural Graphs (SONG). We provide an extensive theoretical study of SONG, complemented by experiments conducted on Letter, Connect4, MNIST, CIFAR, and TinyImageNet datasets, showing that our method performs on par or better than existing decision models.

## gLIME: A new graphical methodology for interpretable model-agnostic explanations 

### TL;DR

A new graphical methodology for interpretable model-agnostic explanations 

### Abstract

Explainable artificial intelligence (XAI) is an emerging new domain in which a set of processes and tools allow humans to better comprehend the outputs / decisions generated by black box models. However, most of the available XAI tools are often limited to simple explanations mainly quantifying the impact of individual features to the models’ output. Therefore, human users are not able to understand how the features are related to each other to make predictions, whereas the inner workings of the trained models remain hidden. This paper contributes to the development of a novel graphical explainability tool that not only indicates the significant features of the model, but also reveals the conditional relationships between features and the inference capturing both the direct and indirect impact of features to the models’ decision.  The proposed XAI methodology, termed as gLIME, can be applied to provide graphical model-agnostic explanations either at the global (for the entire dataset) or the local level (for specific data points). It relies on a combination of local interpretable model-agnostic explanations (LIME) with graphical least absolute shrinkage and selection operator (GLASSO) producing undirected Gaussian graphical models. Regularization is adopted to shrink small partial correlation coefficients to zero providing sparser and more interpretable graphical explanations. Two well-known classification datasets (BIOPSY and OAI) were selected to confirm the superiority of gLIME over LIME in terms of both robustness and consistency/sensitivity over multiple permutations. Specifically, gLIME accomplished increased stability over the two datasets with respect to features’ importance (76\%-96\% compared to 52\%-77\% using LIME). gLIME demonstrates a unique potential to further extend the functionality of the current state-of-the art in XAI by providing more informative graphically given explanations that could unlock black boxes. 

## Learning Explainable Templated Graphical Models

### TL;DR

We propose a new structure learning algorithm that learns a templated graphical model and an explanation framework for its predictions.

### Abstract

Templated graphical models (TGMs) encode model structure using rules that capture the relationships between multiple random variables. While the rules in TGMs are interpretable, it is unclear how they can be used to generate explanations for the individual predictions of the model. Further, learning these rules from data comes with high computational costs: it typically requires an expensive combinatorial search over the space of rules and repeated optimization over rule weights. In this work, we propose a new structure learning algorithm, Explainable Structured Model Search (ESMS), that learns a templated graphical model and an explanation framework for its predictions. ESMS uses a novel search procedure to efficiently search the space of models and discover models that trade-off predictive accuracy and explainability. We introduce the notion of \textit{relational stability} and prove that our proposed explanation framework is stable. Further, our proposed piecewise pseudolikelihood (PPLL) objective does not require re-optimizing the rule weights across models during the search. In our empirical evaluation on two realworld datasets, we show that our proposed approach not only discovers models that are explainable, but also significantly outperforms existing state-out-the-art structure learning approaches.

## Graph Structural Attack by Spectral Distance

### TL;DR

We propose a structural attack in the graph spectral domain by maximizing the spectral distance between the original and the perturbed graphs.

### Abstract

Graph Neural Networks (GNNs) have advanced the state-of-the-art on many graph mining tasks, but are also shown vulnerable to adversarial attacks. In this paper, we investigate structural perturbations in the graph spectral domain. The core principle is to regularize the attack objective by the spectral distance between the original and the perturbed graphs. The spectral distance essentially quantifies the structural difference between graphs, since graph spectrum reflects the global connectivity of different components in a graph. Our recipe only involves the graph structure, thus is widely applicable to many target graph models and attack frameworks. The experiments demonstrate the remarkable effectiveness of spectral regularization for white-box structural attacks at both training and test time. The qualitative analysis shows the connection between the attack behavior and the imposed changes on spectral distribution, which provides empirical evidence that spectral regularization is an effective manner to modify the structural property of graphs.

## DIB-R++: Learning to Disentangle Material from Lighting Using a Deferred Image-based Renderer

### TL;DR

None

### Abstract

We consider the challenging problem of predicting intrinsic object properties from a single image by exploiting differentiable graphics renderers. Many previous learning-based approaches for inverse graphics adopt rasterization-based renderers and assume naive lighting and material models, which often fail to account for non-Lambertian, specular reflections commonly observed in the wild. In this work, we propose DIB-R++, a deferred, image-based renderer which supports these photorealistic  effects by combining rasterization and ray-tracing, taking advantage of their respective strengths---speed and realism. Our renderer incorporates environmental lighting and spatially-varying material models to efficiently approximate light transport, either through direct estimation or via spherical basis functions. Compared to more advanced physics-based differentiable renderers leveraging path tracing, DIB-R++ is highly performant due to its compact and expressive shading model, which enables easy integration with learning frameworks for geometry, reflectance and lighting prediction from a single image without requiring any ground-truth. We experimentally demonstrate that our approach achieves superior material and lighting disentanglement on synthetic and real data compared to existing rasterization-based approaches and showcase several artistic applications including material editing and relighting. 

## Preservation of vector graphical representation classes for generative neural nets

### TL;DR

None

### Abstract

Vector graphics describe an image in terms of coordinates and their connections. The same visualization can be approximated by qualitatively and quantitatively different vector graphical images. Hence, the structure of the vector graphical representation will affect any algorithm that aims at extracting underlying patterns. In this article we investigate the preservation of different vector graphical representations in the context of generative networks. To illustrate our methodology we use the Quicksketch data which contains a collection of line sketches. We convert the Quicksketch data to different representation classes. We then train a generative recurrent neural net, sketch-RNN, for these classes and measure the preservation of the generated output using a pairwise distance function. Our results suggest that representations closest to linear are best preserved by the network. In addition, this article lays the foundation for vector graphical representation theory for neural nets. 

## Inductive Relation Prediction Using Analogy Subgraph Embeddings

### TL;DR

In this paper, we propose GraphANGEL, a novel relation prediction framework that predicts relations between each node pair by checking whether the subgraphs containing the pair are similar to other subgraphs containing the considered relation.

### Abstract

Prevailing methods for relation prediction in heterogeneous graphs aim at learning latent representations (i.e., embeddings) of observed nodes and relations, and thus are limited to the transductive setting where the relation types must be known during training.  Here,  we propose ANalogy  SubGraphEmbeddingLearning (GraphANGEL), a novel relation prediction framework that predicts relations5between each node pair based on the subgraphs containing the pair, as well as other  (analogy)  subgraphs with the same graph patterns.   Each graph pattern explicitly represents a specific logical rule, which contributes to an inductive bias that facilitates generalization to unseen relations and leads to more explainable predictive models. Moreover, our method also removes the limited neighborhood constraint of graph neural networks. Our model consistently outperforms existing models on heterogeneous graph based recommendation as well as knowledge graph completion.  We also empirically demonstrate our model’s capability in generalizing to new relations while producing explainable heat maps of attention scores across the discovered logic.

## On the Power of Edge Independent Graph Models

### TL;DR

We show that "edge-independent" graph generative models face an inherent tradeoff between memorization and the ability to generate realistic triangle dense graphs.

### Abstract

Why do many modern neural-network-based graph generative models fail to reproduce typical real-world network characteristics, such as high triangle density?  In this work we study the limitations  of $edge\ independent\ random\ graph\ models$, in which  each edge is added to the graph independently with some probability. Such models include both the classic Erdos-Renyi and stochastic block models, as well as  modern generative models such as NetGAN, variational graph autoencoders, and CELL. 
 We prove that subject to a $bounded\  overlap$ condition, which ensures that the model does not simply memorize a single graph, edge independent models are inherently limited in their ability to generate graphs with high triangle and other subgraph densities. Notably, such high densities are known to appear in real-world social networks and other graphs. We complement our negative results with a simple generative model that balances overlap and accuracy, performing comparably to more complex models in reconstructing many graph statistics. 

## Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning

### TL;DR

We propose a variational autoencoder that encodes graphs in a fixed-size latent space that is invariant under permutation of the input graph.

### Abstract

Recently, there has been great success in applying deep neural networks on graph structured data. Most work, however, focuses on either node- or graph-level supervised learning, such as node, link or graph classification or node-level unsupervised learning (e.g. node clustering). Despite its wide range of possible applications, graph-level unsupervised learning has not received much attention yet. This might be mainly attributed to the high representation complexity of graphs, which can be represented by $n!$ equivalent adjacency matrices, where $n$ is the number of nodes.
In this work we address this issue by proposing a permutation-invariant variational autoencoder for graph structured data. Our proposed model indirectly learns to match the node ordering of input and output graph, without imposing a particular node ordering or performing expensive graph matching. We demonstrate the effectiveness of our proposed model on various graph reconstruction and generation tasks and evaluate the expressive power of extracted representations for downstream graph-level classification and regression. 

## Gaussian Graphical Model Selection for Huge Data via Minipatch Learning

### TL;DR

None

### Abstract

Gaussian graphical models are  essential unsupervised learning techniques to estimate conditional dependence relationships between sets of nodes. While graphical model selection is a well-studied problem with many popular techniques, there are typically three key practical challenges: i) many existing methods become computationally intractable in huge-data settings with tens of thousands of nodes; ii) the need for separate data-driven tuning hyperparameter selection procedures considerably adds to the computational burden; iii) the statistical accuracy of selected edges often deteriorates as the dimension and/or the complexity of the underlying graph structures increase. We tackle these problems by proposing the Minipatch Graph (MPGraph) estimator. Our approach builds upon insights from the latent variable graphical model problem and utilizes ensembles of thresholded graph estimators fit to tiny, random subsets of both the observations and the nodes, termed minipatches. As estimates are fit on small problems, our approach is computationally fast with integrated stability-based hyperparameter tuning.  Additionally, we prove that under certain conditions our MPGraph algorithm achieves finite-sample graph selection consistency. We compare our approach to state-of-the-art computational approaches to Gaussian graphical model selection including the BigQUIC algorithm, and empirically demonstrate that our approach is not only more accurate but also extensively faster for huge graph selection problems.

## An Expectation-Maximization Perspective on Federated Learning

### TL;DR

We show that FedAvg is performing hard-EM on a specific graphical model and extend the graphical model in order to learn sparse models in federated learning.

### Abstract

Federated learning describes the distributed training of models across multiple clients while keeping the data private on-device. In this work, we view the server-orchestrated federated learning process as a hierarchical latent variable model where the server provides the parameters of a prior distribution over the client-specific model parameters. We show that with simple Gaussian priors and a hard version of the well known Expectation-Maximization (EM) algorithm, learning in such a model corresponds to FedAvg, the most popular algorithm for the federated learning setting. This perspective on FedAvg unifies several recent works in the field and opens up the possibility for extensions through different choices for the hierarchical model. Based on this view, we further propose a variant of the hierarchical model that employs prior distributions to promote sparsity. By similarly using the hard-EM algorithm for learning, we obtain FedSparse, a procedure that can learn sparse neural networks in the federated learning setting. FedSparse reduces communication costs from client to server and vice-versa, as well as the computational costs for inference with the sparsified network – both of which are of great practical importance in federated learning.

## When time is short: fast and simple models for temporal graph learning

### TL;DR

We propose a fast and simple GNN for temporal graphs that performs on par with state-of-the-art models but is orders of magnitude faster.

### Abstract

Modeling continuous-time events in real-world networks (e.g., social networks) is a major goal in graph representation learning. Recently, temporal graph neural networks (T-GNNs) have emerged as a prominent approach to deal with continuous-time dynamic graphs. In general, these models are complex and comprise many architectural components (e.g., attention layers and recurrent nets), hindering their application to large datasets. In addition, there is little understanding of the key factors behind their success. In this paper, we start off by revisiting T-GNNs and assessing the importance of real-valued time representations (i.e., timestamps). Strikingly, we find that using fine-grained timestamp representations hardly helps and sometimes even hurts performance. Based on these findings and on core concepts of T-GNNs, we propose EDGE (efficient dynamic graph encoder) --- a fast and simple model for temporal graph learning. Notably, EDGE outperforms most existing T-GNNs on temporal link prediction benchmarks while running orders of magnitude faster. For instance, our model performs similarly to the best known T-GNN on Reddit, with a $245\times$ speedup in training time.


## FlowGEN: A Generative Model for Flow Graphs

### TL;DR

None

### Abstract

Flow graphs capture the directed flow of a quantity of interest (e.g., water, power, vehicles) being transported through an underlying structure. Modeling and generating realistic flow graphs is key in many applications in infrastructure design, transportation, planning, biomedical and social sciences. However, they pose a great challenge to existing generative models due to their complex dynamics that is often governed by domain-specific physical laws. We introduce FlowGEN, an implicit generative model for flow graphs. FlowGEN learns how to jointly generate graph topologies and flows with diverse dynamics directly from data using a novel (flow) graph neural network. Experiments show that our approach is able to effectively reproduce relevant local and global properties of flow graphs, including flow conservation, cyclic trends, and congestion around hotspots.

## Locality-Based Mini Batching for Graph Neural Networks

### TL;DR

We propose locality-based mini batching, which enables large-scale training and inference for graph neural networks by precomputing fixed mini-batches based on node distances, graph partitioning, and local clustering.

### Abstract

Training graph neural networks on large graphs is challenging since there is no clear way of how to extract mini batches from connected data. Previous methods have largely relied on sampling to solve this. While this often leads to good convergence, it introduces significant overhead and requires expensive random data accesses. In this work we focus on computationally efficient mini batching and propose fixed, locality-based mini batching (LBMB). Our method first partitions the training/validation (primary) nodes into batches, and then selects the most important auxiliary (secondary) nodes for each batch using local clustering. Thanks to precomputed batches and consecutive memory accesses our method accelerates training by up to 10x per epoch compared to previous methods, and thus provides significantly better convergence per runtime. Furthermore, it accelerates inference by around 40x compared to using the full graph, often at little to no cost of accuracy.

## Node Classification Meets Link Prediction on Knowledge Graphs

### TL;DR

We propose a model that jointly tackles transductive link prediction and node classification over knowledge graphs, evaluate it using a carefully constructed knowledge graph, where both tasks are challenging, and obtain state-of-the-art results.

### Abstract

Node classification and link prediction are widely studied tasks in graph representation learning. While both transductive node classification and link prediction operate over a single input graph, they are studied in isolation so far, which leads to discrepancies. Node classification models take as input a graph with node features and incomplete node labels, and implicitly assume that the input graph is relationally complete, i.e., no edges are missing from the input graph. This is in sharp contrast with link prediction models that are solely motivated by the relational incompleteness of the input graph which does not have any node features. We propose a unifying perspective and study the problems of (i) transductive node classification over incomplete graphs and (ii) link prediction over graphs with node features. We propose an extension to an existing box embedding model, and show that this model is fully expressive, and can solve both of these tasks in an end-to-end fashion. To empirically evaluate our model, we construct a knowledge graph with node features, which is challenging both for node classification and link prediction. Our model performs very strongly when compared to the respective state-of-the-art models for node classification and link prediction on this dataset and shows the importance of a unified perspective for node classification and link prediction on knowledge graphs.

## Knowledge Distillation via Constrained Variational Inference

### TL;DR

Distilling knowledge of powerful predictive models in commonly used graphical models

### Abstract

Knowledge distillation has been used to capture the knowledge of a teacher model and distill it into a student model with some desirable characteristics such as being smaller, more efficient, or more generalizable. In this paper, we propose a framework for distilling the knowledge of a powerful discriminative model such as a neural network into commonly used graphical models known to be more interpretable (e.g., topic models, autoregressive Hidden Markov Models). Posterior of latent variables in these graphical models (e.g., topic proportions in topic models) is often used as feature representation for predictive tasks. However, these posterior-derived features are known to have poor predictive performance compared to the features learned via purely discriminative approaches. Our framework constrains variational inference for posterior variables in graphical models with a similarity preserving constraint. This constraint distills the knowledge of the discriminative model into the graphical model by ensuring that input pairs with (dis)similar representation in the teacher model also have (dis)similar representation in the student model. By adding this constraint to the variational inference scheme, we guide the graphical model to be a reasonable density model for the data while having predictive features which are as close as possible to those of a discriminative model. To make our framework applicable to a wide range of graphical models, we build upon the Automatic Differentiation Variational Inference (ADVI), a black-box inference framework for graphical models. We demonstrate the effectiveness of our framework on two real-world tasks of disease subtyping and disease trajectory modeling.

## Multivariate Time-Series Forecasting with Spatial-Temporal Attention Graph Neural Networks

### TL;DR

We represent the variable correlation as a dynamic graph and extend the Transformer to process multivariate time series.

### Abstract

Modeling multivariate time series is critical in modern intelligent systems. The accurate forecast of multivariate time-series data is still challenging due to the complicated latent correlation between pairs of variables. Recent works apply the Graph Neural Networks (GNNs) to the task, with the basic idea of representing the correlation as a static graph. However, the static graph fails to encode dynamic information in the dependence learning if the pattern of inter-series correlation is time-varying. Besides, these methods use fixed convolutional kernels to simultaneously capture intra-series long-term and short-term dependencies, leading to representation confusion. This paper proposes a spatial-temporal attention graph neural network (STAGNN) for accurate multivariate time-series forecasting, which contains two parts: First, The dependence is represented as a dynamic graph by learning two sets of correlation templates with embedding dictionaries. Then, the current inter-series correlation is generated from the templates. The constructed graph captures the precise correlation among variables even there are significant correlation changes. To learn the intra-series dependence, we apply the Transformer structure and present a multivariate embedding scheme, which significantly improves the long-term prediction performance. We conduct extensive experiments on two traffic datasets with prior structure and four real-world datasets without structural information. The results indicate that STAGNN achieves the state-of-the-art on both short-term and long-term predictions.

## Modeling Complexity of Graphs with Probabilistic Message Passing Neural Network

### TL;DR

We propose a probabilistic message passing based neural network that can capture both feature and graph complexity in graphs. Our model achieves state-of-the-art results on both node and graph-level prediction datasets.

### Abstract

Message passing (MP) based graph neural networks (GNNs)---which aim to iteratively aggregate feature and structure information of neighborhoods into each node's representation---have shown great power in predicting properties of graph data. To perform accurate property prediction, modeling feature and structure complexity of graphs is critical for MP-based GNNs. However, many existing MP-based GNNs represent nodes in graphs as points in low-dimensional vector spaces, which have difficulties in representing complexity. To address this challenge, we propose a novel MP-based GNN, namely \textbf{P}robabilistic \textbf{M}essage \textbf{P}assing \textbf{N}eural \textbf{N}etwork (PMPNN), which can effectively capture complexity information in graphs. The major novelties of PMPNN are two-fold: a) by noticing that the variances of random variables can naturally model complexity information, we represent nodes as probability distributions of random variables; b) to enable complexity information to propagate in graphs, we model the aggregation process as the mixture of probability distributions. Experiments demonstrate that PMPNN outperforms existing state-of-the-art MP-based GNNs on both node and graph-level prediction datasets.

## MarioNette: Self-Supervised Sprite Learning

### TL;DR

We jointly learn a dictionary of texture patches and train a network that places them onto a canvas, effectively deconstructing sprite-based content video content.

### Abstract

Artists and video game designers often construct 2D animations using libraries of sprites---textured patches of objects and characters. We propose a deep learning approach that decomposes sprite-based video animations into a disentangled representation of recurring graphic elements, in a self-supervised manner. By jointly learning a dictionary of possibly transparent patches and training a network that places them onto a canvas, we deconstruct sprite-based content into a sparse, consistent, and explicit representation that can be easily used in downstream tasks, like editing or analysis. Our framework offers a promising approach for discovering recurring visual patterns in image collections without supervision. 

## Improving Scene Graph Classification by Exploiting Knowledge from Texts

### TL;DR

We investigate whether textual scene descriptions can substitute for annotated image data. 

### Abstract

Scene graph classification requires a large amount of training data. While image annotation demands extensive labor, collecting textual descriptions of natural scenes requires less effort. We investigate whether textual scene descriptions can be used as a substitute for annotated image data. To this end, we present a novel scene graph classification framework that can be trained not only from annotated images but also from texts. We consider a classification pipeline consisting of two components: a feature extraction backbone and a relational reasoning module. Unlike the feature extraction backbone which takes images as input, the relational reasoning component can be trained from structured forms of knowledge, i.e., knowledge graphs. Even though a structured form of knowledge is not always available, we can generate it from unstructured texts by harnessing the power of transformer-based language models. We show that by fine-tuning the classification pipeline with the extracted knowledge graphs from texts, we can achieve ~8x more accurate results in scene graph classification, ~3x in object classification, and ~1.5x in predicate classification compared to the supervised baselines with only 1% of the annotated images.

## Algorithmic Fairness Verification with Graphical Models

### TL;DR

Verification of causal and group fairness of classifiers and fairness influence functions of features using stochastic subset sum and Bayesian networks.

### Abstract

In recent years, algorithmic fairness verification problem has attracted notable interest to assess different fairness metrics for Machine Learning (ML) algorithms, specifically classifiers. Existing fairness verifiers are limited by accuracy due to imprecise modelling of correlations among features, and scalability due to restrictive formulations as Stochastic SAT (SSAT) and SMT instances. In this paper, we propose a fairness verification framework, called FVGM, that encodes the correlations among features as a Bayesian network, and enables disentangled measurement of bias induced by the data and the classifier. In contrast to existing SSAT and SMT-based verifiers, FVGM proposes a stochastic subset-sum based approach for scalable fairness verification of linear classifiers. Experimentally we show that FVGM leads to accurate and scalable assessment for more diverse families of fairness-enhancing algorithms, fairness attacks, and group/causal fairness metrics than the state-of-the-art. Finally, FVGM allows computation of fairness influence functions as a stepping stone to detect the source of bias induced by subsets of features.

## Graphical Models in Heavy-Tailed Markets

### TL;DR

None

### Abstract

Heavy-tailed statistical distributions have long been considered a more realistic statistical model for the data generating process in financial markets in comparison to their Gaussian counterpart. Nonetheless, mathematical nuisances, including nonconvexities, involved in estimating graphs in heavy-tailed settings pose a significant challenge to the practical design of algorithms for graph learning. In this work, we present graph learning estimators based on the Markov Random Field framework that assume a Student-$t$ data generating process. We design scalable numerical algorithms, via the alternating direction method of multipliers, to learn both connected and $k$-component graphs along with their theoretical convergence guarantees. The proposed methods outperform state-of-the-art benchmarks in an extensive series of practical experiments with publicly available data from the S\&P500 index, foreign exchanges, and cryptocurrencies.

## Learning Time-Lagged Graphical Causal Models

### TL;DR

None

### Abstract

Identifying causal structure from observational time series data is important to learn robust predictive models. However, given a set of causal links, learning their time lag parameters requires us to make sparsity and overlap assumptions. In this paper, we propose to learn time lag parameters with the objective of improving recall of causal links, while learning to defer predictions when the overlap assumption is violated and optimizing the forecasting accuracy of a given observed time series. By learning these overlap-aware time-lagged graphical causal models with two orders of magnitude lesser number of parameters, we demonstrate an increase in the prediction accuracy across three popular datasets (DREAM3 gene expression, MoCAP human motion recognition and NYT news-based stock price prediction) by 18-25%, while increasing recall of causal links by up to 16-19% over several baseline models including Mutlivariate Autoregression, Neural Granger Causality, Graph Neural Networks and Graph Attention models.

## Gaussian Experts Selection using Graphical Models

### TL;DR

None

### Abstract

Local approximations are popular methods to scale Gaussian processes (GPs) to big data. Local approximations reduce time complexity by dividing the original dataset into subsets and training a local expert on each subset. Aggregating the experts' prediction is done assuming either conditional dependence or independence between the experts. Imposing the \emph{conditional independence assumption} (CI) between the experts renders the aggregation of different expert predictions time efficient at the cost of poor uncertainty quantification. On the other hand, modeling dependent experts can provide precise predictions and uncertainty quantification at the expense of impractically high computational costs. By eliminating weak experts via a theory-guided expert selection step, we substantially reduce the computational cost of aggregating dependent experts while ensuring calibrated uncertainty quantification. We leverage techniques from the literature on undirected graphical models, using sparse precision matrices that encode conditional dependencies between experts to select the most important experts. Moreover, our approach also provides a solution to the poor uncertainty quantification in CI-based models.

## KS-GNN: Keywords Search over Incomplete Graphs via Graphs Neural Network

### TL;DR

 To solve the problem of keyword search over incomplete graphs, we propose a novel model based on the graph neural network and the auto-encoder.

### Abstract

Keyword search is a fundamental task to retrieve information that is the most relevant to the query keywords. Keyword search over graphs aims to find subtrees or subgraphs containing all query keywords ranked according to some criteria. Existing studies all assume that the graphs have complete information. However, real-world graphs may usually contain some missing information (such as edges or keywords), thus making the problem much more challenging. To solve the problem of keyword search over incomplete graphs, we propose a novel model named KS-GNN based on the graph neural network and the auto-encoder. By considering the latent relationships and the frequency of different keywords, the proposed KS-GNN aims to alleviate the effect of missing information and is able to learn low-dimensional representative node embeddings that preserve both graph structure and keyword features. Our model can effectively answer keyword search queries with linear time complexity over incomplete graphs. The experiments on four real-world datasets show that our model consistently achieves better performance than state-of-the-art baseline methods in graphs having missing information.

## NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs

### TL;DR

We propose a method for drastic entity vocabulary reduction in knowledge graphs inspired by subword units and tokenizers in NLP

### Abstract

Conventional representation learning algorithms for knowledge graphs (KG) map each entity to a unique embedding vector. 
Such a shallow lookup results in a linear growth of memory consumption for storing the embedding matrix and incurs high computational costs of working with real-world KGs.
Drawing parallels with subword tokenization commonly used in NLP, we explore the landscape of more parameter-efficient node embedding strategies with possibly sublinear memory requirements. 
To this end, we propose NodePiece, an anchor-based approach to learn a fixed-size entity vocabulary. 
In NodePiece, a vocabulary of subword/sub-entity units is constructed from anchor nodes in a graph with known relation types. Given such a fixed-size vocabulary, it is possible to bootstrap an encoding and embedding for any entity, including those unseen during training.
Experiments show that NodePiece performs competitively in node classification, link prediction, and relation prediction tasks retaining less than 10% of explicit nodes in a graph as anchors and often having 10x fewer parameters.

## Approximate Decomposable Submodular Function Minimization for Cardinality-Based Components

### TL;DR

We provide fast approximation algorithms for a widely applied variant of decomposable submodular function minimization based on new sparse graph reduction techniques.

### Abstract

Minimizing a sum of simple submodular functions of limited support is a special case of general submodular function minimization that has seen numerous applications in machine learning. We develop faster techniques for instances where components in the sum are cardinality-based, meaning they depend only on the size of the input set. This variant is one of the most widely applied in practice, encompassing, e.g., common energy functions arising in image segmentation and recent generalized hypergraph cut functions. We develop the first approximation algorithms for this problem, where the approximations can be quickly computed via reduction to a sparse graph cut problem, with graph sparsity controlled by the desired approximation factor. Our method relies on a new connection between sparse graph reduction techniques and piecewise linear approximations to concave functions. Our sparse reduction technique leads to significant improvements in theoretical runtimes, as well as substantial practical gains in problems ranging from benchmark image segmentation tasks to hypergraph clustering problems. 

## Learning latent causal graphs via mixture oracles

### TL;DR

Theoretical guarantees and efficient algorithms for learning causal graphical models with latent variables.

### Abstract

We study the problem of reconstructing a causal graphical model from data in the presence of latent variables. The main problem of interest is recovering the causal structure over the latent variables while allowing for general, potentially nonlinear dependence between the variables. In many practical problems, the dependence between raw observations (e.g. pixels in an image) is much less relevant than the dependence between certain high-level, latent features (e.g. concepts or objects), and this is the setting of interest. We provide conditions under which both the latent representations and the underlying latent causal model are identifiable by a reduction to discrete deconvolution problems. The proof is constructive, and leads to several algorithms for explicitly reconstructing the full graphical model. We discuss efficient algorithms and provide experiments illustrating the algorithms in practice.

## Creating Training Sets via Weak Indirect Supervision

### TL;DR

In this work, we present a new weak supervision paradigm which automatically creates training sets for training a machine learning model given unlabeled dataset and indirect supervision sources.

### Abstract

Creating labeled training sets has become one of the major roadblocks in machine learning. To address this, recent \emph{weak supervision} frameworks synthesize training labels from multiple potentially noisy supervision sources. However, existing frameworks leverage supervision that shares the same output space as the target task. To extend the scope of usable sources, we propose \emph{Weak Indirect Supervision (WIS)}, a new paradigm for automatically synthesizing training labels based on \emph{indirect supervision sources} that have different output label spaces. To overcome the challenge of mismatched output spaces, we develop a probabilistic modeling approach, PLRM, which uses known label relation graphs to model and leverage indirect supervision sources. We provide a theoretically-principled test of the distinguishability of \combined for unseen labels, as well as a generalization error bound. On 40 WIS tasks from two large-scale datasets, we demonstrate that \combined improves over baselines by an average 2 points F1 score and 8 points lowest class-wise F1-score.

## Multiview Collective Graphical Models for Population Movement

### TL;DR

We extend Collective Graphical Models to a multiview setting using a novel hierarchical Bayesian model.

### Abstract

Tracking the movements of dynamic populations is an important task in the domains of urban planning, economics and ecology.  Retrieving this information is often complicated by a lack of information at the individual level, whether due to privacy concerns when analyzing movements of people, or due to the lack of sensor coverage when tracking migrating wildlife.  Collective Graphical Models (CGMs) are able to combat this lack of information on individuals by inferring population movement using aggregate statistics observed at specific locations and times. The tradeoff to this approach is that the model is under-specified, which in practice is often mitigated by a parameter sharing paradigm tailored to each individual problem.  We develop an approach for addressing this issue in the multiview setting, in which multiple datasets (i.e. views) provide different perspectives of the latent population movement. We leverage the shared information between these views to guide the inference of the population movement, resulting in improved accuracy for the multiview model.  Our approach is validated on simulated data as well as on real-world human movement datasets.

## Partial Identification of Counterfactual Distributions

### TL;DR

We develop a novel algorithm for bounding unknown counterfactual distributions from the observational data in an arbitrary causal diagram.

### Abstract

This paper investigates the problem of bounding counterfactual queries from a combination of observational data and qualitative assumptions about the underlying data-generating model. These assumptions are usually represented in the form of a causal diagram (Pearl, 1995). We show that all counterfactual distributions (over finite observed variables) in an arbitrary causal diagram could be generated by a special family of structural causal models (SCMs), compatible with the same causal diagram, where unobserved (exogenous) variables are discrete, taking values in a finite domain.  This entails a reduction in which the space where the original, arbitrary SCM lives can be mapped to a dual, more well-behaved space where the exogenous variables are discrete, and more easily parametrizable. Using this reduction, we translate the bounding problem in the original space into an equivalent optimization program in the new space. Solving such programs leads to optimal bounds over unknown counterfactuals. Finally, we develop effective Monte Carlo algorithms to approximate these optimal bounds from a finite number of observational data. Our algorithms are validated extensively on synthetic datasets.

## Effect Identification in Causal Diagrams with Clustered Variables

### TL;DR

A novel and coarser graphical representation of causal models accompanied by an identification approach for when knowledge about the relationships among the variables is only partially available.

### Abstract

One pervasive task found throughout the empirical sciences is to determine the effect of new interventions from observational data.  It is well understood that assumptions are needed to perform such causal inferences, an idea popularized through Cartwright’s motto: “no causes-in, no causes-out.” One common way of articulating these assumptions is through causal diagrams, which are a special type of graphical model with causal semantics [Pearl, 2000]. The graphical approach has been applied successfully in many settings, but there are still challenges to its use particularly in high-dimensional domains, for example, in medicine, where there is often only background knowledge about the relationships among a subset of variables at play. In this paper, we introduce cluster causal diagrams, or C-DAGs, which allow for partial understanding of the relationships among variables to be represented, alleviating DAGs’ somewhat stringent requirements. C-DAGs provide a simple yet effective way to partially abstract a grouping of variables among which causal relationships are unknown. Our goal is to develop machinery to reason on top of C-DAG’s new representation.  In particular, we first define a new version of d-separation and prove its soundness and completeness. Second, we extend these new separation criteria and prove the validity of the corresponding do-calculus. Lastly, we show the soundness of the causal identification algorithm for effect identification from observational data with cluster causal diagrams.

## Federated Graph Classification over Non-IID Graphs

### TL;DR

We propose new methods for cross-dataset/cross-domain graph classification with federated learning

### Abstract

Federated learning has emerged as an important paradigm of training machine learning models in different domains. For graph-level tasks such as graph classification, graphs can also be regarded as a special type of data samples, which can be collected and stored in separate local systems. Similar to other domains, multiple local systems, each holding a small set of graphs, may benefit from collaboratively training a powerful graph mining model, such as the popular graph neural networks (GNNs). To provide more motivation towards such endeavors, we analyze real-world graphs from different domains to confirm that they indeed share certain graph properties that are statistically significant compared with random graphs. However, we also find that different sets of graphs, even from the same domain or same dataset, are non-IID regarding both graph structures and node features. To handle this, we propose a graph clustering federated learning (GCFL) framework that dynamically finds clusters of local systems based on the gradients of GNNs, and theoretically justify that such clusters can reduce the structure and feature heterogeneity among graphs owned by the local systems. Moreover, we observe the gradients of GNNs to be rather fluctuating in GCFL which impedes high-quality clustering, and design a gradient sequence-based clustering mechanism based on dynamic time warping (GCFL+). Extensive experimental results and in-depth analysis demonstrate the effectiveness of our proposed frameworks.

## When Opposites Attract: Compatible Label Propagation for Heterophilous Graphs

### TL;DR

Simple but efficient Compatible Label Propagation algorithm for graph learning tasks with different levels of heterophily. 

### Abstract

Graph Neural Networks (GNNs) have been predominant for graph learning tasks; however, recent studies showed that a well-known graph algorithm Label Propagation (LP) combined with a shallow neural network can achieve comparable performance to GNNs in semi-supervised node classification on graphs with high homophily. In this paper, we show that this approach falls short on graphs with low homophily, where nodes often connect to the nodes of the opposite classes. We then propose a simple and strong adaptation of LP algorithm to the graphs with different levels of homophily. Our algorithm first learns the class compatibility matrix and then aggregates label predictions using LP algorithm weighted by class compatibilities. On a wide variety of benchmarks, we show that our approach achieves the top performance on graphs with low homophily and matches GNNs’ performance on graphs with high homophily. Meanwhile, it has orders of magnitude fewer parameters and is significantly faster than GNNs. Empirical evaluations demonstrate that simple adaptations of LP can be competitive in semi-supervised node classification in both homophily and heterophily regimes.

## Simulating Continuum Mechanics with Multi-Scale Graph Neural Networks

### TL;DR

Present a framework to predict the spatio-temporal evolution of continuum mechanics by applying MP between graphs with different levels of spatial resolutions.

### Abstract

Continuum mechanics simulators, numerically solving one or more partial differential equations, are essential tools in many areas of science and engineering, but their performance often limits application in practice. Recent modern machine learning approaches have demonstrated their ability to accelerate spatio-temporal predictions, although, with only moderate accuracy in comparison. Here we introduce MultiScaleGNN, a novel multi-scale graph neural network model for learning to infer unsteady continuum mechanics. MultiScaleGNN represents the physical domain as an unstructured set of nodes, and it constructs one or more graphs, each of them encoding different scales of spatial resolution. Successive learnt message passing between these graphs improves the ability of GNNs to capture and forecast the system state in problems encompassing a range of length scales. Using graph representations, MultiScaleGNN can impose periodic boundary conditions as an inductive bias on the edges in the graphs, and achieve independence to the nodes' positions. We demonstrate this method on advection problems and incompressible fluid dynamics. Our results show that the proposed model can generalise from uniform advection fields to high-gradient fields on complex domains at test time and infer long-term Navier-Stokes solutions within a range of Reynolds numbers. Simulations obtained with MultiScaleGNN are between two and four orders of magnitude faster than the ones on which it was trained.

## Neighbor Averaging over Relation Subgraphs for Heterogeneous Graphs

### TL;DR

None

### Abstract

Graph neural networks (GNNs) have recently attracted intense research interest for their ability to learn from graph-structured data.  However, it has been difficult to scale GNNs to large-scale graphs, as the memory requirement far exceeds that available on a single GPU. Recent work has proposed using the graph for feature smoothing which averages neighboring nodes' features before learning, resulting in simpler and more scalable computation than learning a hierarchy of internal representations, as is done in GNNs.  

Unfortunately, neighbor averaging cannot be directly applied to heterogeneous graphs, in which an entity (node) can have different types of relations (edges) with different neighbors. To utilize entity and relation types, we propose
Neighbor Averaging over Relation Subgraphs (NARS), which  randomly samples a subset of relation types and performs neighbor
averaging over each subgraph that consists only of edges with the sampled relations.  By aggregating the resulting smoothed features of different subgraphs using a trainable convolution, Neighbor Averaging over Relation Subgraphs can automatically select useful features for the learning task. We also describe optimizations to perform feature smoothing in a memory-efficient way, both at training and inference time. NARS achieves a new state of the art accuracy on several benchmark datasets, outperforming more expensive GNN-based methods.

## Techniques for Analysing Multi-decision Influence Diagrams

### TL;DR

We establish graphical criteria for value of information and value of control for sequential decisions.

### Abstract

Influence diagrams have recently been used to analyse the safety and fairness properties of AI systems. Key building blocks are the graphical criteria for concepts like value of information and value of control. In this paper, we develop tools for proving graphical criteria in multi-decision influence diagrams. We use these tools to prove that a previously known criterion for value of information is complete, and to establish a new sound and complete criterion for value of control.

## A hybrid approach for Causal Inference in Knowledge Graphs

### TL;DR

None

### Abstract

In the recent years, causal modelling has been used widely to improve generalization and to provide interpretability in machine learning models. To determine cause-effect relationships in the absence of a randomized trial, we can model causal systems with counterfactuals and interventions given enough domain knowledge. However, there are several cases where domain knowledge is almost absent and the only recourse is using a statistical method to estimate causal relationships. While there have been several works done in estimating causal relationships in unstructured data, we are yet to find a well-defined framework for estimating causal relationships in Knowledge Graphs. Knowledge graphs are commonly used to provide a semantic framework for data with complex inter-domain relationships. In this paper, we try to define a hybrid approach that allows us to discover cause-effect relationships in Knowledge Graphs.  The proposed approach is based around the finding of the instantaneous causal structure of a non-experimental matrix using a non-Gaussian model, i.e; finding the causal ordering of the variables in a non-Gaussian setting. The non-experimental matrix is a low-dimensional tensor projection obtained by decomposing the adjacency tensor of a Knowledge Graph. We use two different pre-existing algorithms, one for the causal discovery and the other for decomposing the Knowledge Graph and combine them to get the causal structure in a Knowledge Graph.

## Parameter Prediction for Unseen Deep Architectures

### TL;DR

We propose a benchmark and models for predicting parameters of diverse neural networks in a single forward pass using another network

### Abstract

Deep learning has been successful in automating the design of features and other critical components in machine learning pipelines. One critical element that remains largely hand-designed is the optimization algorithm used to iteratively optimize neural network parameters. Thus, a natural question is whether we can use deep learning to directly predict these parameters. To study this, we introduce a large-scale dataset of diverse architectures - DEEPNETS-1M - and use it to explore parameter prediction on CIFAR-10 and ImageNet. By leveraging advances in graph neural networks, we propose a hypernetwork that can predict performant parameters in a single forward pass taking a fraction of a second, even on a CPU. The proposed model achieves surprisingly good results on unseen and diverse networks. For example, it can predict all 24 million parameters of ResNet-50 achieving a 60% accuracy on CIFAR-10. We further scale our model to ImageNet, where predicted parameters achieve a top-5 accuracy close to 50% on some architectures. As a useful byproduct, our model learns a strong representation of neural network architectures.

## Adaptive Filters and Aggregator Fusion for Efficient Graph Convolutions

### TL;DR

We propose a new graph neural network architecture that consumes asymptotically less memory than current approaches, while also achieving better accuracy across 6 datasets.

### Abstract

Deploying graph neural networks (GNNs) remains difficult due to their high memory consumption and inference latency. In this work we present a new type of GNN architecture that achieves state-of-the-art performance with lower memory consumption and latency, along with characteristics suited to accelerator implementation. Our proposal uses memory proportional to the number of vertices in the graph, in contrast to competing methods which require memory proportional to the number of edges; we find our efficient approach actually achieves higher accuracy than competing approaches across 6 large and varied datasets against strong baselines. We achieve our results by using a novel adaptive filtering approach inspired by signal processing; it can be interpreted as enabling each vertex to have its own weight matrix, and is not directly related to attention. Following our focus on efficient hardware usage, we propose aggregator fusion, a technique to enable GNNs to significantly boost their representational power, with negligible memory overheads and only a small increase in latency of 14% over standard sparse matrix multiplication.

## Novel Upper Bounds for the Constrained Most Probable Explanation Task

### TL;DR

A novel method that integrates fast knapsack algorithms, mini buckets and Lagrange relaxations and decompositions to yield upper bounds on the optimal value of a hard discrete constrained optimization problem.

### Abstract

 We propose several schemes for upper bounding the optimal value of the constrained most probable explanation (CMPE) problem. Given a set of discrete random variables, two probabilistic graphical models defined over them and a real number $q$, this problem involves finding an assignment of values to all the variables such that the probability of the assignment is maximized according to the first model and is bounded by $q$ w.r.t. the second model. In prior work, it was shown that CMPE is a unifying problem with several applications and special cases including the nearest assignment problem, the decision preserving most probable explanation task and robust estimation. It was also shown that CMPE is NP-hard even on tractable models such as bounded treewidth networks and is hard for integer linear programming methods because it includes a dense global constraint. The main idea in our approach is to simplify the problem via Lagrange relaxation and decomposition to yield either a knapsack problem or the unconstrained most probable explanation (MPE) problem, and then solving the two problems, respectively using specialized knapsack algorithms and mini-buckets based upper bounding schemes. We evaluate our proposed scheme along several dimensions including quality of the bounds and computation time required on various benchmark graphical models and how it can be used to find heuristic, near-optimal feasible solutions in an example application pertaining to robust estimation and adversarial attacks on classifiers.

## A Theory of the Discoverable

### TL;DR

This paper proposes and studies a new theoretical model (PACC-discovery) of resource-constrained causal discovery that builds on foundations from both PAC-learning and causal inference.

### Abstract

Causal discovery from observational data---data one observes passively in contrast to actively performing experiments such as in a randomized controlled trial---is one of the hottest topics currently in artificial intelligence, machine learning, statistics, epidemiology, economics, and beyond.  Example tasks include determining from electronic health records (EHRs) whether a new virus such as covid-19 or a new drug on the market is causing a previously-unanicipated downtream event in some patients.  While there exists much elegant theory for causal discovery and causal inference, motivating methods from propensity scoring and inverse probability weighting to discovery of causal models represented as Bayesian networks or other probabilistic graphical models, the theory and proofs often make very strong assumptions including unlimited data and computation, analogous to early theories of supervised machine learning such as identification in the limit or BC-identification.  Valiant's paper, ``"A Theory of the Learnable,'' replaced the exact learning of identification in the limit  with approximate learning and took into account the resource-bounded nature of real-world learning: a learner has only a finite amount of data and time, and it should receive more of each when higher accuracy is required.  Successful methods in causal discovery from observational data also must work well with finite data and time, where "working well" may mean achieving high but imperfect accuracy.  In analogy to Valiant's model, which eventually came to be called "probably approximately correct (PAC) learning,'' this paper defines approximately correct causal discovery.  It formalizes the variations in protocols in causal discovery from observational data, for example that the learner may have partial knowledge of the true causal model or may assume no unmeasured confounders (strong sequential ignorability).  The resulting model is called protocoled approximately correct causal discovery, or PACC discovery.  Intuitively, a PACC discovery algorithm must with high accuracy distinguish between a true causal model and a competing imposter, or null model, both formalized as randomized Turing machines.  The more similar the models, and the higher the accuracy required in distinguishing between them, the more data and time the discovery algorithm should receive.  This paper provides initial results in this PACC discovery model that take into account limits on time and data and that show the promise of the model both for motivating new algorithms and providing theoretical insights into existing algorithms that work well in practice but that until now have lacked theoretical guarantees.  Methods studied included causal graphical model learning, propensity scoring, instrumental variable methods, and self-controlled case series.

## Distributionally Robust Structure Learning for Discrete Pairwise Markov Networks

### TL;DR

We propose a computationally efficient and distributionally robust method for structure learning of discrete pairwise Markov random fields with near-optimal sample complexity.

### Abstract

We consider the problem of learning the underlying structure of a general discrete pairwise Markov network. Existing approaches that rely on empirical risk minimization may perform poorly in settings with noisy or scarce data. To overcome these limitations, we propose a computationally efficient and robust learning method for this problem with near-optimal sample complexity. Our approach builds upon distributionally robust optimization (DRO) and maximum conditional log-likelihood. The proposed DRO estimator minimizes the worst-case expected risk over an ambiguity set of adversarial distributions within bounded transport cost or f-divergence of the empirical data distribution. We show that the primal minimax learning problem can be efficiently solved by leveraging sufficient statistics and greedy maximization in the intractable dual formulation. Based on DRO's approximation to Lipschitz and variance regularization, we derive near-optimal sample complexities matching existing results. Extensive empirical evidence with different corruption models corroborates the effectiveness of the proposed method.

## Improving the Reconstruction of Disentangled Representation Learners via Multi-Stage Modelling

### TL;DR

None

### Abstract

Current autoencoder-based disentangled representation learning methods achieve disentanglement by penalizing the (aggregate) posterior to encourage statistical independence of the latent factors. This approach introduces a trade-off between disentangled representation learning and reconstruction quality since the model does not have enough capacity to learn correlated latent variables that capture detailed information present in most image data. To overcome this trade-off, we present a novel multi-stage modelling approach where the disentangled factors are first learned using a penalty-based disentangled representation learning method; then, the low-quality reconstruction is improved with another deep generative model that is trained to model the missing correlated latent variables, adding detail information while maintaining conditioning on the previously learned disentangled factors. Taken together, our multi-stage modelling approach results in a single, coherent probabilistic model that is theoretically justified by the principle of D-separation and can be realized with a variety of model classes including likelihood-based models such as variational autoencoders, implicit models such as generative adversarial networks, and tractable models like normalizing flows or mixtures of Gaussians. We demonstrate that our multi-stage model has higher reconstruction quality than current state-of-the-art methods with equivalent disentanglement performance across multiple standard benchmarks.

## Correlated Stochastic Block Models: Exact Graph Matching with Applications to Recovering Communities

### TL;DR

By determining the information-theoretic limits for exact graph matching in edge-correlated stochastic block models, we show how to combine information from multiple networks to improve community recovery algorithms.

### Abstract

Motivated by applications in machine learning, we consider the task of learning latent community structure from multiple correlated networks. First, we study the problem of learning the latent vertex correspondence between two edge-correlated stochastic block models, focusing on the regime where the average degree is logarithmic in the number of vertices. We derive the precise information-theoretic threshold for exact recovery: above the threshold there exists an estimator that outputs the true correspondence with probability close to 1, while below the threshold no estimator can recover the true correspondence with probability bounded away from 0. As an application of our results, we show how one can exactly recover the latent communities using {\it multiple} correlated graphs in parameter regimes where it is information-theoretically impossible to do so using just a single graph. 

## Scalable and Adaptive Graph Neural Networks with Self-Label-Enhanced Training

### TL;DR

We propose a scalable GNN method (SAGN) with a graph structure-aware attention mechanism and associated training approach (SLE) effectively combining self-training and label propagation for scalable methods.

### Abstract

Besides of the existing neighbor sampling techniques applied on common Graph Neural Networks (GNNs), scalable methods allowing normal minibatch training can more easily scale to large scaled graphs. They decouple graph convolutions and other learnable transformations into preprocessing and a scalable classifier. A complex and graph structure-aware classifier is important to achieve competitive performances. By replacing redundant concatenation operation in Scalable Inception Graph Neural Networks (SIGN) with a more graph structure-aware attention mechanism, we propose Scalable and Adaptive Graph Neural Networks (SAGN). SAGN can adaptively gather neighborhood information among different hops. To further improve scalable GNNs by introducing the existing techniques applied on common GNNs for semi-supervised learning tasks, we propose Self-Label-Enhanced (SLE) training approach combining the self-training approach and label propagation in depth. We add the base model with a scalable label model. Then we iteratively train models and enhance the training set in several stages. To generate input of the label model, we apply label propagation based on one-hot encoded label vectors without inner random masking. We find out that empirically the label leakage has been effectively alleviated with enough propagation depth. The hard pseudo labels in the enhanced training set participate in label propagation with true labels, which propagates model knowledge and label information into the whole graph. Experiments on both inductive and transductive datasets demonstrate that, compared with other sampling-based and sampling-free methods, SAGN achieves better or comparable results and SLE can further improve performance.

## Sequential Causal Imitation Learning with Unobserved Confounders

### TL;DR

We provide a complete graphical condition for determining feasibility of sequential imitation in the presence of latent confounding

### Abstract

"Monkey see monkey do" is an age-old adage, referring to naive imitation without a deep understanding of a system's underlying mechanics. Indeed, if a demonstrator has access to information unavailable to the imitator (monkey), such as a different set of sensors, then no matter how perfectly the imitator models its perceived environment (See), attempting to directly reproduce the demonstrator's behavior (Do) can lead to poor outcomes. Imitation learning in the presence of a mismatch between demonstrator and imitator has been studied in the literature under the rubric of causal imitation learning  (Zhang et. al. 2020), but existing solutions are limited to single-stage decision-making. This paper investigates the problem of causal imitation learning in sequential settings, where the imitator must make multiple decisions per episode. We develop a graphical criterion that is both necessary and sufficient for determining the feasibility of causal imitation, providing conditions when an imitator can match a demonstrator's performance despite differing capabilities. Finally, we provide an efficient algorithm for determining imitability, and corroborate our theory with simulations.

## Contrastive Graph Representation Learning with Complement Graph for Graph Classification

### TL;DR

None

### Abstract

Graph representation learning has attracted much attention to work out the real-world tasks in social network analysis and bioinformatics. Graph embedding with readout function produces node embedding vectors for input to classifier, but graph representation learning projects the nodes with similar properties to a closer point in the embedding space, which can be a local optimum that all nodes converge into. Previous contrastive learning methods learn the node features without relational information and take a lot of time to train huge pairs of positive and negative connections. In this paper, we propose a novel graph representation learning method that utilizes a complement graph on the same vertices so that the original graph $G_1$ and the complement graph $G_2$ are utilized as positive and negative pairs as in the contrastive learning, respectively. It produces two embedding vectors from $G_1$ and $G_2$ with a graph modeling method. As a result, they represent the connectivity and the non-connectivity of $G_1$, respectively. We can obtain the final embedding vector by merging them. Since the proposed method uses the same structure of graph neural network twice, the time complexity is not significantly increased, and the graph property is used more diversely. A thorough experiment with six well-known benchmark datasets and ten graph neural networks reveals that the classification performance with the proposed method increases by about 2~3% and a maximum of 10%. Moreover, the time taken to learn the graph embedding increases by about 0.5 sec/batch.

## A simpler spectral approach for clustering in directed networks

### TL;DR

None

### Abstract

We study the task of clustering in directed networks, focusing on the
directed stochastic block model and its variants. We show that using the
eigenvalue/eigenvector decomposition of the non-hermitian adjacency
matrix yields non-trivial information down to the very sparse regime
where the average degree is of constant order. This is simpler than
common methods which are based on a combination of data regularization
and SVD truncation applied to hermitizations of the adjacency matrix.
Our analysis is based on a new Master Theorem describing sharp
asymptotics for isolated eigenvalues/eigenvectors of sparse,
non-symmetric matrices with independent entries. We also study the
limiting distribution of the entries of these eigenvectors; in the task
of digraph clustering with spectral embeddings, we provide numerical
evidence for the superiority of Gaussian Mixture clustering over the
widely used k-means algorithm.

## AAAN: Anomaly Alignment in Attributed Networks

### TL;DR

We propose an approach to detecting an optimal anomaly subgraph in the attributed graph which lacks sufficient anomaly features.

### Abstract

Anomaly subgraph detection is an important problem that has been well-researched in various applications, ranging from cyberattacks in computer networks to malicious activities in social networks.  Most existing approaches deal with anomaly detection in attributed networks with sufficient features. However, many industry data with insufficient anomalous attributes are the main challenge to anomaly detection. In an attributed network without sufficient anomalous attributes, the approaches fail to identify real anomalies, \textit{e.g.}, crime hotspots in city block networks, or terrorist events in private social networks with few public reports. To address this problem, we propose \textit{Anomaly Alignment in Attributed Networks} (AAAN), which performs anomaly subgraph detection by aligning anomalies in two attributed networks (one with insufficient anomalous features and the other with sufficient anomalous features). Extensive experiments on three real-world datasets (Weibo, Baidu migration network, Covid-19 pandemic) show the effectiveness and efficiency of our algorithm. We can identify the crime hotspots in terms of the city blocks from urban traffic networks, which align with the criminal events reported in social networks.  The results also demonstrate how AAAN outperforms competitive approaches in the Covid-19 outbreak anomaly subgraph detection and urban crime hotspot detection tasks.

## Probabilistic Graphical Models and Tensor Networks: A Hybrid Framework

### TL;DR

We develop a principled method for integrating probabilistic graphical models and tensor networks for the task of discrete structured probabilistic modeling

### Abstract

We investigate a correspondence between two formalisms for discrete probabilistic modeling: probabilistic graphical models (PGMs) and tensor networks (TNs), a powerful modeling framework for simulating complex quantum systems. The graphical calculus of PGMs and TNs exhibits many similarities, with discrete undirected graphical models (UGMs) being a special case of TNs. However, more general probabilistic TN models such as Born machines (BMs) employ complex-valued hidden states to produce quantum-inspired correlations among the probabilities. While representing a novel modeling resource for capturing structure in discrete probability distributions, this behavior also renders the direct application of standard PGM inference and learning algorithms impossible. We aim to bridge this gap by introducing a hybrid PGM-TN formalism that integrates quantum-like correlations into PGM models in a principled manner, using the physically-motivated concept of decoherence. We first prove that applying decoherence to the entirety of a BM model converts it into a discrete UGM, and conversely, that any subgraph of a discrete UGM can be represented as a decohered BM. This method allows a broad family of probabilistic TN models to be encoded as partially decohered BMs, a fact we leverage to combine the representational strengths of both model families. We experimentally verify the performance of such hybrid models in a sequential modeling task, and identify several promising uses of our method within the context of existing applications of graphical models.

## Task and Model Agnostic Adversarial Attack on Graph Neural Networks

### TL;DR

targeted black box evasion attack on graphs neural networks

### Abstract

Graph neural networks (GNNs) have witnessed significant adoption in the industry owing to impressive performance on various predictive tasks. Performance alone, however, is not enough. Any widely deployed machine learning algorithm must be robust to adversarial attacks. In this work, we investigate this aspect for GNNs, identify vulnerabilities, and link them to graph properties that may potentially lead to the development of more secure and robust GNNs. Specifically, we formulate the problem of task and model agnostic evasion attacks where adversaries modify the test graph to affect the performance of any unknown downstream task. The proposed algorithm, GRAND shows that distortion of node neighborhoods is effective in drastically compromising prediction performance. Although neighborhood distortion is an NP-hard problem, GRAND designs an effective heuristic through a novel combination of Graph Isomorphism Network with deep Q-learning. Extensive experiments on real datasets show that, on average, GRAND is up to 50% more effective than state of the art techniques, while being more than 100 times faster.

## Links between causal inference and system identification in control theory:  optimal selection of adjusting variables in Dynamic Bayesian Networks

### TL;DR

The article presents a systematic approach to select the optimal set of auxiliary variables for consistent learning of multiple causal relations in a Dynamic Bayesian Network while minimizing the cost of observations

### Abstract

The article focuses on the consistent estimation of the direct causal relation existing between two given stochastic processes in a class of linear Dynamic Bayesian Networks (DBNs) with a specific structure. The considered class of DBNs is characterized by the presence of latent variables which act as confounders by correlating the observations within each process at different times. This class of models occurs frequently in applications and it has been extensively studied in the area of control theory, as well. While the sequential application of standard graphical model criteria (i.e. Single Door Criterion or Backdoor criterion) could potentially lead to the same solution, recent approaches from control theory offer a more straightforward and systematic way to consistently determining the causal relation of interest. These control theoretical techniques offer a different perspective since they are based on so-called ``frequency'' methods, namely methods that make use of formal power series. A first contribution of this article is to connect causal inference methods developed in the area of graphical models with these more recent results. Indeed, we show that these control theoretical approaches can be reformulated by translating their conditions in terms of well-established concepts such as $d$-separation relations. A related contribution of the article is the determination of an optimal set of observed processes needed to adjust for in order to consistently estimate a causal relation between two processes. Indeed, in many applications different processes might be associated with different costs of observation. The equivalence with $d$-separation conditions enables us to cast this problem as an optimization over a flow graph. A final contribution is to extend this optimization problem to the case where multiple causal relations need to be simultaneously estimated. This non-trivial extension is obtained via an optimal multi-commodity flow problem with additional commodity specific constraints.

## Consistency of mechanistic causal discovery in continuous-time using Neural ODEs

### TL;DR

A study of the consistency of causal discovery in continuous-time and an algorithm based on Neural ODEs for irregularly-sampled time series.

### Abstract

The discovery of causal mechanisms from time series data is a key problem in fields working with complex systems. Most identifiability results and learning algorithms assume the underlying dynamics to be discrete in time. Comparatively few, in contrast, explicitly define causal associations in infinitesimal intervals of time, independently of the scale of observation and of the regularity of sampling. In this paper, we consider causal discovery in continuous-time for the study of dynamical systems. We prove that for vector fields parameterized in a large class of neural networks, adaptive regularization schemes consistently recover causal graphs in systems of ordinary differential equations (ODEs). Using this insight, we propose a causal discovery algorithm based on penalized Neural ODEs that we show to be applicable to the general setting of irregularly-sampled multivariate time series and to strongly outperform the state of the art.

## Robustifying Algorithms of Learning Latent Trees with Vector Variables

### TL;DR

We robustify the structure learning algorithms of latent tree-structured graphical models and derive the first instance-dependent impossibility result of latent tree structure learning to verify the optimality of some algorithms.

### Abstract

We consider learning the structures of Gaussian latent tree models with vector observations when a subset of them are arbitrarily corrupted. First, we present the sample complexities of Recursive Grouping (RG) and Chow-Liu Recursive Grouping (CLRG) without the assumption that the effective depth is bounded in the number of observed nodes, significantly generalizing the results in Choi et al. (2011). We show that Chow-Liu initialization in CLRG greatly reduces the sample complexity of RG from being exponential in the diameter of the tree to only logarithmic in the diameter for the hidden Markov model (HMM). Second, we robustify RG, CLRG, Neighbor Joining (NJ) and Spectral NJ (SNJ) by using the truncated inner product. These robustified algorithms can tolerate a number of corruptions up to the square root of the number of clean samples. Finally, we derive the first known instance-dependent impossibility result for structure learning of latent trees. The optimalities of the robust version of CLRG and NJ are verified by comparing their sample complexities and the impossibility result.

## EvoKG: A Unified Framework for Reasoning over Temporal Knowledge Graphs

### TL;DR

This paper presents EvoKG, an effective and efficient method for reasoning over temporal knowledge graphs, which models the evolving network structure and event time in a unified framework.

### Abstract

How can we perform knowledge reasoning over temporal knowledge graphs (TKGs)? TKGs represent facts about entities and their relations, where each fact is associated with a timestamp. Reasoning over TKGs, i.e., inferring new facts from time-evolving KGs, is crucial for many applications to provide intelligent services. However, despite the prevalence of real-world data that can be represented as TKGs, most methods focus on reasoning over static knowledge graphs, or cannot predict future events. In this paper, we present a problem formulation that unifies the two major problems that need to be addressed for an effective reasoning over TKGs, namely, modeling the evolving network structure and the event time. Our proposed method EvoKG addresses both tasks in a unified framework, which captures the ever-changing structural and temporal dynamics in TKGs via recurrent event modeling, and models the interactions between entities based on the time-aware neighborhood aggregation framework. Further, EvoKG models event time using flexible and efficient mechanisms based on neural density estimation. Experimental results show that EvoKG outperforms existing methods in terms of effectiveness (up to 76% more accurate time and link prediction) and efficiency.

## Identification of Partially Observed Linear Causal Models: Graphical Conditions for the Non-Gaussian and Heterogeneous Cases

### TL;DR

We provide graphical conditions which are necessary and sufficient for the identification of partially observed linear non-Gaussian causal models.

### Abstract

In causal discovery, linear non-Gaussian acyclic models (LiNGAMs) have been studied extensively. While the causally sufficient case is well understood, in many real problems the observed variables are not causally related. Rather, they are generated by latent variables, such as confounders and mediators, which may themselves be causally related. Existing results on the identification of the causal structure among the latent variables often require very strong graphical assumptions. In this paper, we consider partially observed linear models with either non-Gaussian or heterogeneous errors. In that case we give two graphical conditions which are necessary for identification of the causal structure. These conditions are closely related to sparsity of the causal edges. Together with one additional condition on the coefficients, which holds generically for any graph, the two graphical conditions are also sufficient for identifiability. These new conditions can be satisfied even when there is a large number of latent variables. We demonstrate the validity of our results on synthetic data.

## Semi-supervised Regression with Multi-view Diversity and Consistency

### TL;DR

A semi-supervised regression technique that combines diversity and consistency in a multi-view framework, based on probabilistic graphical assumptions. 

### Abstract

Label collection is costly in many applications, which poses the need for label-efficient learning. In this work, we present Semi-supervised Regression with Multi-view Diversity and Consistency (MVDC) -- a novel semi-supervised regression technique based on a multi-view learning framework. MVDC combines diversity with consistency -- two seemingly opposing yet complementary principles of multi-view learning - based on underlying probabilistic graphical assumptions. Given multiple deep views of the same input, MVDC encourages a negative correlation among the views' predictions on labeled data, while simultaneously enforces their agreement on unlabeled data. Under realistic evaluation setups, we demonstrate that MVDC outperforms competing methods on both tabular and image data. Ablation studies confirm the importance of having both consistency and diversity.

## Amortized Probabilistic Detection of Communities in Graphs

### TL;DR

A framework for amortized probabilistic detection of communities in graphs

### Abstract

Learning community structures in graphs has broad applications across scientific domains. While graph neural networks (GNNs) have been successful in encoding graph structures, existing GNN-based methods for community detection are limited by requiring knowledge of the number of communities in advance, in addition to lacking a proper probabilistic formulation to handle uncertainty. We propose a simple framework for amortized community detection, which addresses both of these issues by combining the expressive power of GNNs with recent methods for amortized clustering. Our models consist of a graph representation backbone that extracts structural information and an amortized clustering network that naturally handles variable numbers of clusters. Both components combine into well-defined models of the posterior distribution of graph communities and are jointly optimized given labeled graphs. At inference time, the models yield parallel samples from the posterior of community labels, quantifying uncertainty in a principled way. We evaluate several models from our framework on synthetic and real datasets and demonstrate superior performance to previous methods. As a separate contribution, we extend recent amortized probabilistic clustering architectures by adding attention modules, which yield further improvements on community detection tasks.


## Knowledge Graph Reasoning with Relational Directed Graph

### TL;DR

None

### Abstract

Reasoning on the knowledge graph (KG) aims to infer new facts from existing ones. Methods based on the relational path in the literature have shown strong, interpretable, and inductive reasoning ability. However, the paths are naturally limited in capturing complex topology in KG. In this paper, we introduce a novel relational structure, i.e., relational directed graph (digraph), which is composed of overlapped relational paths, to capture the KG’s structural information. Since the digraph exhibits more complex structure than paths, constructing and learning on the relational digraph are challenging. Here, we propose a variant of graph neural network, i.e., RED-GNN, to address the above challenges by learning the RElational Digraph with a variant of GNN. Specifically, RED-GNN recursively encodes multiple relational digraphs with shared edges and selects the strongly correlated edges through query-dependent attention weights. We demonstrate the significant gains on reasoning both KG with unseen entities and incompletion KG benchmarks by the relational digraph, the efficiency of RED-GNN, and the interpretable dependencies learned on the relational digraph.

## Learning Graph Structure for Scene Graph Generation

### TL;DR

None

### Abstract

In this paper, we investigate the problem of graph structure learning in presence of noisy and missing annotations for Scene Graph Generation (SGG). The goal is to find an optimal graph structure for each input image to facilitate the downstream SGG task. To this end, we formulate graph structure learning and Graph Neural Network (GNN) parameter learning as a bilevel programming problem, and solve it by optimizing the SGG objective. This allows graph structure learning no longer requiring ground-truth graph structures and eliminates the inconsistency between graph structure learning and GNN parameter learning. Moreover, we propose an improved Neumann-IFT algorithm to obtain an approximate solution for the bilevel programming problem, which is more stable and accurate than existing solutions. Extensive experiments on large-scale Visual Genome dataset show that proposed method can achieve competitive or state-of-the-art performances, especially under the mean recall metric.

## Graph Neural Networks with Local Graph  Parameters

### TL;DR

Addin hom counts of patterns to feature vectors of GNNs increases their power. We show this theoretically with a precise characterization and tools to recomend which patterns are best. We also show it empirically.. 

### Abstract

Various recent proposals increase the distinguishing power of Graph Neural Networks (GNNs) by propagating features between k-tuples of vertices. The distinguishing power of these “higher-order” GNNs is known to be bounded by the k-dimensional Weisfeiler-Leman (WL) test, yet their O(n^k) memory requirements limit their applicability. Other proposals infuse GNNs with local higher-order graph structural information from the start, hereby inheriting the desirable O(n) memory requirement from GNNs at the cost of a one-time, possibly non-linear, preprocessing step. We propose local graph parameter enabled GNNs as a framework for studying the latter kind of approaches and precisely characterize their distinguishing power, in terms of a variant of the WL test, and in terms of the graph structural properties that they can take into account. Local graph parameters can be added to any GNN architecture, and are cheap to compute. In terms of expressive power, our proposal lies in the middle of GNNs and their higher-order counterparts. Further, we propose several techniques to aide in choosing the right local graph parameters. Our results connect GNNs with deep results in finite model theory and finite variable logics. Our experimental evaluation shows that adding local graph parameters often has a positive effect for a variety of GNNs, datasets and graph learning tasks. 


## Hoformers: Graphical Interpretation of Transformers for an Improved Model Design

### TL;DR

We propose Hoformers, Transformers with heads-only attentions that have a guaranteed improvement in expressive power,  and we introduce tensor diagram representations of Transformers.

### Abstract

Transformers are neural network architectures that achieve remarkable performance in many areas. The key component of Transformers, Multi-Head Self-Attention(MHSA), has many parameters that could cause memory and computational inefficiencies. In addition, rigorous theoretical justifications of MHSA are insufficient. In the face of the situation, we first introduce the tensor diagram, which is mathematically rigorous yet intuitive to interpret MHSA. Guided by tensor diagram representations, we study how multi-head mechanism and latent features (i.e., token embedding in a sequence) coordinate on context capturing. We theoretically prove that the heads are more important than the number of latent features for each token in terms of expressive power. Therefore, we propose a novel design of attention units, namely Heads-Only Transformers (Hoformers), where we reduce every hidden representation of each token to a scalar. Under the same number of parameters,  we prove that Hoformers have higher expressive power and empirically show that they outperform vanilla Transformers in several tasks(for example,  a 4.7% performance increase in neural machine translation). Furthermore, Hoformers complement other efficient designs of Transformers(e.g., kernel method, low-rank approximation), as the former increase the expressive power while the latter reduce computational complexity in the sequence length. When combined with kernel methods,  Hoformers improve the result by 11% in image generation.

## Emergent Graphical Conventions in a Multi-agent Visual Communication Game

### TL;DR

None

### Abstract

Humans communicate with both symbolic languages and graphical sketches. Recent studies of emergent communication primarily focus on symbolic languages using grounded language learning games, which require agents to learn associations between presented visual concepts and pre-selected discrete tokens. However, such settings overlook the graphical sketches existing in human communications; they do not account for the evolution process from iconic sketches to abstract symbols, which we hypothesize is crucial for preserving the continuum of semantics in symbols. In this work, we take the very first step to computationally model and simulate such an evolution process via two neural agents playing a visual communication game; the sender communicates with the receiver by sketching on a canvas. With our distinctive game setup, we explore critical environmental drivers for the emergence of graphical conventions, spanning over game contexts, termination conditions, and communication duration. Since such communication medium is different from prior arts in emergent communication, we carefully design quantitative and qualitative methods to evaluate the emerged representations. Of note, our experiments uncover a specification of visual communication games where the emergent graphical conventions exhibit consistency and generalizability from the viewpoint of discrete symbols and maintain semantic similarity from the viewpoint of continuous vectors, wherein evolved sketches may potentially serve as primitives in symbolic open-domain communication.


## SQALER: Scaling Question Answering by Decoupling Multi-Hop and Logical Reasoning

### TL;DR

None

### Abstract

State-of-the-art approaches to reasoning and question answering over knowledge graphs (KGs) usually scale with the number of edges and can only be applied effectively on small instance-dependent subgraphs. In this paper, we address this issue by showing that multi-hop and more complex logical reasoning can be accomplished separately without loosing expressive power. Motivated by this insight, we propose an approach to multi-hop reasoning that scales linearly with the number of relation types in the graph, which is usually significantly smaller than the number of edges or nodes. This produces a set of candidate solutions that can be provably refined to recover the solution to the original problem. Our experiments on knowledge-based question answering show that our approach solves the multi-hop MetaQA dataset, achieves a new state-of-the-art on the more challenging WebQuestionsSP, is orders of magnitude more scalable than competitive approaches, and can achieve compositional generalization out of the training distribution.

## PLUR: A Unifying, Graph-Based View of Program Learning, Understanding, and Repair

### TL;DR

A single graph-based architecture can be applied to 15 seemingly different ML4Code tasks and achieves great results.

### Abstract

Machine learning for understanding and editing source code has recently attracted significant interest, with many developments in new models, new code representations, and new tasks.
This proliferation can appear disparate and disconnected, making each approach seemingly unique and incompatible, thus obscuring the core machine learning challenges and contributions.
In this work, we demonstrate that the landscape can be significantly simplified by taking a general approach of mapping a graph to a sequence of tokens and pointers.
Our main result is to show that 15 recently published tasks of different shapes can be cast in this form, based on which a single model architecture achieves near or above state-of-the-art results on nearly all tasks, outperforming custom models like code2seq and alternative generic models like Transformers.
This unification further enables multi-task learning and a series of cross-cutting experiments about the importance of different modeling choices for code understanding and repair tasks.
The full framework, called PLUR, is easily extensible to more tasks, and will be open-sourced.

## No RL, No Simulation: Learning to Navigate without Navigating

### TL;DR

None

### Abstract

Most prior methods for learning navigation policies for unseen test environments require access to simulation environments as they need online policy interaction and rely on ground-truth maps for rewards. However, there are some limitations to training in simulation environments. First, creating realistic simulation scenes based on real-world reconstructions, such as Gibson and MP3D, is expensive and requires a lot of manual effort. This limits the scalability of training data and consequently most navigation policies are trained only with tens or few hundreds of training scenes in simulation. Second, there is a fundamental challenge in transferring these simulation policies to robot platforms in the real-world due to the sim-to-real domain gap. 

## BayesIMP: Uncertainty Quantification for Causal Data Fusion

### TL;DR

Quantifying uncertainty in multiple causal graphs with Bayesian kernel mean embeddings.

### Abstract

While causal models are becoming one of the mainstays of machine learning, the problem of uncertainty quantification in causal inference remains challenging. In this paper, we study the causal data fusion problem, where data arising from multiple causal graphs are combined to estimate the average treatment effect of a target variable. As data arises from multiple sources and can vary in quality and sample size, principled uncertainty quantification becomes essential. To that end, we introduce \emph{Bayesian Causal Mean Processes}, the framework which combines ideas from probabilistic integration and kernel mean embeddings to represent interventional distributions in the reproducing kernel Hilbert space, while taking into account the uncertainty within each causal graph. To demonstrate the informativeness of our uncertainty estimation, we apply our method to the Causal Bayesian Optimisation task and show improvements over state-of-the-art methods.

## Convergence of Generalized Belief Propagation Algorithm on Graphs with Motifs

### TL;DR

Convergence of BP on ferromagnetic Ising model on graphs with motifs

### Abstract

Belief propagation is a fundamental message-passing algorithm for numerous applications. It is known that belief propagation algorithm is exact on tree graphs. However, belief propagation is run on loopy graphs in most applications. So, understanding the behavior of belief propagation on loopy graphs had been a major topic for researchers in different areas. We study the convergence behavior of generalized belief propagation algorithm on graphs with motifs (triangles, loops, etc.) We show under a certain initialization, generalized belief propagation converges to the global optimum of the Bethe free energy for ferromagnetic Ising models on graphs with motifs.

## Neural Trees for Learning on Graphs

### TL;DR

None

### Abstract

Graph Neural Networks (GNNs) have emerged as a flexible and powerful approach for learning over graphs. Despite this success, existing GNNs are constrained by their local message-passing architecture and are provably limited in their expressive power. In this work, we propose a new GNN architecture – the Neural Tree. The neural tree architecture does not perform message passing on the input graph, but on a tree-structured graph, called the H-tree, that is constructed from the input graph. Nodes in the H-tree correspond to subgraphs in the input graph, and they are reorganized in a hierarchical manner such that the parent of a node in the H-tree always corresponds to a larger subgraph in the input graph. We show that the neural tree architecture can approximate any smooth probability distribution function over an undirected graph. We also prove that the number of parameters needed to achieve an $\epsilon$-approximation of the distribution function is exponential in the treewidth of the input graph, but linear in its size. We prove that any continuous G-invariant/equivariant function can be approximated by a nonlinear combination of such probability distribution functions over G. We apply the neural tree to semi-supervised node classification in 3D scene graphs, and show that these theoretical properties translate into significant gains in prediction accuracy, over the more traditional GNN architectures. We also show the applicability of the neural tree architecture to citation networks with large treewidth, by using a graph sub-sampling technique.

## Graph Context Encoder: Graph Feature Inpainting for Graph Generation and Self-supervised Pretraining

### TL;DR

None

### Abstract

We propose the Graph Context Encoder (GCE), a simple but efficient approach for graph representation learning based on graph feature masking and reconstruction.
GCE models are trained to efficiently reconstruct input graphs similarly to a graph autoencoder where node and edge labels are masked. In particular, our model is also allowed to change graph structures by masking and reconstructing random pseudo-edges.
We show that GCE can be used for novel graph generation, with applications for molecule generation. Used as a pretraining method, we also show that GCE improves baseline performances in supervised classification tasks tested on multiple standard benchmark graph datasets.

## Recoverability Landscape of Tree Structured Markov Random Fields under Symmetric Noise

### TL;DR

We fully characterize recoverability in tree-structured MRF, and provide sample-efficient polynomial time algorithm for recovery.

### Abstract

We study the problem of learning tree-structured Markov random fields (MRF) on discrete random variables with common support when the observations are corrupted by a $k$-ary symmetric noise channel with unknown probability of error. 
For Ising models (support size = 2), past work has shown that graph structure can only be recovered up to the leaf clusters (a leaf node, its parent, and its siblings form a leaf cluster) and exact recovery is impossible. No prior work has addressed the setting of support size of 3 or more, and indeed this setting is far richer. As we show, when the support size is 3 or more, the structure of the leaf clusters may be partially or fully identifiable. We provide a precise characterization of this phenomenon and show that the extent of recoverability is dictated by the joint PMF of the random variables.  In particular, we provide necessary and sufficient conditions for exact recoverability. Furthermore, we present a polynomial time, sample efficient algorithm that recovers the exact tree when this is possible, or up to the unidentifiability as promised by our characterization, when full recoverability is impossible. Finally, we  demonstrate the efficacy of our algorithm experimentally.

## Linearized Structured Models

### TL;DR

None

### Abstract

Structured distributions, i.e. distributions over combinatorial spaces, are commonly used to learn latent probabilistic representations from observed data. However, scaling these models is bottlenecked by the high computational and memory complexity with respect to the size of the latent representations. Common models such as Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs) require time and space quadratic and cubic in the number of hidden states respectively. This work demonstrates a  simple approach to reduce the computational and memory complexity of a large class of structured models. We show that by viewing the central inference step as a matrix-vector product and using a kernel linearization constraint, we can reduce the complexity of inference by a factor of the hidden state size, critical when scaling to large models. This method provides exact inference for a model approximation of standard structured distributions.  Experiments with neural parameterized structured models for language modeling, polyphonic music modeling, unsupervised grammar induction, and video modeling show that our approach matches the accuracy of standard models at large state spaces while providing practical speedups.

## Non-approximate Inference for Collective Graphical Models on Path Graphs via Discrete Difference of Convex Algorithm

### TL;DR

None

### Abstract

The importance of aggregated count data, which is calculated from the data of multiple individuals, continues to increase. Collective Graphical Model (CGM) is a probabilistic approach to the analysis of aggregated data. One of the most important operations in CGM is maximum a posteriori (MAP) inference of unobserved variables under given observations. Because the MAP inference problem for general CGMs has been shown to be NP-hard, an approach that solves an approximate problem has been proposed. However, this approach has two major drawbacks. First, the quality of the solution deteriorates when the values in the count tables are small, because the approximation becomes inaccurate. Second, since continuous relaxation is applied, the integrality constraints of the output are violated. To resolve these problems, this paper proposes a new method for MAP inference for CGMs on path graphs. Our method is based on the Difference of Convex Algorithm (DCA), which is a general methodology to minimize a function represented as the sum of a convex function and a concave function. In our algorithm, important subroutines in DCA can be efficiently calculated by minimum convex cost flow algorithms. Experiments show that the proposed method outputs higher quality solutions than the conventional approach. 

## Adversarial Graph Augmentation to Improve Graph Contrastive Learning

### TL;DR

Adversarial training to learn augmentation strategies for better self-supervised graph representations.

### Abstract

Self-supervised learning of graph neural networks (GNN) is in great need because of the widespread label scarcity issue in real-world graph/network data. Graph contrastive learning (GCL), by training GNNs to maximize the correspondence between the representations of the same graph in its different augmented forms, may yield robust and transferable GNNs even without using labels. However, GNNs trained by traditional GCL often risk capturing redundant graph features and thus may be brittle and provide sub-par performance in downstream tasks. Here, we propose a novel principle, termed adversarial-GCL (\textit{AD-GCL}), which enables GNNs to avoid capturing redundant information during the training by optimizing adversarial graph augmentation strategies used in GCL. We pair AD-GCL with theoretical explanations and design a practical instantiation based on trainable edge-dropping graph augmentation. We experimentally validate AD-GCL by comparing with the state-of-the-art GCL methods and achieve performance gains of up-to~14\% in unsupervised, ~6\% in transfer and~3\% in semi-supervised learning settings overall with 18 different benchmark datasets for the tasks of molecule property regression and classification, and social network classification.

## KerGNNs: Interpretable Graph Neural Networks with Graph Kernels

### TL;DR

We make the first attempt to consider using neighborhood subgraph topology combined with kernel methods for GNN neighborhood aggregation, which can be more expressive than traditional GNNs with improved interpretability and transparency.

### Abstract

Graph kernels are historically the most widely-used techniques for graph classification tasks. However, these methods suffer from limited performance because of the hand-crafted combinatorial features of graphs. During recent years, graph neural networks (GNNs) have become state-of-the-art methods in downstream graph-related tasks due to the superior performance. Most GNNs are based on Message Passing Neural Network (MPNN) frameworks. However, recent studies show MPNNs can not exceed the power of the Weisfeiler-Lehman (WL) algorithm in graph isomorphism test. To address the limitations of existing graph kernel and GNN methods, in this paper, we propose a novel GNN framework, termed Kernel Graph Neural Networks (KerGNNs), which integrates graph kernels into the message passing process of GNNs. Inspired by convolution filters in convolutional neural networks (CNN), KerGNNs adopt trainable hidden graphs as graph filters and compute the graph kernels between graph filters and local subgraphs of the input graph. In addition, we show that MPNNs can be viewed as special cases of KerGNNs. We apply KerGNNs to multiple graph prediction tasks and use cross-validation to make fair comparisons with benchmarks. We show that our method achieves competitive performance compared with existing state-of-the-art methods, demonstrating the potential to increase the representation ability of GNNs. We also show that the trained graph filters in KerGNNs can reveal the local graph structures of the dataset, which significantly improves the model interpretability compared with conventional GNN models.

## Graph Convolutional Networks with Community-Based Graph Decomposition

### TL;DR

None

### Abstract

The success of graph convolutional networks (GCNs) on graph data analysis heavily relies on absorbing node features through links from its neighbors. The structural difference between nodes in the same neighborhood is inadequately expressed in most existing convolution-based aggregators. However, stochastic graph generative theories indicate that even linked to the same central node, the adjoining nodes may have different community identities, which potentially discerns their optimal way to share features with the center. In this work, we introduce an EM framework, the ComGCN, to learn graph convolution kernels that preserve structural information in neighborhoods. In the E-step, with a community-aware decomposition prior, ComGCN retrieves a set of kernels by decomposing the observed graph using a set of partitioning weights sampled from the variational posterior. In the M-step, ComGCN first aggregates node features in parallel for each individual graph component, then globally mix the features localized within these components for downstream graph-analytic tasks. We have evaluated ComGCN on several benchmarks and the promising results demonstrate the effectiveness of our model in both node and graph level feature extraction.

## Connecting Graph Convolutional Networks and Graph-Regularized PCA

### TL;DR

None

### Abstract

Graph convolution operator of the GCN model is originally motivated from a localized first-order approximation of spectral graph convolutions. This work stands on a different view; establishing a mathematical connection between graph convolution and graph-regularized PCA (GPCA). Based on this connection, the GCN architecture, shaped by stacking graph convolution layers, shares a close relationship with stacking GPCA. We empirically demonstrate that the unsupervised embeddings by GPCA paired with a 1- or 2-layer MLP achieves similar or even better performance than many sophisticated baselines on semi-supervised node classification tasks across five datasets including Open Graph Benchmark. This suggests that the prowess of graph convolution is driven by graph based regularization. In addition, we extend GPCA to the (semi-)supervised setting and show that it is equivalent to GPCA on a graph extended with “ghost” edges between nodes of the same label. Finally, we capitalize on the discovered relationship to design an effective initialization strategy based on stacking GPCA, enabling GCN to converge faster and achieve robust performance at large number of layers.

## Node-Variant Graph Filters in Graph Neural Networks

### TL;DR

Generate frequencies by means of node-variant graph filters; use them in lieu of nonlinear activation functions in GNNs

### Abstract

Graph neural networks (GNNs) have been successfully employed in a myriad of applications involving graph-structured data. Theoretical findings establish that GNNs use nonlinear activation functions to create low-eigenvalue frequency content that can be processed in a stable manner by subsequent graph convolutional filters. However, the exact shape of the frequency content created by nonlinear functions is not known, and thus, it cannot be learned nor controlled. In this work, node-variant graph filters (NVGFs) are shown to be capable of creating frequency content and are thus used in lieu of nonlinear activation functions. This results in a novel GNN architecture that, although linear, is capable of creating frequency content as well. Furthermore, this new frequency content can be either designed or learned from data. In this way, the role of frequency creation is separated from the nonlinear nature of traditional GNNs. Extensive simulations are carried out to differentiate the contributions of frequency creation from those of the nonlinearity.

## Structure and Feature Readout For Graph Representation

### TL;DR

None

### Abstract

Representation learning on graph data has attracted significant attention from researchers and Graph Neural Network (GNN) was proposed to deal with graph-structured data. For such data, graph readout modules are used universally to generate the graph level representation. However, many graph readout modules lose a significant amount of graph details and are not sensitive to the topology of the data, which is a major limitation of the GNN feature representation. In order to address this problem, we propose a powerful graph readout module named Structure-Feature-Combined graph readout module (SFC) , which generates a graph representation matrix to capture both features and topology information without losing any graph details, and a bilinear-like linear layer (BiLinear), which aims to classify/regress the representation matrix. Based on our proposed components, we build different architectures to address three main graph level tasks, i.e., graph similarity, graph classification, and regression, and our proposed method achieves state-of-the-art performance on nine datasets. Futhermore, we discuss the expression ability of GNNs on single-graph and multi-graph tasks. Our code is publicly available on https://github.com//anonymized-SFC-submission//Structure-Feature-Combined-Graph-Representation.


## Adapting Distilled Knowledge for Few-shot Relation Reasoning over Knowledge Graphs

### TL;DR

a novel few-shot knowledge graphs relation reasoning model  with sota performance.

### Abstract

Knowledge graphs (KGs) are serving as important resources for many applications, such as semantic search, question answering, or dialogue generation. As one of the fundamental tasks, multi-hop KG reasoning aims at generating effective and explainable relation prediction through reasoning paths. The current methods often require sufficient amount of training data (i.e., fact triples) for each query relation, impairing their applicabilities and performances over few-shot relations (with limited data) which are common in KGs. Despite that few few-shot relation reasoning methods have been proposed, their effectiveness and efficiency remain to be improved. To address these challenges, we propose a novel model ADK-KG for multi-hop few-shot relation reasoning over KGs. In ADK-KG, we introduce a reinforcement learning framework to model the sequential reasoning process, where the entity embeddings are refined by a text-enhanced heterogeneous graph neural network. Later, we employ a task-aware meta-learning algorithm to optimize model parameters that could be fast adapted for few-shot relations. A knowledge distillation module is further designed to make use of unlabeled data for improving model. A battery of empirical study on three benchmark datasets demonstrate that ADK-KG has strong efficiency and outperforms state-of-the-art approaches. 

## Adaptive Gaussian Processes on Graphs via Spectral Graph Wavelets

### TL;DR

We introduce a Gaussian process model for multi-scale data on graphs.

### Abstract

Graph-based models have been built on aggregating local and global information in the graph to extract representations that facilitate downstream learning tasks. When the data exhibit different levels of smoothness on the graph, however, a multi-scale approach is required to capture the relevant information. In this work, we propose a Gaussian process (GP) model with such property by utilising spectral graph wavelets. Using a mother wavelet to define band-pass graph filters in the GP kernel, the model naturally enables a way of aggregating neighbourhood information at different scales. Furthermore, through maximum likelihood optimisation of the GP, the wavelets automatically adapt to the different frequencies in the data, and as a result our model goes beyond capturing low frequency information. We achieve scalability to larger graphs by using a spectrum-adaptive polynomial approximation of the filter function, which is designed to yield a low approximation error in dense areas of the graph spectrum.
Synthetic and real-world experiments demonstrate the ability of our model to infer scales accurately and produce competitive performances against state-of-the-art models in graph-based learning tasks.

## Efficient Bayesian network structure learning via local Markov boundary search

### TL;DR

Provably efficient algorithms for learning directed acyclic graphs without specific distributional assumptions. 

### Abstract

We analyze the complexity of learning directed acyclic graphical models from observational data in general settings without specific distributional assumptions. Our approach is information-theoretic and uses a local Markov boundary search procedure in order to recursively construct ancestral sets in the underlying graphical model. Perhaps surprisingly, we show that for certain graph ensembles, a simple forward greedy search algorithm (i.e. without a backward pruning phase) suffices to learn the Markov boundary of each node. This substantially improves the sample complexity, which we show is at most polynomial in the number of nodes. This is then applied to learn the entire graph under a novel identifiability condition that generalizes existing conditions from the literature. We further illustrate the performance of the algorithm, which is easy to implement, in a simulation study. Our approach is general, works for discrete or continuous distributions without distributional assumptions, and as such sheds light on the minimal assumptions required to efficiently learn the structure of directed graphical models from data.

## Streaming Belief Propagation for Community Detection

### TL;DR

We characterized the fundamental statistical limits of community detection on dynamic graphs. 

### Abstract

The community detection problem requires to cluster the nodes of a network into a small number of well-connected ‘communities’. There has been substantial recent progress in characterizing the fundamental statistical limits of community detection under simple stochastic block models.  However, in real-world applications, the network structure is typically dynamic, with nodes that join over time. In this setting, we would like a detection algorithm to perform only a limited number of updates at each node arrival. While standard voting approaches satisfy this constraint, it is unclear whether they exploit the network information optimally. We introduce a simple model for networks growing over time which we refer to as streaming stochastic block model (StSBM). Within this model, we prove that voting algorithms have fundamental limitations.  We also develop a streaming belief-propagation (STREAMBP)  approach, for which we prove optimality in certain regimes. We validate our theoretical findings on synthetic and real data

## Hierarchical Image Generation via Transformer-based Sequential Patch Selection

### TL;DR

This paper proposes a novel framework to synthesize images from scene graphs.

### Abstract

To synthesize images with preferred objects and rich interactions, a controllable way is to generate the image from a scene graph and a large pool of object crops, where the spatial arrangements of the objects in the image are defined by the scene graph while the object appearances are determined by the retrieved crops from the pool. In this paper, we propose a novel framework with such a semi-parametric generation strategy. First, to encourage the retrieval of mutually compatible crops, we design a sequential selection strategy where the crop selection for each object is determined by the contents and locations of all object crops that have been chosen previously. Such process is implemented via a novel transformer trained with contrastive losses. Second, to generate the final image, our hierarchical generation strategy leverages hierarchical gated convolutions which are employed to synthesize areas not covered by any image crops, and a patch-guided spatially adaptive normalization module (PSANM) which is proposed to guarantee the final generated images complying with the crop appearance and the scene graph. Evaluated on the challenging Visual Genome and COCO-Stuff dataset, our experimental results demonstrate the superiority of our proposed method over existing methods. 

## Neural Networks as Probabilistic Graphical Models: the Continuous Bernoulli Belief Network

### TL;DR

We show that all neural networks are probabilistic graphical models, then propose the continuous Bernoulli belief network, which permits efficient learning by contrastive divergence.

### Abstract

Deep neural networks (DNNs) are the leading machine learning approach for many tasks, but lack the precise semantics and clear probabilistic interpretation of probabilistic graphical models (PGMs). In this paper, we demonstrate that all neural networks are also PGMs, and argue that this view immediately suggests new learning algorithms that take advantage of current high-dimensional sampling methods. We begin by showing that learning by stochastic gradient descent (SGD) in a neural network with sigmoid activations is equivalent, algorithmically, to learning by contrastive divergence in a related binary pairwise Markov network. We then explore the natural correspondence between DNNs and Bayesian Networks, and show that SGD may be viewed as identical to contrastive divergence (CD), which in one normal formulation ignores evidence from the output variables during sampling.  This suggests an alternative variant of CD that does not ignore the outputs during sampling, leading us to propose the continuous Bernoulli belief network, a DNN-inspired PGM that permits efficient learning by CD with Hamiltonian Monte Carlo sampling. Early results demonstrate that this learning procedure converges more quickly than SGD in the corresponding DNN.

## Applicable and Partial Learning of Gaussian Graphical Models without Sparsity Priors

### TL;DR

None

### Abstract

This paper considers learning the underlying graph topology of Gaussian Graphical Models (GGMs) from data. Under the high-dimensional setting, to achieve low sample complexity, many existing graph learning algorithms assume structural constraints such as sparsity to hold. Without prior knowledge of these constraints, the validity of their results is difficult to justify. Our work in this paper aims to do away with these assumptions by developing algorithms for learning degree-bounded GGMs and separable GGMs without any sparsity priors. Our algorithms, which are based only on the knowledge of conditional independence relations in the distribution, require minimal structural assumptions while still achieving low sample complexity, and hence are `applicable'. We show that our algorithms can consistently identify whether a $p$-dimensional GGM is $k$-degree bounded (or strongly $k$-separable) with sample complexity $\Omega(k \log p)$. Moreover, our algorithms demonstrate ‘partial’ learning properties whenever the overall graph is not entirely sparse, in the sense that not all nodes have bounded degree $k$ (or are strongly $k$-separable). In this case, nodes having bounded degree $k$ (or being strongly $k$-separable) can still be identified by our methods. We complement our work with experiments showing that existing algorithms fail even in some simple settings where sparsity assumptions do not hold, whereas our algorithms do not.

## Graph Over-Parameterization: Why the Graph Helps the Training of Deep Graph Convolutional Network

### TL;DR

The convergence rate of the training algorithms of Deep Graph Convolutional Network and how it depends on the graph structure.

### Abstract

We study the convergence rate for the gradient descent for training the graph neural network. It has been widely studied in the literature on the convergence rate of the gradient descent on the deep neural network (DNN), and these works revealed the over-parameterized phenomenon: the gradient descent algorithm can learn the DNN in polynomial time when the width of DNN is sufficiently large. This over-parameterized phenomenon applies to the architectures including the fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet). However, the existing theory does not apply to the graph convolutional  network (GCN) as the network in the GCN brings additional topological structures. It has be heuristically observed in the study of GCN that if the graph structure will help the training of the GCN if it is close to the underlying network of the data. However, there is few theoretical justification of such phenomenon.  In this paper, we give a theoretical analysis to associate the training complexity of GCN  with a novel measurement of the relatedness between  the graph of GCN with the training data, called the ``graph similarity coefficient". In specific, 
  we prove that the convergence of the gradient descent on the GCN depends on not only the number of nodes in the graph but also the ratio between the correlation of the training data and  the leading eigenvector of the graph Laplacian. This novel insight provide the rationale on how the graph in GCN help the training and will provide useful guidance in GCN training in practice.

## Graph Filtration Kernels

### TL;DR

We introduce a general graph kernel framework which tracks features over graph filtrations and show that it increases the expressive power of e.g. the Weisfeiler-Lehman subtree kernel. 

### Abstract

The majority of popular graph kernels is based on the concept of Haussler's $\mathcal{R}$-convolution kernel and defines graph similarities in terms of mutual substructures. In this work, we enrich these similarity measures by considering graph filtrations: Using meaningful orders on the set of edges, which allow to construct a sequence of nested graphs, we can consider a graph at multiple granularities. For one thing, this provides access to features on different levels of resolution. Furthermore, rather than to simply compare frequencies of features in  graphs, it allows for their comparison in terms of when and for how long they exist in the sequences. In this work, we propose a family of graph kernels that incorporate these existence intervals of features. While our approach can be applied to arbitrary graph features, we particularly highlight Weisfeiler-Lehman vertex labels, leading to efficient kernels. We show that using Weisfeiler-Lehman labels over certain filtrations strictly increases the expressive power over the ordinary Weisfeiler-Lehman procedure in terms of deciding graph isomorphism. In fact, this result directly yields more powerful graph kernels based on such features and has implications to graph neural networks due to their close relationship to the Weisfeiler-Lehman method. We empirically validate the expressive power of our graph kernels and show significant improvements over state-of-the-art graph kernels in terms of predictive performance on various real-world benchmark datasets. 

## Federated Graph Learning

### TL;DR

None

### Abstract

Graph Convolutional Networks (GCN) proposed recently have achieved promising results on various graph learning tasks. Federated learning (FL) for GCN training is needed when learning from geo-distributed graph datasets. Existing FL paradigms are inefficient for geo-distributed GCN training since neighbour sampling across geo-locations will soon dominate the whole training process and consume large WAN bandwidth. We derive a practical federated graph learning algorithm, carefully striking the trade-off among GCN convergence error, wall-clock runtime, and neighbour sampling frequencies. Our analysis is divided into two cases according to the budget for neighbour sampling.  In the unconstrained case, we obtain the optimal neighbour sampling interval, that achieves the best trade-off between convergence and runtime;  in the constrained case, we show that determining the optimal sampling interval is actually an online problem and we propose a novel online algorithm with bounded competitive ratio to solve it. Combining the two cases, we propose a unified algorithm to decide the neighbour sampling interval in federated graph learning, and demonstrate its effectiveness with extensive simulation over graph datasets from real applications.   

## Graph Laplacian Networks

### TL;DR

None

### Abstract

The aptitude of Graph Neural Networks (GNNs) for extracting inter-series correlations from evolving graph structures has demonstrated promising results in multivariate time series forecasting. Recent GNN-based forecasting work has relied extensively on the availability of a priori graph topologies designed according to the corresponding application domain, such as road or social networks. However, such prior graphs may not be optimal for mining task-specific graph correlations in real-world applications, since connectivity between graph vertices does not ensure strong data correlations. In addition, prior graphs may introduce bias to the learned representations and may be difficult to build for applications where connectivity depends on human interpretation. As such, we propose to jointly learn the graph Laplacian matrix together with the associated forecasting task. To this end, we introduce a Graph Laplacian Network (GLN) for multivariate time series forecasting based on the graph Laplacian learning paradigm. GLN learns inter-series correlations using one of two interchangeable graph learning modules, based on QR decomposition and incidence matrices, respectively. Our comprehensive experiments on six datasets spanning multiple application domains empirically demonstrate the superiority of GLN over competitive baselines.

## Ensemble Graph Prediction

### TL;DR

None

### Abstract

In many machine learning tasks, models are trained to predict structure data like trees or graphs. For example, in natural language processing, it is very common to parse texts into trees like dependency trees or graphs like abstract meaning representation (AMR). On the other hand, ensemble methods combine predictions from multiple models to create a new one that is more accurate than any individual prediction. In the literature, there are many ensembling techniques proposed for classification or regression problems, however, the problem of ensemble graph prediction has not been studied thoroughly. In this work, we formalize the graph ensemble problem as mining the largest subgraph that is most supported by a collection of graph predictions given by multiple machine learning models. As the problem is NP-Hard, we propose an efficient heuristic technique to approximate the optimal solution. To validate our approach, we carried out experiments in AMR parsing problems. The experimental results demonstrate that the proposed graph ensemble approach can combine strength from the state-of-the-art AMR parsers to create new predictions that are more accurate than any individual model in all sets of benchmark data. The source code is included for reproducibility.

## Convergent Graph Solvers

### TL;DR

CGS is an implicit graph neural network model that constructs the iterative map from the input and solves the fixed point with guaranteed convergence as a forward propagation step. 

### Abstract

We propose the convergent graph solver (CGS), a deep learning method that learns iterative mappings to predict the properties of a graph system at its stationary state (fixed point) with guaranteed convergence. CGS systematically computes the fixed points of a target graph system  and decodes them to estimate the stationary properties of the system without the prior knowledge of existing solvers or intermediate solutions. The forward propagation of CGS proceeds in three steps: (1) constructing the input dependent linear contracting iterative maps, (2) computing the fixed-points of the linear maps, and (3) decoding the fixed-points to estimate the properties. The contractivity of the constructed linear maps guarantees the existence and uniqueness of the fixed points following the Banach fixed point theorem. To train CGS efficiently, we also derive a tractable analytical expression for its gradient by leveraging the implicit function theorem. We evaluate the performance of CGS by applying it to various network-analytic and graph benchmark problems. The results indicate that CGS has competitive capabilities for predicting the stationary properties of graph systems,  irrespective of whether the target systems are linear or non-linear. CGS also shows high performance for graph classification problems where the existence or the meaning of a fixed point is hard to be clearly defined, which highlights the potential of CGS as a general graph neural network architecture.

## Anchor-Siamese GCN Clustering with Adaptive $O(n)$ Bipartite Convolution

### TL;DR

A novel clustering method with fast bipartite graph convolution for general data

### Abstract

As graph convolution networks (GCN) have been extensively studied and graph-based clustering methods perform well in clustering tasks, it is attractive to employ GCNs to augment the graph-based clustering methods. However, given $n$ samples (and $|\mathcal{E}|$ edges if the data is graph-type as social networks), the graph-based methods usually need at least $O(n^2)$ time while the graph convolution requires nearly $O(n^2)$ for a dense graph and $O(|\mathcal{E}|)$ for a sparse one. To tackle this problem and further employ GCN to promote the capacity of graph-based clustering, we propose a novel clustering method with an adaptive $O(n)$ bipartite convolution. As the graph structure is not provided in general, we first show how to transform a non-graph-type dataset into a graph by introducing the generative graph model. Anchors are generated from the original data to construct a bipartite graph such that the computational complexity of graph convolution is reduced from $O(n^2)$ or $O(|\mathcal{E}|)$ to $O(n)$. The succeeding steps for clustering can be easily designed as $O(n)$ operations. Interestingly, the anchors naturally lead to a siamese GCN architecture. The bipartite graph constructed by anchors is updated dynamically to exploit the high-level information behind data, i.e., the bipartite convolution is adaptive.

## Edge Representation Learning with Hypergraphs

### TL;DR

We propose a novel edge representation learning scheme with hypergraphs, which can be further exploited for graph pooling.

### Abstract

Graph neural networks have recently achieved remarkable success in representing graph-structured data, with rapid progress in both the node embedding and graph pooling methods. Yet, they mostly focus on capturing information from the nodes considering their connectivity, and not much work has been done in representing the edges, which are essential components of a graph. However, for tasks such as graph reconstruction and generation, as well as graph classification tasks for which the edges are important for discrimination, accurately representing edges of a given graph is crucial to the success of the graph representation learning. To this end, we propose a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT), which transforms the edges of a graph into the nodes of a hypergraph. This dual hypergraph construction allows us to apply message passing techniques for node representations to edges. After obtaining edge representations from the hypergraphs, we then cluster or drop edges to obtain holistic graph-level edge representations. We validate our edge representation learning method with hypergraphs on diverse graph datasets for graph representation and generation performance, on which our method largely outperforms existing graph representation learning methods. Moreover, our edge representation learning and pooling method also largely outperforms state-of-the-art graph pooling methods on graph classification, not only because of its accurate edge representation learning, but also due to its lossless compression of the nodes and removal of irrelevant edges for effective message passing.

## Lifelong Graph Learning with Feature Correlation

### TL;DR

None

### Abstract

Graph neural networks (GNNs) are powerful models for many graph-structured tasks. Existing models often assume that a complete structure of a graph is available during training, however, in practice, graph-structured data is usually formed in a streaming fashion, so that learning a graph continuously is often necessary. In this paper, we aim to bridge GNN to lifelong learning by converting a graph problem to a regular learning problem, so that GNN is able to inherit the lifelong learning techniques developed for convolutional neural networks (CNNs). To this end, we propose a new graph topology based on feature cross-correlation, called the feature graph. It takes features as new nodes and turns nodes into independent graphs. This successfully converts the original problem of node classification to graph classification, in which the increasing nodes are turned into independent training samples. In the experiments, we demonstrate the efficiency and effectiveness of feature graph networks (FGN) by continuously learning a sequence of classical graph datasets. We also show that FGN achieves superior performance in human action recognition with distributed streaming signals for wearable devices.

## Generating the Graph Gestalt: Kernel-Regularized Graph Representation Learning

### TL;DR

Introduces a graph VAE framework that models both local and global graph structure: regularize adjacency matrix reconstruction with a graph kernel term that measures how well the reconstructed graph matches the global structure of the input graph.

### Abstract

Recent work on graph generative models has made remarkable progress towards generating increasingly realistic graphs, as measured by global graph features such as degree distribution, density, and clustering coefficients. Deep generative models have also made significant advances through better modeling of the local correlations in the graph topology, which have been very useful for predicting unobserved graph components, such as the existence of a link or the class of a node, from nearby observed graph components.  A complete scientific understanding of graph data should address both global and local structure. In this paper, we propose a joint model for both as mutually reinforcing objectives in a graph VAE framework. Global structure is captured by incorporating graph kernels in a probabilistic model whose loss function is closely related to the maximum mean discrepancy (MMD) between the global structures of the reconstructed and the input graphs. The ELBO objective derived from the model regularizes a standard local link reconstruction term with an MMD term. Our experiments demonstrate a significant improvement in the realism of the generated graph structures, typically by 1-2 orders of magnitude of graph structure metrics, compared to leading graph VAE and GAN models. Local link reconstruction improves as well in many cases. 

## Self-Supervised Graph Structure Refinement for Graph Neural Networks

### TL;DR

An efficient and scalable graph structure learning model in pretrain-finetune framework with SOTA performance.

### Abstract

Graph structure learning (GSL), which aims to learn the adjacency matrix for graph neural networks (GNNs), has shown great potential in boosting the performance of GNNs. Most existing works apply a joint learning framework where the estimated adjacency matrix and GNN parameters are optimized for downstream tasks (e.g. semi-supervised node classification). However, this framework suffers from two major drawbacks: First, the learning of the adjacency matrix is essentially a link prediction task, whose goal may largely differ from the goal of the downstream task. The inconsistency of these two goals limits the graph structure learning process to learn the optimal graph structure. Second, the joint learning framework suffers from scalability issues in terms of time and space due to the estimation and optimization of the adjacency matrix. To mitigate these issues, we propose a graph structure refinement (GSR) framework. Specifically, GSR models the graph structure learning problem as a pretrain-finetune process. The pre-training process aims to comprehensively estimate the underlying graph structure by a multi-view contrastive learning framework with both intra- and inter-view link prediction tasks. Then, the graph structure is refined by adding and removing edges according to the edge probability estimation by the pre-trained model. Finally, the GNN for the downstream task is initialized by the pre-trained model and fine-tuned by supervised data. Though unaware of supervised information, the pre-trained model is capable of boosting fine-tuning GNN for downstream task by migrating knowledge and providing refined graph structure. To demonstrate this, extensive experiments are conducted to evaluate the effectiveness (best performance on six benchmark datasets), efficiency, and scalability (13.8$\times$ faster using 32.8% GPU memory compared to the best GSL baseline on Cora) of the proposed model. 

## Rethinking Graph Neural Networks for the Graph Coloring Problem

### TL;DR

We discuss what kinds of designs make a GNN more/less powerful in the graph coloring problem.

### Abstract

The development of graph neural networks (GNNs) stimulated the interest in GNNs for NP-hard problems, while most works apply GNNs for NP-hard problems by empirical intuition and experimental trials. We start from a simple contradiction that a graph coloring problem requires two connected and symmetric nodes to be assigned different colors while an expressively powerful GNN always maps the two nodes to the same node embeddings. To characterize the power of GNNs for the graph coloring problem, we first formalize the discrimination power of GNNs as the capability to assign nodes different colors. Through theoretical analysis on the discrimination power of AC-GNNs, a popular class of GNNs, we identify node pairs that AC-GNNs fail to discriminate and provide corresponding solutions to discriminate them. Furthermore, we show that any AC-GNN is a local coloring method and any local coloring method is non-optimal by exploring the limits of local methods over sparse random graphs, thereby demonstrating the non-optimality of AC-GNNs due to its local property.
Moreover, we discuss the color equivariance of graphs and give conditions for the color equivariant AC-GNN theoretically.
Following the discussions above, we develop a simple architecture for the graph coloring problem which proves to enhance the discrimination power and retain the color equivariance.
We empirically validate our theoretical findings and demonstrate that our model is discriminatively powerful and even comparable with state-of-the-art heuristic algorithms.

## The Vanishing Graph Information in Implicit Graph Neural Networks

### TL;DR

We found a interesting phenomenon named Graph Information Vanishing which is very important while has been ignored. Moreover, we proposed a new method calld GinfoNN to address this problem.

### Abstract

Graph Neural Network (GNNs) has been a great success in the field of graph representation learning. The core idea of GNNs is to enrich node representations by aggregating information from neighbouring nodes. Many previous works focus on introducing various graph information with representational capabilities to improve the quality of aggregation. A class of GNNs that encodes graph information into neighbour node importance through different Learnable Transformation Structures (LTS) to improve the quality of aggregation is called implicit GNNs. We argue that LTS causes the disappearance of the special properties of graph information during training, thus making graph information no longer facilitate learning node representations, a phenomenon referred to as Graph Information Vanishing (GIV). To verify this idea, we replaced the graph information utilized by the five implicit GNNs with random values and conducted thousands of experiments on a seven-node classification benchmark dataset, and were surprised to find that the classification accuracies varied within less than 0.3%. Further, we propose the GinfoNN framework, which treats advanced graph information as an additional supervision signal to constrain the training of GNNs, and effectively solving the problem that graph information cannot be exploited by implicit GNNs due to LTS. Specifically, we make use of discrete graph curvature, which is a good measure of the structural relationship between neighbouring nodes of a node pair, to enhance the generalizability of GNNs. Experimental results show that the classification accuracies of GinfoNN increase by two percentage points over baselines on large and dense Amazon Couputers and Amazon Photo. GinfoNN provides new insights on how GNNs exploit a variety of valuable graph information.

## Efficient Dynamic Graph Representation Learning at Scale

### TL;DR

None

### Abstract

Dynamic graphs with ordered sequences of events between nodes are prevalent in real-world industrial applications such as e-commerce and social platforms. However, representation learning for dynamic graphs has posed great computational challenges due to the time and structure dependency and irregular nature of the data, preventing such models from being deployed to real-world applications. To tackle this challenge, we propose an efficient algorithm, Efficient Dynamic Graph lEarning (EDGE), which selectively expresses certain temporal dependency via training loss to improve the parallelism in computations. We show that EDGE can scale to dynamic graphs with millions of nodes and hundreds of millions of temporal events and achieve new state-of-the-art (SOTA) performance.

## Domain-Adaptive Pre-Training across Graphs

### TL;DR

None

### Abstract

Due to the success of self-supervised learning, recent advances have focused on pre-training graph neural networks (GNNs) by directly learning from the predefined proxy signals and then fine-tuning the learned model on the downstream tasks. However, the current pre-training strategies for GNNs are mostly hand-engineered or ad-hoc, i.e., relying on hyperparameters to balance the importance of heterogeneous proxy signals. As a result, noisy and irrelevant proxy signals may lead to negative transfer and largely limit the reliability of the pre-trained GNNs in the downstream tasks. This paper addresses this limitation from a new perspective, i.e., domain-adaptive graph pre-training. We propose an end-to-end model named MentorGNN that aims to supervise the pre-training process of GNNs across graphs (domains) with diverse structures and disparate feature spaces. To comprehend heterogeneous proxy signals at different granularities (e.g., nodes, edges, subgraphs) of graphs, we propose a curriculum learning paradigm that automatically re-weighs graph signals in order to ensure a good generalization in the downstream tasks. Moreover, we derive a natural and interpretable generalization bound that explicitly considers the cross-graph heterogeneity and graph-signal heterogeneity. Extensive experiments on a wealth of real graphs validate and verify the generalization performance of our approach.

## A Spectral Representation of Networks: The Path of Subgraphs

### TL;DR

A spectral representation of a network by using the spectral information of its subgraphs

### Abstract

Network representation learning has played a critical role in studying networks. One way to study a graph is to focus on its spectrum (i.e., eigenvalue distribution) of its associated matrices. Recent advancements in spectral graph theory show that spectral moments of a network can be used to capture the network structure and various graph properties. However, sometimes networks with different structures or sizes can have the same or close spectral moments, not to mention the existence of the cospectral graphs. To address such problems, we propose a 3D network representation that relies on the spectral information of subgraphs: the Spectral Path, a path connecting the spectral moments of the network and those of its subgraphs of different sizes. The spectral path is interpretable as it can capture relationship between a network and its subgraphs, for which we present a theoretical foundation. We demonstrate the effectiveness of the spectral path in applications such as network visualization, network identification, and graph classification.

## Defending Graph Neural Networks via Tensor-Based Robust Graph Structure Aggregation

### TL;DR

A tensor-based method to improve robustness of graph neural network models against adversarial attacks.

### Abstract

Graph Neural Networks (GNN) have achieved outstanding success in a wide variety of domains and applications. However, they are still vulnerable to unnoticeable perturbations of graph structure specially designed by attackers, which can significantly reduce the performance of GNN models. Developing algorithms to defend GNN models with robust graph structures vaccinating from adversarial attacks still remains a challenging issue. In this paper, we propose a tensor-based framework for GNN models to learn a low-rank robust graph structure from adversarial graph structure by aggregating predefined robust graph structures to enhance the robustness of GNN models. The tensor-based robust graph structure, jointly learned with GNN models, can effectively exploit common low-rank patterns between adversarial graph structure and the predefined robust graph structures while improving node classification performance. Extensive experiments on real-world graph datasets show that the proposed framework effectively mitigates the adverse effects of adversarial attacks and outperforms state-of-the-art defense methods.

## Incomplete Graph Neural Networks via Partial Aggregation Functions

### TL;DR

None

### Abstract

Graph Neural Networks (GNNs) are gaining increasing attention on graph data learning tasks in recent years. However, in many applications, graph may be coming in an incomplete form where attributes of graph nodes are partially unknown/missing. Existing GNNs are generally designed on complete graphs which can not deal with attribute-incomplete graph data directly. To address this problem, we develop a novel partial aggregation based GNNs, named Partial Graph Neural Networks (PaGNNs), for attribute-incomplete graph representation and learning. Our work is motivated by the observation that the neighborhood aggregation function in standard GNNs can be equivalently viewed as the neighborhood reconstruction formulation. Based on it, we define two novel partial aggregation (reconstruction) functions on incomplete graph and derive PaGNNs for incomplete graph data learning. Extensive experiments on several datasets demonstrate the effectiveness and efficiency of the proposed PaGNNs.

## Structural Landmarking and Interaction Modelling: A “SLIM” Network for Graph Classification

### TL;DR

A new framework for graph pooling that allows explicit modelling of graph substructures and their interacting relations.

### Abstract

Graph neural networks are promising architecture for learning and inference with graph-structured data. Yet, how to generate informative, fixed-dimensional features for graphs with varying size and topology can still be challenging. Typically, this is achieved through graph-pooling, which summarizes a graph by compressing all its nodes into a single vector. Is such a “collapsing-style” graph pooling the only choice for graph classification? From complex system's point of view, properties of a complex system arise largely from the interaction among its components . Therefore, we speculate that preserving the interacting relation between parts, instead of pooling them together, could also benefit system-level prediction. To verify it, we propose SLIM, a graph neural network model for Structural Landmarking and Interaction Modelling. The main idea is to compute a set of end-to-end optimizable sub-structure landmarks, so that any input graph can be projected onto these (spatially) local str uctural representatives for a faithful, global characterization. By doing so, explicit interaction between component parts of a graph can be leveraged directly in generating discriminative graph representation. Encouraging results are observed on benchmark datasets for graph classification, demonstrating the value of interaction modelling in the design of graph neural networks.

## Scalable Graph Topology Optimization with Hierarchical Reinforcement Learning

### TL;DR

We propose a novel hierarchical reinforcement learning (HRL) to decompose the quadratic edge searching space using hierarchical structure and submodular maximization for scalable training. 

### Abstract

Graph Neural Networks (GNNs) have been achieved great success in various applications on graph-structured data. However, most of the GNNs process the underlying graph as though it is a ground-truth depiction of the relationship between nodes. But the observed graph is usually derived from noisy data or modelling assumptions. There are several attempts that aim to achieve graph topology optimization or to model the uncertainty of the underlying graph during GNN training. However, they suffer from high computational complexity coming from the quadratic edge search space or assume the observed graph as a random realization from a parametric random graph model which restrict their applicability. In this paper, we focus on the graph topology optimization for GNNs to learn a better underlying graph that better tailors for downstream tasks. We propose a novel hierarchical reinforcement learning (HRL) model for Scalable Graph Topology Optimization (SGTO). SGTO decomposes the quadratic edge searching space into a hierarchy of subproblems for scalable training. Extensive experiments on various real-world graphs demonstrate that SGTO can be easily integrated into various GNN architectures and consistently shows improvement in terms of performance accuracy, scalability and robustness against various kinds of adversarial attacks.  

## Deep Graph Similarity through Linear Attention and Heat Kernel

### TL;DR

We propose a deep learning approach for graph similarity (or matching) problem

### Abstract

The graph similarity (or matching) problem attempts to find node and edge correspondences between graphs. Traditionally, obtaining an exact solution with heuristics is NP-hard and may cause long latency; recently, research employing deep graph architectures has been proposed in finding an approximate solution, which leverages speed and accuracy. Previous works mainly focused on using localized node embeddings to obtain an approximate node alignment. However, investigating only local features cannot reflect the entire graph-level structure; the overall graph topology plays a vital role in determining the edge alignment between graphs. Heat kernels, which depict the probability distributions of graph signals over a graph, effectively assist graph topology exploration. In this work, we present Transformer-based Graph Similarity (TRGSim), a lightweight deep graph matching framework incorporating the graph heat kernel to evaluate the random feature map of graph embedding, and the efficient linear Transformer encoder to measure the similarity between graphs. We also mathematically prove that it is possible to transform a Quadratic Assignment Problem (QAP) of high-order combinatorial nature into a lower dimension problem. Experiments show that TRGSim achieves superior and robust performances and can easily handle matching problems with large graphs.

## Lightweight Data Fusion: Learning Sufficient Statistics with Neural Networks for Efficient Inference

### TL;DR

For multimodal data fusion learn sufficient statistics using neural networks for efficient inference.

### Abstract

We present a novel approach for Bayesian data fusion: combining multiple sources of data to make inferences about a shared latent variables. Our method, lightweight data fusion (LDF), provides an approach for posterior inference even when some data sources lack a well-defined statistical relationship to the latent variables, without the need of explicit labels. LDF combines the interpretability of structured probabilistic graphical models with the expressiveness of neural networks, yielding a modular approach that can be applied when known likelihoods are in the exponential family and impose mild restrictions on model structure. Under LDF, data from sources without well-characterized likelihoods are converted into sufficient statistics w.r.t. the latent variables using neural network transformations, thus enabling the use of standard Bayesian inference procedures. The resulting posteriors are interpretable and enable prediction, uncertainty quantification, and model checking, as we demonstrate on two challenging real-world applications.

## Learning Sparse Fixed-Structure Gaussian Bayesian Networks

### TL;DR

We analyze algorithms to learn Gaussian Bayesian networks with known structure up to bounded error in total variation distance.

### Abstract

Gaussian Bayesian networks (a.k.a. linear Gaussian structural equation models) are widely used to model causal interactions among continuous variables. In this work, we study the problem of learning a fixed-structure Gaussian Bayesian network up to a bounded error in total variation distance. We analyze the commonly used node-wise least squares regression LeastSquares and prove that it has the near-optimal sample complexity. We also propose a new estimator CauchyEst based on some interesting properties of Cauchy random variables, and prove near-optimal sample complexity for polytrees. Experimentally, we show that CauchyEst and its extension CauchyEstGeneral compare favorably to LeastSquares.

## VMAPD: Variational Multi-Agent Policy Diversification

### TL;DR

We propose a new algorithm, denoted as VMAPD, for obtaining diverse solutions for multi-agent tasks.

### Abstract

Recent algorithms designed for multi-agent tasks focus on finding a single (approximately) optimal solution for all the agents. However, in many tasks (e.g., matrix games and transportation dispatching), there may exist more than one optimal solution, while previous algorithms can only converge to one of them. In many practical applications (e.g., game AI design and automatic driving), it is important to develop reasonable agents with diverse behaviors. In this paper, we propose "variational multi-agent policy diversification" (VMAPD), a framework for discovering diverse policies for coordination patterns of multiple agents. By taking advantage of latent variables and exploiting the connection between variational inference and multi-agent reinforcement learning, we derive a tractable evidence lower bound (ELBO) on the trajectories of all agents. Our algorithm uses policy iteration to maximize the derived lower bound and can be simply implemented by adding a pseudo reward during centralized learning. And the trained agents do not need to access the pseudo reward during decentralized execution. We demonstrate the effectiveness of our algorithm on several popular multi-agent testbeds. Experimental results show that VMAPD achieves competitive performance while better diversity compared with state-of-the-art algorithms. 

## Summary Markov Models for Event Sequences

### TL;DR

Novel family of models for event sequences where the probability of observing an event type of interest depends only on a summary of historical occurrences of its influencing set of parent event types.

### Abstract

Datasets involving sequences of different types of events without meaningful time stamps are prevalent in many real-world applications, for instance when extracted from textual corpora. We propose a family of models for modeling such event sequences -- summary Markov models -- where the probability of observing an event type of interest depends only on a summary of historical occurrences of its influencing set of event types. This Markov model family is motivated by Granger causal graphical models for time series, with the important distinction that only one event can occur in a position in an event sequence. We show that a unique minimal influencing set exists for any set of event types of interest and choice of summary function, formulate two specific novel models from the general family and propose a greedy search algorithm for learning them from event sequence data. We conduct an experimental investigation comparing the proposed models with relevant interpretable baselines, and illustrate their knowledge acquisition and discovery capabilities through two case studies involving sequences from text.

## Dual Decomposition of Convex Optimization Layers for Consistent Attention in Medical Images

### TL;DR

None

### Abstract

A key concern in integrating machine learning models in medicine is the ability to interpret their reasoning. 
Popular explainability methods have demonstrated satisfactory results in natural image recognition, yet in medical image analysis, many of these approaches provide partial and noisy explanations. Recently, attention mechanisms have shown compelling results both in their predictive performance and in their interpretable qualities. A fundamental trait of attention is that it leverages salient parts of the input which contribute to the model's prediction. To this end, our work focuses on the explanatory value of attention weight distributions. We propose a multi-layer attention mechanism that enforces consistent interpretations between attended convolutional layers using convex optimization. We apply duality to decompose the consistency constraints between the layers by reparameterizing their attention probability distributions. We further suggest learning the dual witness by optimizing with respect to our objective; thus, our implementation uses standard back-propagation, hence it is highly efficient. While preserving predictive performance, our proposed method leverages weakly annotated medical imaging data and provides complete and faithful explanations to the model's prediction.

## SIMLR: Machine Learning inside the SIR model for COVID-19 forecasting

### TL;DR

We propose an interpretable model that tracks changes in government policies to make accurate predictions on the number of new infections of COVID-19 one to four weeks in advance.

### Abstract

Accurate forecasts of the number of newly infected people during an epidemic are critical for making effective timely decisions. This paper addresses this challenge using the SIMLR model, which incorporates machine learning (ML) into the epidemiological SIR model. For each region, SIMLR tracks the changes in the policies implemented at the government level, which it uses to estimate the time-varying parameters of an SIR model for forecasting the number of new infections 1- to 4-weeks in advance. It also forecasts the probability of changes in those government policies at each of these future times, which is essential for the longer-range forecasts. We applied SIMLR to data from regions in Canada and in the United States, and show that its MAPE (mean average percentage error) performance is as good as SOTA forecasting models, with the added advantage of being an interpretable model. We expect that this approach will be useful not only for forecasting COVID-19 infections, but also in predicting the evolution of other infectious diseases.

## On the Equivalence Between Temporal and Static Equivariant Graph Representations

### TL;DR

Existing work on temporal graph representation learning considers a time-and-graph framework. This work shows that a time-then-graph framework works as well in practice and is theoretically more expressive when GNNs are used.

### Abstract

In this work we formalize the observational task of predicting node attribute evolution in temporal graphs. We show that node representations of temporal graphs can be cast into two distinct frameworks: (a) The most popular approach, which we denote as time-and-graph, where equivariant graph (e.g., GNN) and sequence (e.g., RNN) representations are intertwined to represent the temporal evolution of the graph; and (b) an approach that we denote as time-then-graph, where the sequences describing the node and edge dynamics are represented first (e.g., RNN), then fed as node and edge attributes into a static equivariant graph representation (e.g., GNN) that comes after. Interestingly, time-then-graph representations have an expressivity advantage over time-and-graph representations when both use component GNNs that are not most-expressive (e.g., 1-Weisfeiler-Lehman GNNs). This expressivity advantage reduces any temporal node representation task into a static node representation task, allowing researchers to directly adopt state-of-the-art static graph studies and techniques onto temporal graphs. In real-world datasets, we show that our time-then-graph framework achieves the same prediction performance as state-of-the-art time-and-graph methods. Besides, we introduce a task where this expressivity advantage allows time-then-graph methods to succeed while state-of-the-art time-and-graph methods fail.

## Graph Grafting : Node Saliency-Guided Graph Mixup with Local Structure Preservation

### TL;DR

None

### Abstract

Graph-structured datasets usually have irregular graph sizes and connectivities, rendering recent data augmentation techniques, such as Mixup, impractical to be applied. To tackle this problem, we present the first Mixup-like graph augmentation method called Graph Grafting for graph classification tasks. Our method identifies the sub-structures, well-defined on various scales of the graph, as mixing units that can preserve the local information. As mixing approaches without consideration of the context are prone to generate noisy samples, our method explicitly employs node saliency information to select meaningful subgraphs and adaptively determine the labels. We extensively validate our method with diverse GNN architectures on multiple graph classification benchmark datasets from a wide range of graph domains including large-scale datasets. Experimental results show the consistent superiority of our method over other basic data augmentation baselines. We also demonstrate that Graph Grafting enhances the performance in terms of robustness and model calibration.

## Multi-view Contrastive Graph Clustering

### TL;DR

This paper develops a clustering method for multi-view  attributed graph data. It applied graph filtering to obtain a good representation and contrastive regularizer to achieve a high quality graph.

### Abstract

With the explosive growth of information technology, multi-view graph data have become increasingly prevalent and valuable. Most existing multi-view clustering techniques either focus on the scenario of multiple graphs or multi-view attributes. In this paper, we propose a generic framework to cluster multi-view attributed graph data. Specifically, inspired by the success of contrastive learning, we propose multi-view contrastive graph clustering (MCGC) method to learn a consensus graph since the original graph could be noisy or incomplete and is not directly applicable. Our method composes of two key steps: we first filter out the undesirable high-frequency noise while preserving the graph geometric features via graph filtering and obtain a smooth representation of nodes; we then learn a consensus graph regularized by graph contrastive loss. Results on several benchmark datasets show the superiority of our method with respect to state-of-the-art approaches. In particular, our simple approach outperforms existing deep learning-based methods.

## Learning Graph Representation without Pooling

### TL;DR

None

### Abstract

Graph pooling is one way that applies down-sampling and summarizes information of nodes to provide a unified graph-level representation for graph-structured data. However, graph pooling inevitably loses structural information in the downsizing and coarsening process. Moreover, it is hard to extend existing pooling methods to sparse and large graph-structured data. In this work, we abandon the graph pooling architecture and propose a new method named Global Graph Representation Network (GGRN) that learns a graph-level representation for different graphs from the perspective of data dimension and degree distribution. GGRN utilizes Graph Convolutional Network (GCN) to extract the feature matrix for each graph and multiplies the transpose of the feature matrix by the feature matrix itself to extract a unified transpose feature representation. Moreover, we introduce a unified degree feature representation according to the degree distribution of graphs. Then, GGRN concatenates the two feature representations and computes the outputs for input graphs. In this way, GGRN guarantees a unified representation and retains global information for different graphs. Moreover, because GGRN focuses on the dimension of nodes, it is without processing for nodes and not sensitive to the number of nodes. Experimental results on multiple graph classification benchmarks demonstrate the effectiveness of the proposed GGRN.

## Continuous-Time Dynamic Graph Forecasting

### TL;DR

A continuous-time graph forecasting method, extending neural ODE to the graph domain.

### Abstract

Dynamic graph forecasting has found a wide range of applications including social media, recommendation systems, and computational finance. However, existing dynamic graph models typically focus on discrete-time dynamic graphs, treating dynamic graphs as temporally discrete graph snapshots. We argue that such discrete treatment is inadequate for capturing the underlying dynamics which are intrinsically continuous. To overcome such deficiency, we extend fully connected neural ordinary differential equations (FC-NODE) to graph-connected neural ordinary differential equations (GNODE), which considers graph structures in the input space, output space, and the transition in the latent space. Experiments show that our GNODE naturally captures the continuous-time dynamics in graph sequences and consistently outperforms state-of-the-art graph forecasting methods.

## Topological Relational Learning on Graphs

### TL;DR

We introduce a new local topological representation of graph to enhance robustness of Graph Neural Networks in classification tasks and derive its theoretical guarrantees

### Abstract

Graph neural networks (GNNs) have emerged as a powerful tool for graph classification and representation learning. However, GNNs tend to suffer from over-smoothing problems and are vulnerable to graph perturbations. To address these challenges, we propose a novel topological neural framework of topological relational inference (TRI) which allows for integrating higher-order graph information to GNNs and for systematically learning a local graph structure. The key idea is to rewire the original graph by using the persistent homology of the small neighborhoods of the nodes and then to incorporate the extracted topological summaries as the side information into the local algorithm. As a result, the new framework enables us to harness both the conventional information on the graph structure and information on higher order topological properties of the graph. We derive theoretical properties on stability of the new local topological representation of the graph and discuss its implications on the graph algebraic connectivity. The experimental results on node classification tasks demonstrate that the new TRI-GNN outperforms all 14 state-of-the-art baselines on 6 out 7 graphs and exhibit higher robustness to perturbations, yielding
up to 10\% better performance under noisy scenarios.

## From Canonical Correlation Analysis to Self-supervised Graph Neural Networks

### TL;DR

Propose a Canonical Correlation Analysis-based method for self-supervised representation learning on graphs

### Abstract

We introduce a conceptually simple yet effective model for self-supervised representation learning with graph data. It follows the previous methods that generate two views of an input graph through data augmentation. However, unlike contrastive methods that focus on instance-level discrimination, we optimize an innovative feature-level objective inspired by classical Canonical Correlation Analysis. Compared with other works, our approach requires none of the parameterized mutual information estimator, additional projector, asymmetric structures, and most importantly, negative samples which can be costly. We show that the new objective essentially 1) aims at discarding augmentation-variant information by learning invariant representations, and 2) can prevent degenerated solutions by decorrelating features in different dimensions. Our theoretical analysis further provides an understanding for the new objective which can be equivalently seen as an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method performs competitively on seven public graph datasets.

## Counterfactual Graph Learning for Link Prediction

### TL;DR

Design a novel method that uses counterfactual inference to improve link prediction on graphs.

### Abstract

Learning to predict missing links is important for many graph-based applications. Existing methods were designed to learn the observed association between two sets of variables: (1) the observed graph structure and (2) the existence of link between a pair of nodes. However, the causal relationship between these variables was ignored and we visit the possibility of learning it by simply asking a counterfactual question: "would the link exist or not if the observed graph structure became different?" To answer this question by causal inference, we consider the information of the node pair as context, global graph structural properties as treatment, and link existence as outcome. In this work, we propose a novel link prediction method that enhances graph learning by the counterfactual inference. It creates counterfactual links from the observed ones, and our method learns representations from both of them. Experiments on a number of benchmark datasets show that our proposed method achieves the state-of-the-art performance on link prediction.

## Equivariant Neural Network for Factor Graphs

### TL;DR

We propose two types of Equivariant Neural Network to perform inference on factor graphs.

### Abstract

Several indices used in a factor graph data structure can be permuted without changing the underlying probability distribution. An algorithm that performs inference on a factor graph should ideally be equivariant or invariant to permutations of global indices of nodes, variable orderings within a factor, and variable assignment orderings. However, existing neural network-based inference procedures fail to take advantage of this inductive bias. In this paper, we precisely characterize these isomorphic properties of factor graphs and propose two inference models: Factor-Equivariant Neural Belief Propagation (FE-NBP) and Factor-Equivariant Graph Neural Networks (FE-GNN). FE-NBP is a neural network that generalizes BP and respects each of the above properties of factor graphs while FE-GNN is an expressive GNN model that relaxes an isomorphic property in favor of greater expressivity. Empirically, we demonstrate on both real-world and synthetic datasets, marginal inference and MAP inference, that FE-NBP and FE-GNN together cover a range of sample complexity regimes: FE-NBP achieves state-of-the-art performance on small datasets while FE-GNN achieves state-of-the-art performance on large datasets.

## GraphEBM: Molecular Graph Generation with Energy-Based Models

### TL;DR

Generating molecular graphs in random, goal-directed, and compositional manners using energy-based models.

### Abstract

Molecular graph generation is an emerging area of research with numerous applications. This problem remains challenging as molecular graphs are discrete, irregular, and permutation invariant to node order. Notably, most existing approaches fail to guarantee the intrinsic property of permutation invariance, resulting in unexpected bias in generative models. In this work, we propose GraphEBM to generate molecular graphs using energy-based models. In particular, we parameterize the energy function in a permutation invariant manner, thus making GraphEBM permutation invariant. We train the energy function by maximum likelihood training with MCMC and generate molecular graphs according to the trained energy function. Furthermore, to generate molecules with a specific desirable property, we propose a simple yet effective strategy, which pushes down energies with flexible degrees according to the properties of corresponding molecules. Finally, we explore the use of GraphEBM for generating molecules with multiple objectives in a compositional manner. Comprehensive experimental results on random, goal-directed, and compositional generation tasks demonstrate the effectiveness of our method.

## Maximum Entropy Weighted Independent Set Pooling for Graph Neural Networks

### TL;DR

None

### Abstract

In this paper, we propose a novel pooling layer for graph neural networks based on maximizing the mutual information between the pooled graph and the input graph. Since the maximum mutual information is difficult to compute, we employ the Shannon capacity of a graph as an inductive bias to our pooling method. More precisely, we show that the input graph to the pooling layer can be viewed as a representation of a noisy communication channel. For such a channel, sending the symbols belonging to an independent set of the graph yields a reliable and error-free transmission of information. We show that reaching the maximum mutual information is equivalent to finding a maximum weight independent set of the graph where the weights convey entropy contents. Through this communication theoretic standpoint, we provide a distinct perspective for posing the problem of graph pooling as maximizing the information transmission rate across a noisy communication channel, implemented by a graph neural network. We evaluate our method, referred to as Maximum Entropy Weighted Independent Set Pooling (MEWISPool), on graph classification tasks and the combinatorial optimization problem of the maximum independent set. Empirical results demonstrate that our method achieves the state-of-the-art and competitive results on graph classification tasks and the maximum independent set problem in several benchmark datasets.

## InfoGCL: Information-Aware Graph Contrastive Learning

### TL;DR

We study how graph information is transformed and transferred during the contrastive learning process, and propose an information-aware graph contrastive learning framework called InfoGCL.

### Abstract

Various graph contrastive learning models have been proposed to improve the performance of tasks on graph datasets in recent years. While effective and prevalent, these models are usually carefully customized. In particular, despite all recent work create two contrastive views, they differ in a variety of view augmentations, architectures, and objectives. It remains an open question how to build your graph contrastive learning model from scratch for particular graph tasks and datasets. In this work, we aim to fill this gap by studying how graph information is transformed and transferred during the contrastive learning process, and proposing an information-aware graph contrastive learning framework called InfoGCL. The key to the success of the proposed framework is to follow the Information Bottleneck principle to reduce the mutual information between contrastive parts while keeping task-relevant information intact at both the levels of the individual module and the entire framework so that the information loss during graph representation learning can be minimized. We show for the first time that all recent graph contrastive learning methods can be unified by our framework. Based on theoretical and empirical analysis on benchmark graph datasets, we show that InfoGCL achieves state-of-the-art performance in the settings of both graph classification and node classification tasks.

## Deeper Insights into the Spectral Domain: Spectrum-adapted Graph Wavelet Network

### TL;DR

The paper proposes two graph neural networks based on the spectral graph wavelets and the key feature of the proposed model is that the spectral filter is adaptable to the spectrum of the graph (distribution of eigenvalues of graph Laplacian).

### Abstract

Graph convolution is the key to most Graph Neural Networks (GNNs) and is usually summarized as the feature aggregation operation. However, the spectral density differences between various graphs have not been sufficiently investigated in the feature aggregation design. In this paper, we first provide a unified framework for graph convolutions from the perspective of spectral graph wavelets to understand the close connection between the feature aggregation and the spectral filter theoretically. Based on this view, we propose the spectrum-adapted graph wavelet network (SAWNet), which uses the multi-scale technique combined with the asymptotic-decreasing spectral filter to adapt the spectral density of the given graph. Moreover, we propose the spectrum-adapted learnable graph wavelet network (SAWLNet) to adapt the spectrum automatically by approximating the wavelets using polynomials that incorporate the spectral density information. Experimental results demonstrate that our models outperform or match the state-of-the-art baselines on graph node classification tasks.

## Self-supervised Graph Anomaly Detection via Contrastive Learning

### TL;DR

A contrastive learning GNN-based algorithm for graph anomaly detection

### Abstract

Anomaly detection on attributed networks attracts attention recently. Due to the lack of labels for the graphs, self-supervised contrastive learning methods have shown promising results on graph embedding. However, contrastive learning methods have not been applied to graph anomaly detection yet. So in this work, we propose a contrastive learning-based algorithm for graph anomaly detection. Moreover, existing graph anomaly detection approaches apply graph embedding first and do clustering for anomaly detection afterward. In this case, the graph embedding model and cluster are independent. The proposed algorithm replaces the cluster with an anomaly score calculation based on the well-trained discriminator in contrastive learning. Thus, the graph embedding model is trained with a precise anomaly detection-aware target. Experimental results show that the proposed framework outperforms the baseline methods on benchmark datasets. 

## Disentangled Contrastive Learning on Graphs

### TL;DR

We introduce the Disentangled Graph Contrastive Learning (DGCL) method, which is able to learn disentangled graph-level representations with self-supervision.

### Abstract

Recently, self-supervised learning for graph neural networks (GNNs) has attracted considerable attention because of their notable successes in learning the representation of graph-structure data. However, the formation of a real-world graph typically arises from the highly complex interaction of many latent factors. The existing self-supervised learning methods for GNNs are inherently holistic and neglect the entanglement of the latent factors, resulting in the learned representations suboptimal for downstream tasks and difficult to be interpreted. Learning disentangled graph representations with self-supervised learning poses great challenges and remains largely ignored by the existing literature. In this paper, we introduce the Disentangled Graph Contrastive Learning (DGCL) method, which is able to learn disentangled graph-level representations with self-supervision. In particular, we first identify the latent factors of the input graph and derive its factorized representations. Each of the factorized representations describes a latent and disentangled aspect pertinent to a specific latent factor of the graph. Then we propose a novel factor-wise discrimination objective in a contrastive learning manner, which can force the factorized representations to independently reflect the expressive information from different latent factors. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of our method against several state-of-the-art baselines, which achieves a new SOTA in self-supervised graph representation learning on nine graph classification datasets and one new Open Graph Benchmark.

## Revisiting Virtual Nodes in Graph Neural Networks for Link Prediction

### TL;DR

We propose new methods for extending graph neural networks with virtual nodes for link prediction.

### Abstract

It is well known that the graph classification performance of graph neural networks often improves by adding an artificial virtual node to the graphs, which is connected to all nodes in the graph. Intuitively, the virtual node provides a shortcut for message passing between nodes along the graph edges. Surprisingly, the impact of virtual nodes with other problems is still an open research question. 

In this paper, we adapt the concept of virtual nodes to the link prediction scenario, where we usually have much larger, often dense, and more heterogeneous graphs. In particular, we use multiple virtual nodes per graph and graph-based clustering to determine the connections to the graph nodes. We also investigate alternative clustering approaches (e.g., random or more advanced) and compare to the original model with a single virtual node. We conducted extensive experiments over different datasets of the Open Graph Benchmark (OGB) and analyze the results in detail. We show that our virtual node extensions yield rather stable performance increases and allow standard graph neural networks to compete with complex state-of-the-art models, as well as with the models leading the OGB leaderboards.

## Reconstruction for Powerful Graph Representations

### TL;DR

The paper explores graph reconstruction in the context of graph representation learning

### Abstract

Graph neural networks (GNNs) have limited expressive power, failing to represent many graph classes correctly. While more expressive graph representation learning (GRL) alternatives can distinguish some of these classes, they are significantly harder to implement, may not scale well, and have not been shown to outperform well-tuned GNNs in real-world tasks. Thus, devising simple, scalable, and expressive GRL architectures that also achieve real-world improvements remains an open challenge. In this work, we show the extent to which graph reconstruction---reconstructing a graph from its subgraphs---can mitigate the theoretical and practical problems currently faced by GRL architectures. First, we leverage graph reconstruction to build two new classes of expressive graph representations. Secondly, we show how graph reconstruction boosts the expressive power of any GNN architecture while being a (provably) powerful inductive bias for invariances to vertex removals. Empirically,  we show how reconstruction can boost GNN's expressive power---while maintaining its invariance to permutations of the vertices---by solving seven graph property tasks not solvable by the original GNN. Further, we demonstrate how it boosts state-of-the-art GNN's performance across nine real-world benchmark datasets.

## Meta-Contrast for Graph Representation Learning

### TL;DR

This paper provides a meta-learning mechanism to unsupervisedly learn graph contrastive coding.

### Abstract

Contrastive learning has been recently applied to graph representation learning and obtained promising results. To make contrastive learning effective for representation learning on graphs, a key is to construct ``desired'' augmentations of each input graph as its positive samples such that along with negative samples (dissimilar graphs to the input graph) a contrastive learner can be trained to learn more robust and discriminative features. A ``desired'' augmented graph is expected to have the following properties: being similar to the input graph and containing difference to a certain extent to the input graph. However, existing contrastive methods on graph data fail to generate graph augmentations with the aforementioned properties. To address this issue, in this paper, we propose a meta-learning framework for contrastive representation learning on graphs, where rather than using a handcrafted graph augmentation module a meta-learner is trained by converting the desired properties into constraints to generate augmented graphs adaptively. The whole meta-learning framework can be trained in an end-to-end manner. We conduct extensive experiments on several benchmark datasets compared with state-of-the-art baselines to demonstrate the superior performance of our proposed framework.

## CAGCN: Convolutional Network on Graphs with Capsule Adaptive Attention Mechanism

### TL;DR

None

### Abstract

From the perspective of the spatial domain, Graph Convolutional Network (GCN) is essentially a process of iteratively aggregating neighbor nodes. However, every graph convolution operation will cause the potential loss of relevant node information. To tackle this problem and enhance the adaptability of GCN to graph data, we propose a capsule graph neural network based on an adaptive attention mechanism named CAGCN. Inspired by CapsNet, the capsule vector can save node information and reserve the topological structure attribute of its space. The capsule idea can make up for the lack of spatial information caused by the basic graph convolution operations. We design various GCN connection methods to extract multi-scale node features from different GCN layers. We also present a graph adaptive attention mechanisms to explore the context information in different global GCN layers, so as to effectively improve the next dynamic routing connection and the final graph classification. Our extensive evaluations with 13 graph-structured datasets demonstrate that CAGCN has a powerful mechanism that operates to capture macroscopic properties of the whole graph by data-driven. Meanwhile, experiments show that our proposed algorithm achieves either state-of-the-art or competitive results across all the datasets.

## Multi-View Node Pruning for Accurate Graph Representation

### TL;DR

Sophisticated graph pruning technique with Multi-view framework and reconstruction-based node scoring for graph representation.

### Abstract

Graph pooling, which compresses a whole graph into a smaller coarsened graph, is an essential component of graph representation learning. To efficiently compress a given graph,  graph pooling methods often drop their nodes with attention-based scoring with the task loss. However, this often results in simply removing nodes with lower degrees without consideration of their feature-level relevance to the given task. To fix this problem, we propose a Multi-View Pruning (MVP), a graph pruning method based on multi-view framework and reconstruction loss. Given a graph, MVP first constructs multiple graphs for different views either by utilizing the predefined modalities or by randomly partitioning the input features, to consider the importance of each node in diverse perspectives. Then, it learns the score for each node by considering both the reconstruction and the task loss. MVP can be incorporated with any hierarchical pooling frameworks to score the nodes. We validate MVP on multiple benchmark datasets by coupling it with two graph pooling methods, and show that it significantly improves the performance of the base graph pooling method, outperforming all baselines. Further analysis shows that both the encoding of multiple views and the consideration of reconstruction loss are the key to the success of MVP, and that it indeed identifies nodes that are less important according to domain knowledge.

## Training Robust Graph Neural Networks with Topology Adaptive Edge Dropping

### TL;DR

None

### Abstract

Graph neural networks (GNNs) are processing architectures that exploit graph structural information to model representations from network data. Despite their success, GNNs suffer from sub-optimal generalization performance given limited training data, referred to as over-fitting. This paper proposes Topology Adaptive Edge Dropping (TADropEdge) method as an adaptive data augmentation technique to improve generalization performance and learn robust GNN models. We start by explicitly analyzing how random edge dropping increases the data diversity during training, while indicating i.i.d. edge dropping does not account for graph structural information and could result in noisy augmented data degrading performance. To overcome this issue, we consider graph connectivity as the key property that captures graph topology. TADropEdge incorporates this factor into random edge dropping such that the edge-dropped subgraphs maintain similar topology as the underlying graph, yielding more satisfactory data augmentation. In particular, TADropEdge first leverages the graph spectrum to assign proper weights to graph edges, which represent their criticality for establishing the graph connectivity. It then normalizes the edge weights and drops graph edges adaptively based on their normalized weights. Besides improving generalization performance, TADropEdge reduces variance for efficient training and can be applied as a generic method modular to different GNN models. Intensive experiments on real-life and synthetic datasets corroborate theory and verify the effectiveness of the proposed method.

## Bayesian Link Prediction with Deep Graph Convolutional Gaussian Processes

### TL;DR

This paper presents a Gaussian process model for prediction of missing links in a graph.

### Abstract

Link prediction aims to reveal missing edges in a graph. We introduce a deep graph convolutional Gaussian process model for this task, which addresses recent challenges in graph machine learning with oversmoothing and overfitting. Using simplified graph convolutions, we transform a Gaussian process to leverage the topological information of the graph domain.
To scale the Gaussian process model to larger graphs, we introduce a variational inducing point method that places pseudo-inputs on a graph-structured domain. Multiple Gaussian processes are assembled into a hierarchy whose structure allows skipping convolutions and thus counteracting oversmoothing. The proposed model represents the first Gaussian process for link prediction that makes use of both node features and topological information. We evaluate our model on multiple graph data sets with up to thousands of nodes and report consistent improvements over competitive link prediction approaches.

## Optimal Transport Graph Neural Networks

### TL;DR

We leverage optimal transport geometry for learning prototype graph neural networks.

### Abstract

Current graph neural network (GNN) architectures naively average or sum node embeddings into an aggregated graph representation---potentially losing structural or semantic information. We here introduce OT-GNN, a model that computes graph embeddings using parametric prototypes that highlight key facets of different graph aspects. Towards this goal, we  successfully combine optimal transport (OT) with parametric graph models. Graph representations are obtained from Wasserstein distances between the set of GNN node embeddings and ``prototype'' point clouds as free parameters. We theoretically prove that, unlike traditional sum aggregation, our function class on point clouds satisfies a fundamental universal approximation theorem. Empirically, we address an inherent collapse optimization issue by proposing a noise contrastive regularizer to steer the model towards truly exploiting the OT geometry. Finally, we outperform popular methods on several molecular property prediction tasks, while exhibiting smoother graph representations.

## Self-augmented Graph Contrastive Representation Learning

### TL;DR

A novel research aims at building a self-distilling graph contrastive learning framework with self-augmented views.

### Abstract

This paper studies learning node representations with graph neural networks (GNNs) for unsupervised scenario. Specifically, we derive a theoretical analysis and provide an empirical demonstration about the non-steady performance of GNNs over different graph datasets, when the supervision signals are not appropriately defined. The performance of GNNs depends on both the node feature smoothness and the locality of graph structure. To smooth the discrepancy of node proximity measured by graph topology and node feature, we proposed SAIL - a novel Self-augmented graph contrastive learning framework, with two complementary self-distilling regularization modules, i.e., intra- and inter-graph knowledge distillation. We demonstrate the competitive performance of SAIL on a variety of graph applications. Even with a single GNN layer, SAIL has consistently competitive or even better performance on various benchmark datasets, comparing with state-of-the-art baselines.

## GraphSAD: Learning Graph Representations with Structure-Attribute Disentanglement

### TL;DR

This work seeks to learn structure-attribute disentangled node/graph representations and measure such disentanglement quantitatively.

### Abstract

Graph Neural Networks (GNNs) learn effective node/graph representations by aggregating the attributes of neighboring nodes, which commonly derives a single representation mixing the information of graph structure and node attributes. However, these two kinds of information might be semantically inconsistent and could possess different extent of importance for different tasks. In this paper, we aim at learning node/graph representations with Structure-Attribute Disentanglement (GraphSAD). We propose to disentangle graph structure and node attributes into two distinct sets of representations, and such a disentanglement can be done in either the input or the embedding space. We further design a metric to quantitatively measure such a disentanglement. Extensive experiments show that our approach can indeed disentangle the semantics of graph structure and node attributes, and it achieves superior performance on both node and graph classification tasks.

## Extracting the Brain's Cognitive Map: SLAM on Neural Population Activity Data

### TL;DR

We present a system capable of taking data from a rats brain as it moves around a map and generating a graphical representation of that map.

### Abstract

Simultaneous localisation and mapping (SLAM) algorithms are commonly used in robotic systems for learning maps of novel environments. Brains also appear to learn maps, but the mechanisms are not known and it is unclear how to infer these maps from neural activity data. We present a method for performing SLAM using only population activity (local field potential, LFP) data simultaneously recorded from three brain regions in rats: hippocampus, prefrontal cortex, and parietal cortex. This system uses a convolutional neural network (CNN) to decode velocity and familiarity information from wavelet scalograms of neural local field potential data recorded from rats as they navigate a 2D maze. The CNN's output drives a RatSLAM-inspired architecture, powering an attractor network which performs path integration plus a separate system which performs `loop closure' (detecting previously visited locations and correcting map aliasing errors). Together, these three components can construct faithful representations of the environment while simultaneously tracking the animal's location. This is the first demonstration of inference of a spatial map from brain recordings. Our findings will facilitate a better understanding of the role of cognitive maps in navigation and decision making.

## Graph Neural Networks Meet with Distributed Graph Partitioners and Reconciliations

### TL;DR

We propose a hybrid-cut partitioner with model reconciliations for distributed GNNs training.

### Abstract

Graph neural networks (GNNs) have shown great success in various applications. As real-world graphs are large, training GNNs in distributed systems is desirable. In current training schemes, their edge partition strategies have a strong impact on the performance of GNNs for the unbalanced influence of high-degree nodes and the damaged neighbor integrity of low-degree nodes. Meanwhile, lack of reconciliations of high-degree anchor (or overlapped) nodes leads to converging up and down across workers. In this work, we design MARS, a suitable framework for distributed GNN training. We propose an influence-balancing and locality-preserving edge partitioner to adapt distributed GNNs training without compromising the accuracy by following an owner-compute rule (each partition performs all the computations involving data that it owns). And then knowledge distillation and contrastive learning are used to reconcile the fusion of local models and boost convergence. We show in extensive empirical experiments on three large-scale graph datasets (Reddit, Amazon, and OGB-Products) that MARS achieves 2x speedup of convergence and get absolute up 3.97% performance improvement with increasing the influence balance of high-degree nodes and the locality of low-degree nodes by 4.87% and 17.78% respectively on average.

## Graph bottlenecks and curvature-based rewiring

### TL;DR

None

### Abstract

Most graph neural networks (GNNs) use the message passing paradigm, in which the node features are propagated on the input graph.  Recent works pointed to the bottleneck and ‘over-squashing’ phenomena arising in certain graph topologies where the number of neighbors grows rapidly with the number of hops and tasks relying on long-distance interactions, which limit the efficiency of message passing. We provide a precise description of the bottleneck phenomenon in GNNs and analyze how it is affected by local changes to the graph topology. For this purpose, we introduce a new notion of edge-based curvature that accounts for combinatorial structures such as triangles and 4-cycles. We prove that our curvature represents a sharp lower bound for the standard Ollivier curvature while being more computationally tractable.  We link negatively-curved edges to graph bottlenecks and propose a curvature-based graph rewiring process to reduce this phenomenon. In comparison with the one we are proposing in this paper, we finally prove that a recently proposed random-walk based graph rewiring may generally not be able to remove graph-bottlenecks.

## Graph Reordering for Cache-Efficient Near Neighbor Search

### TL;DR

Graph-based near neighbor search queries can be sped up by up to 40% by changing the relative locations of nodes in memory (graph reordering).

### Abstract

Graph search is one of the most successful algorithmic trends in near neighbor search. Several of the most popular and empirically successful algorithms are, at their core, a simple walk along a pruned near neighbor graph. Such algorithms consistently perform at the top of industrial speed benchmarks for applications such as embedding search. However, graph traversal applications often suffer from poor memory access patterns, and near neighbor search is no exception to this rule. Our measurements show that popular search indices such as the hierarchical navigable small-world graph (HNSW) can have poor cache miss performance. To address this problem, we apply graph reordering algorithms to near neighbor graphs. Graph reordering is a memory layout optimization that groups commonly-accessed nodes together in memory. We present exhaustive experiments applying several reordering algorithms to a leading graph-based near neighbor method based on the HNSW index. We find that reordering improves the query time by up to 40%, and we demonstrate that the time needed to reorder the graph is negligible compared to the time required to construct the index.

## Knowledge Sheaves: A Sheaf-Theoretic Framework for Knowledge Graph Embedding

### TL;DR

None

### Abstract

Knowledge graph embedding involves learning representations of entities---the vertices of the graph---and relations---the edges of the graph---such that the resulting representations encode the known factual information represented by the knowledge graph are internally consistent and can be used in the inference of new relations. We show that knowledge graph embedding is naturally expressed in the topological and categorical language of \textit{cellular sheaves}: learning a knowledge graph embedding corresponds to learning a \textit{knowledge sheaf} over the graph, subject to certain constraints. In addition to providing a generalized framework for reasoning about knowledge graph embedding models, this sheaf-theoretic perspective admits the expression of a broad class of prior constraints on embeddings and offers novel inferential capabilities. We leverage the recently developed spectral theory of sheaf Laplacians to understand the local and global consistency of embeddings and develop new methods for reasoning over composite relations through harmonic extension with respect to the sheaf Laplacian. We then implement these ideas to highlight the benefits of the extensions inspired by this new perspective.

## CC-GCN: A Community and Contraction-based Graph Convolutional Network

### TL;DR

None

### Abstract

Graph Convolutional Networks (GCNs) have been attracting much research interest due to their effective applications on graph-structured data. Despite the effectiveness, due to the data dependency, GCNs are confronted with the neighborhood explosion and over-smoothing problems. Many sampling methods have been proposed to solve the neighborhood explosion problem to boost the efficiency. However, the samplers are often complicated and reach a bottleneck when GCNs go deeper. Moreover, the over-smoothing problem prevents GCNs from exploring more distant neighborhood effectively. In this paper, we present a Community and Contraction based Graph Convolutional Network (CC-GCN), which leverages community and contraction to boost the time and space efficiency of GCNs. CC-GCN first performs graph contraction and retrieves detected communities as super nodes. The super nodes are connected according to a similarity function to obtain an informative community contracted graph (CC-Graph). The representations of the super nodes in the CC-Graph are then learned, which are used to reconstruct the representations of the original nodes. CC-GCN can explore larger and farther local structures without explicitly using more convolutional layers, alleviating the over-smoothing problem. Since CC-GCN conducts the costly training on a much smaller contracted graph, the efficiency is boosted significantly. Most importantly, we have proved that the information loss of node representations obtained from the CC-Graph compared to that of the original graph is bounded. The efficiency boost and the effectiveness is verified by extensive experimental studies.

## Autobahn: Automorphism-based Graph Neural Nets

### TL;DR

We introduce a family of graph neural networks that use convolutions against the automorphism group of local subgraphs.  This generalizes message passing neural networks and leads to flexible, geometrically intuitive convolutions.

### Abstract

We introduce Automorphism-based graph neural networks (Autobahn), a new family of graph neural networks. In an Autobahn, we decompose the graph into a collection of subgraphs and apply local convolutions that are equivariant to each subgraph's automorphism group. Specific choices of local neighborhoods and subgraphs recover existing architectures such as message passing neural networks. Our formalism also encompasses novel architectures: as an example, we introduce a graph neural network that decomposes the graph into paths and cycles. The resulting convolutions reflect the natural way that parts of the graph can transform, preserving the intuitive meaning of convolution without sacrificing global permutation equivariance. We validate our approach by applying Autobahn to molecular graphs, where it achieves state-of-the-art results.

## Multiresolution Graph Variational Autoencoder

### TL;DR

Multiresolution Graph VAE based on hierarchical generative models for graph generation, molecular generation, and unsupervised graph representation learning.

### Abstract

In this paper, we propose Multiresolution Graph Networks (MGN) and Multiresolution Graph Variational Autoencoders (MGVAE) to learn and generate graphs in a multiresolution and equivariant manner. At each resolution level, MGN employs higher order message passing to encode the graph while learning to partition it into mutually exclusive clusters and coarsening into a lower resolution. MGVAE constructs a hierarchical generative model based on MGN to variationally autoencode the hierarchy of coarsened graphs. Our proposed framework is end-to-end permutation equivariant with respect to node ordering. Our methods have been successful with several generative tasks including link prediction on citation graphs, unsupervised molecular representation learning to predict molecular properties, molecular generation, general graph generation and graph-based image generation.

## BernNet: Learning Arbitrary Graph Spectral Filters via Bernstein Approximation

### TL;DR

We propose BernNet, a graph neural network that interprets and learns an arbitrary spectral filter via Bernstein polynomial approximation.

### Abstract

Many representative graph neural networks, $e.g.$, GPR-GNN and ChebyNet, approximate graph convolutions with graph spectral filters. However, existing work either applies predefined filter weights or learns them without necessary constraints, which may lead to oversimplified or ill-posed filters. To overcome these issues, we propose $\textit{BernNet}$, a novel graph neural network with theoretical support that provides a simple but effective scheme for designing and learning arbitrary graph spectral filters. In particular, for any filter over the normalized Laplacian spectrum of a graph, our BernNet estimates it by an order-$K$ Bernstein polynomial approximation and designs its spectral property by setting the coefficients of the Bernstein basis. Moreover, we can learn the coefficients (and the corresponding filter weights) based on observed graphs and their associated signals and thus achieve the BernNet specialized for the data. Our experiments demonstrate that BernNet can learn arbitrary spectral filters, including complicated band-rejection and comb filters, and it achieves superior performance in real-world graph modeling tasks.

## Distribution Preserving Graph Representation Learning

### TL;DR

None

### Abstract

Graph neural network (GNN) is effective to model graphs for distributed representations of nodes and an entire graph. Recently, research on the expressive power of GNN attracted growing attention. A highly-expressive GNN has the ability to generate discriminative graph representations. However, in the end-to-end training process for a certain graph learning task, a highly-expressive GNN risks generating graph representations overfitting the training data for the target task, while losing information important for the model generalization. In this paper, we propose Distribution Preserving GNN (DP-GNN) - a GNN framework that can improve the generalizability of expressive GNN models by preserving several kinds of distribution information in graph representations and node representations. Besides the generalizability, by applying an expressive GNN backbone, DP-GNN can also have high expressive power. We evaluate the proposed DP-GNN framework on multiple benchmark datasets for graph classification tasks. The experimental results demonstrate that our model achieves state-of-the-art performances. 

## Deformable Graph Convolutional Networks

### TL;DR

We propose Deformable Graph Convolution Networks that address the limitations of most existing graph neural networks.

### Abstract

Graph neural networks (GNNs) have significantly improved the representation power for graph-structured data. Despite of the recent success of GNNs, the graph convolution in most GNNs have two limitations. Since the graph convolution is performed in a small local neighborhood on the input graph, it is inherently incapable to capture long-range dependencies between distance nodes. In addition, when a node has neighbors that belong to different classes, i.e., heterophily, the aggregated messages from them often negatively affect representation learning. To address the two common problems of graph convolution, in this paper, we propose Deformable Graph Convolutional Networks (Deformable GCNs) that adaptively perform convolution in multiple latent spaces and capture short/long-range dependencies between nodes. Separated from node representations (features), our framework simultaneously learns the node positional embeddings (coordinates) to determine the relations between nodes in an end-to-end fashion. Depending on node position, the convolution kernels are deformed by deformation vectors and apply different transformations to its neighbor nodes. Our extensive experiments demonstrate that Deformable GCNs flexibly handles the heterophily and achieve the best performance in node classification tasks on six heterophilic graph datasets.

## Spectrum-based Query-free Black-box Adversarial Attacks on Graph Representation Learning Models

### TL;DR

We propose a query-free black-box graph adversarial attack methods with the aim of maximizing the graph Laplacian spectrum.

### Abstract

Graph representation learning (GRL) models have achieved remarkable performance in various tasks on graphs. However, recent studies have shown that GRL models are vulnerable to adversarial attacks. Regardless of the fruitful progress, most of the existing graph adversarial attack methods require attackers to be either accessible to target models or to send queries to target models. In this work, we observe that existing graph attack methods consistently increase the graph Laplacian spectrum of the input structured data after performing attacks to different target models. Inspired by the observation, we propose a query-free black-box adversarial attack method, which assumes the attacker only has access to the input adjacency matrix of graph-structured data. Specifically, we first investigate the relationship that the increase of the graph Laplacian spectrum will degrade the latent representation's quality generated by GRL models. Our attack algorithm is then formulated as an optimization problem with the aim of maximizing the graph Laplacian spectrum, and adversarial edges are selected greedily by leveraging the eigenvalue perturbation theory. Extensive experiments demonstrate the effectiveness of our proposed attacked method on various tasks, including node classification and graph classification tasks.


## Building Powerful and Scalable Graph Neural Networks with Inception Modules

### TL;DR

Scalable Graph Neural Networks for Graph Classification and Isomorphism test

### Abstract

Graph neural networks (GNNs) are becoming increasingly popular for processing and analyzing graph-structured data. Existing GNN models have a sequential or parallel architecture. In sequential GNNs, the representation vector of a node is obtained by recursively aggregating feature vectors of its neighboring nodes. On the other hand, parallel GNNs have a bank of neighborhood aggregation operators arranged in parallel. Parallel GNNs scale well to large graphs, unlike sequential GNNs. Existing theoretical results to characterize the expressive power of GNNs to discriminate different graph structures are limited to a few popular sequential GNN variants. This paper extends these results to parallel GNNs. We present a theoretical framework for analyzing the discriminative power of parallel GNNs in distinguishing different graph structures. Then, we develop a parallel GNN architecture, called SPIN, which is provably as powerful as the one-dimensional Weisfeiler-Lehman graph isomorphism test. We validate our theory on nine graph classification benchmark datasets, demonstrating that SPIN has a faster training time independent of the graph structure, unlike sequential GNNs, and achieves state-of-the-art performance on many datasets. 

## Deconvolutional Networks on Graph Data

### TL;DR

The first work about deconvolutional networks on graph data

### Abstract

 Recent studies have indicated that Graph Convolutional Network (GCN) acts as a low pass filter in spectral domain and encodes smoothed node representations.  In this paper, we consider the opposite, namely Graph Deconvolutional Network (GDN) that reconstructs graph signals from smoothed node representations. We motivate the design of GDN via a combination of inverse filters in spectral domain and de-noising layers in wavelet domain, as the inverse operation results in a high pass filter and may amplify the noise. We demonstrate the effectiveness of the proposed method on several tasks including graph feature imputation and graph structure generation. 

## Embedding Signals on Knowledge Graphs with Unbalanced Diffusion Earth Mover’s Distance

### TL;DR

An efficient embedding of unbalanced optimal transport between distributions on a graph into $L^1$.

### Abstract

In modern relational machine learning it is common to encounter large graphs that arise via interactions or similarities between observations in many domains. Further, in many cases the target entities for analysis are actually signals on such graphs. We propose to compare and organize such datasets of graph signals by using an earth mover's distance (EMD) with a geodesic cost over the underlying graph. Typically, EMD is computed by optimizing over the cost of transporting one probability distribution to another over an underlying metric space. However, this is inefficient when computing the EMD between many signals. Here, we propose an unbalanced graph earth mover’s distance that efficiently embeds the unbalanced EMD on an underlying graph into an $L^1$ space, whose metric we call {\em unbalanced diffusion earth mover's distance (UDEMD)}. This leads us to an efficient nearest neighbors kernel over many signals defined on a large graph. Next, we show how this gives distances between graph signals that are robust to noise. Finally, we apply this to organizing patients based on clinical notes who are modelled as signals on the SNOMED-CT medical knowledge graph, embedding lymphoblast cells modeled as signals on a gene graph, and organizing genes modeled as signals over a large peripheral blood mononuclear (PBMC) cell graph. In each case, we show that UDEMD-based embeddings find accurate distances that are  highly efficient compared to other methods.

## Optimization-Based Algebraic Multigrid Coarsening Using Reinforcement Learning

### TL;DR

Providing a reinforcement learning method utilizing graph neural networks for algebraic multigrid coarsening, outperforming existing algorithms.

### Abstract

 Large sparse linear systems of equations are ubiquitous in science and engineering, such as those arising from discretizations of partial differential equations. Algebraic multigrid (AMG) methods are one of the most common methods of solving such linear systems, with an extensive body of underlying mathematical theory. A system of linear equations defines a graph on the set of unknowns and each level of a multigrid solver requires the selection of an appropriate coarse graph along with restriction and interpolation operators that map to and from the coarse representation. The efficiency of the multigrid solver depends critically on this selection and many selection methods have been developed over the years. Recently, it has been demonstrated that it is possible to directly learn the AMG interpolation and restriction operators, given a coarse graph selection. In this paper, we consider the complementary problem of learning to coarsen graphs for a multigrid solver. We propose a method using a reinforcement learning (RL) agent based on graph neural networks (GNNs), which can learn to perform graph coarsening on small training graphs and then be applied to unstructured large graphs. We demonstrate that this method can produce better coarse graphs than existing algorithms, even as the graph size increases and other properties of the graph are varied. We also propose an efficient inference procedure for performing graph coarsening that results in linear time complexity in graph size.

## Exploring Robustness of Neural Networks through Graph Measures

### TL;DR

This paper aims to apply graph theoretical methods and measures to artificial neural networks in order to characterize their robustness. 

### Abstract

Motivated by graph theory, artificial neural networks (ANNs) are traditionally structured as layers of neurons (nodes), which learn useful information by the passage of data through interconnections (edges). In the machine learning realm, graph structures (i.e., neurons and connections) of ANNs have recently been explored using various graph-theoretic measures linked to their predictive performance. On the other hand, in network science (NetSci), certain graph measures including entropy and curvature are known to provide insight into the robustness and fragility of real-world networks. In this work, we use these graph measures to explore the robustness of various ANNs to adversarial attacks. To this end, we (1) explore the design space of inter-layer and intra-layers connectivity regimes of ANNs in the graph domain and record their predictive performance after training under different types of adversarial attacks, (2) use graph representations for both inter-layer and intra-layers connectivity regimes to calculate various graph-theoretic measures, including curvature and entropy, and (3) analyze the relationship between these graph measures and the adversarial performance of ANNs. We show that curvature and entropy, while operating in the graph domain, can quantify the robustness of ANNs without having to train these ANNs. Our results suggest that the real-world networks, including brain networks, financial networks, and social networks may provide important clues to the neural architecture search for robust ANNs. We propose a search strategy that efficiently finds robust ANNs amongst a set of well-performing ANNs without having a need to train all of these ANNs.

## TD-GEN: Graph Generation With Tree Decomposition

### TL;DR

None

### Abstract

We propose TD-GEN, a graph generation framework based on tree decomposition, and introduce a reduced upper bound on the maximum number of decisions needed for graph generation.
The framework includes a permutation invariant tree generation model which 
forms the backbone of graph generation. 
Tree nodes are supernodes, each representing a cluster of nodes in the graph. Graph nodes and edges are incrementally generated inside the clusters by traversing the tree supernodes, respecting the structure of the tree decomposition, and following node sharing decisions between the clusters. 
Finally, we discuss the shortcomings of standard evaluation criteria based on statistical properties of the generated graphs as performance measures.  We propose to compare the performance of models based on likelihood. Empirical results on a variety of standard graph generation datasets demonstrate the superior performance of our method.

## Boundary-aware Graph Reasoning for Semantic Segmentation 

### TL;DR

None

### Abstract

In this paper, we propose a Boundary-aware Graph Reasoning (BGR) module to learn long-range contextual features for semantic segmentation. Rather than directly construct the graph based on the backbone features, our BGR module explores a reasonable way to combine segmentation erroneous regions with the graph construction scenario. Motivated by the fact that most hard-to-segment pixels broadly distribute on boundary regions, our BGR module uses the boundary score map as prior knowledge to intensify the graph node connections and thereby guide the graph reasoning focus on boundary regions. In addition, we employ an efficient graph convolution implementation to reduce the computational cost, which benefits the integration of our BGR module into current segmentation backbones. Extensive experiments on three challenging segmentation benchmarks demonstrate the effectiveness of our proposed BGR module for semantic segmentation.

## Graph Tree Neural Networks

### TL;DR

The novel neural network architecture

### Abstract

Graph neural networks (GNNs) have recently shown good performance in various fields. In this paper, we propose graph tree neural networks (GTNNs) designed to solve the problems of existing networks by analyzing the structure of human neural networks. In GTNNs, information units are related to the form of a graph and then they become a bigger unit of information again and have a relationship with other information units. At this point, the unit of information is a set of neurons, and we can express it as a vector with GTNN. Defining the starting and ending points in a single graph is difficult, and a tree cannot express the relationship among sibling nodes. However, a graph tree can be expressed using leaf and root nodes as its starting and ending points and the relationship among sibling nodes. Depth-first convolution (DFC) encodes the interaction result from leaf nodes to the root node in a bottom-up approach, and depth-first deconvolution (DFD) decodes the interaction result from the root node to the leaf nodes in a top-down approach. GTNN is data-driven learning in which the number of convolutions varies according to the depth of the tree. Moreover, learning features of different types together is possible. Supervised, unsupervised, and semi-supervised learning using graph tree recursive neural network (GTR) , graph tree recursive attention networks (GTRAs), and graph tree recursive autoencoders (GTRAEs) are introduced in this paper. Using the source code dataset for LineGraphCode, these algorithms are classified and their performances are compared.

## Learning Graph Search Heuristics

### TL;DR

Imitation learning algorithm and architecture for learning neural graph search heuristics.

### Abstract

Searching for a path between two nodes in a graph is one of the most well-studied and fundamental problems in computer science. In numerous domains such as robotics, AI, or biology, practitioners develop search heuristics to accelerate their pathfinding algorithms. However, it is a laborious and complex process to hand-design heuristics based on the problem and the structure of a given use case/graph. Here we present PHIL (Path Heuristic with Imitation Learning), a novel neural architecture and a training algorithm for discovering graph search and navigation heuristics from data by leveraging recent advances in imitation learning and graph representation learning. At training time, we aggregate datasets of search trajectories and ground-truth shortest path distances, which we use to train a specialized graph neural network-based heuristic function using backpropagation through steps of the pathfinding process. Our heuristic function learns graph embeddings useful for inferring node distances, runs in constant time independent of graph sizes, and can be easily incorporated in an algorithm such as A* at test time. Experiments show that PHIL reduces the number of explored nodes compared to state-of-the-art methods on benchmark datasets by $40.8\%$ on average, can be directly applied in diverse graphs ranging from biological networks to road networks, and allows for fast planning in time-critical robotics domains.

## Graph Belief Propagation Networks

### TL;DR

None

### Abstract

With the wide-spread availability of complex relational data, semi-supervised node classification in graphs has become a central machine learning problem. Graph neural networks are a recent class of easy-to-train and accurate methods for this problem that map the features in the neighborhood of a node to its label, but they ignore label correlation during inference and their predictions are difficult to interpret. On the other hand, collective classification is a traditional approach based on interpretable graphical models that explicitly model label correlations. Here, we introduce a model that combines the advantages of these two approaches, where we compute the marginal probabilities in a conditional random field, similar to collective classification, and the potentials in the random field are learned through end-to-end training, akin to graph neural networks. In our model, potentials on each node only depend on that node’s features, and edge potentials are learned via a coupling matrix. This structure enables simple training with interpretable parameters, scales to large networks, naturally incorporates training labels at inference, and is often more accurate than related approaches. Our approach can be viewed as either an interpretable message-passing graph neural network or a collective classification method with higher capacity and modernized training.

## Graph-Relational Domain Adaptation

### TL;DR

We generalize existing adversarial domain adaptation with a graph discriminator to work on graph-relational domains and provide theoretical guarantees.

### Abstract

Existing domain adaptation methods tend to treat every domain equally and align them all perfectly. Such uniform alignment ignores topological structures among different domains; therefore it may be beneficial for nearby domains, but not necessarily for distant domains. In this work, we relax such uniform alignment by using a domain graph to encode domain adjacency, e.g., a graph of states in the US with each state as a domain and each edge indicating adjacency, thereby allowing domains to align flexibly based on the graph structure. We generalize the existing adversarial learning framework with a novel graph discriminator using encoding-conditioned graph embeddings. Theoretical analysis shows that at equilibrium, our method recovers classic domain adaptation when the graph is a clique, and achieves non-trivial alignment for other types of graphs. Empirical results show that our approach successfully generalizes uniform alignment, naturally incorporates domain information represented by graphs, and improves upon existing domain adaptation methods on both synthetic and real-world datasets.

## Learning Combinatorial Graph Labeling

### TL;DR

None

### Abstract

We present a graph neural network to learn graph coloring heuristics using reinforcement learning. Our learned deterministic heuristics give better solutions than classical degree-based greedy heuristics and only take seconds to evaluate on graphs with tens of thousands of vertices. As our approach is based on policy-gradients, it also learns a probabilistic policy as well. These probabilistic policies outperform all greedy coloring baselines and a machine learning baseline. Our approach generalizes several previous machine-learning frameworks, which applied to problems like minimum vertex cover. We also demonstrate that our approach outperforms two greedy heuristics on minimum vertex cover.

## Directed Graph Auto-Encoders

### TL;DR

We  introduce a new class of parameterized graph auto-encoders and apply this model on directed link prediction tasks

### Abstract

We introduce a new class of auto-encoders for directed graphs, motivated by a direct extension of Weisfeiler-Leman algorithm to pairs of node labels. This model learns pairs of interpretable latent representations for the nodes of directed graphs. It uses parameterized graph convolutional network (GCN) layers for its  encoder and an asymmetric inner product decoder. Parameters in the encoder control the weighting of representations exchanged between neighboring nodes. We demonstrate the ability of the proposed model to learn meaningful latent embeddings on a suite of directed link prediction tasks and on several popular citation network datasets.

## Prototypical Graph Contrastive Learning

### TL;DR

None

### Abstract

Graph-level representations are critical in various real-world applications, such as predicting the properties of molecules. But in practice, precise graph annotations are generally very expensive and time-consuming. To address this issue, graph contrastive learning constructs instance discrimination task which pulls together positive pairs (augmentation pairs of the same graph) and pushes away negative pairs (augmentation pairs of different graphs) for unsupervised representation learning. However, since for a query, its negatives are uniformly sampled from all graphs, existing methods suffer from the critical sampling bias issue, i.e., the negatives likely having the same semantic structure with the query, leading to performance degradation. To mitigate this sampling bias issue, in this paper, we propose a Prototypical Graph Contrastive Learning (PGCL) approach. Specifically, PGCL models the underlying semantic structure of the graph data via clustering semantically similar graphs into the same group, and simultaneously encourages the clustering consistency for different augmentations of the same graph. Then given a query, it performs negative sampling via drawing the graphs from those clusters that differ from the cluster of query, which ensures the semantic difference between query and its negative samples. Moreover, for a query, PGCL further reweights its negative samples based on the distance between their prototypes (cluster centroids) and the query prototype such that those negatives having moderate prototype distance enjoy relatively large weights. This reweighting strategy is proved to be more effective than uniform sampling. Experimental results on various graph benchmarks testify the advantages of our PGCL over state-of-the-art methods.

## Inverse Graph Convolution Networks

### TL;DR

We show repeated message passing is equivalent to solving a sparse linear system and present efficient implementations.

### Abstract

We demonstrate that an infinite number of graph convolutions is equivalent to solving a sparse linear system, and discuss why this formulation is attractive from a spectral clustering perspective. We demonstrate that this smoothing-via-inverse is very similar numerically to other work and present optimized network implementations that allow for scalable models that can learn in the presence of smoothing without having to learn to smooth themselves. Results on semi-supervised learning tasks show these optimized models can train many times faster with little or no reduction in classification accuracy.

## Contrastive Graph Attentive Augmentation

### TL;DR

We propose a dynamic self attention based graph augmentation method, which achieve new state-of-the-art results.

### Abstract

Inspired by the effectiveness of data augmentation in contrastive learning for vision and language, we present a novel graph augmentation approach to boost graph contrastive learning. Specifically, we propose CGA-Aug: \underline{C}ontrastive \underline{G}raph \underline{A}ttentive \underline{Aug}mentation for the adjacency matrix. In our model, the augmentation function is dynamically updated via self attention with a diversity enhancement, which is performed at both graph-structure and node levels, to increase the disagreement between input signal. In contrast, previous works use random augmentation at node level, and perform the structure level augmentation by a static scheme, which limits the difficulty of contrasting. We conduct extensive experiments on three node classification datasets (Cora, Citeseer, Pubmed) and five graph classification datasets (MUTAG, PTC\_MR, etc). We further evaluate our method on downstream combinatorial tasks (graph edit distance and clustering). It shows our methods can achieve new state-of-the-art results for self-supervised learning on most datasets.

## Asymmetric Graph Contrastive Learning

### TL;DR

None

### Abstract

Learning effective graph representations in an unsupervised manner is a popular research topic in graph data analysis. Recently, contrastive learning has shown its success in unsupervised graph representation learning. However, how to avoid collapsing solutions for contrastive learning methods remains a critical challenge. In this paper, we propose a simple method to solve this problem for graph representation learning, which is different from the existing common-used techniques (such as negative samples or predictor network). The proposed model mainly relies on an asymmetric design to solve the collapsing solutions problem. In specific, it uses two graph neural networks (GNNs) with unequal depth layers to learn node representations from two augmented views and defines contrastive loss only based on positive sample pairs. Our simple method has lower computational and memory complexity than the existing methods. Furthermore, we give a theoretical analysis to prove that this asymmetric design can avoid collapsing solutions when training together with a stop-gradient operation, and this is also validated by experiments. We compared our method with three classic and six the state-of-the-art methods on six real-world datasets to demonstrate its superiority. The ablation experiments further validated the essential role of the asymmetric architecture.

## Topological Graph Neural Networks

### TL;DR

We describe a new layer for graph neural networks that incorporates multi-scale (from local to global) topological information.

### Abstract

Graph neural networks (GNNs) are a powerful architecture for tackling graph learning tasks, yet have been shown to be oblivious to eminent substructures, such as cycles. We present TOGL, a novel layer that incorporates global topological information of a graph using persistent homology. TOGL can be easily integrated into any type of GNN and is strictly more expressive in terms of the Weisfeiler–Lehman test of isomorphism. Augmenting GNNs with our layer leads to beneficial predictive performance for graph and node classification tasks, both on synthetic data sets, which can be classified by humans using their topology but not by ordinary GNNs, and on real-world data.

## Towards Expressive Graph Representations

### TL;DR

None

### Abstract

Graph Neural Network (GNN) shows its powerful capability for graph representation learning in various application areas. However, most existing GNN variants learn the graph representations in a non-injective or non-continuous fashion, both reducing the model expressive power. In this paper, we present a theoretical framework to improve the expressive power of GNN by taking both injectivity and continuity into account. Accordingly, we develop Injective Continuous Graph Neural Network (ICGNN) that learns the graph and node representations in an injective and continuous fashion, so that it can map similar nodes or graphs to similar embeddings, and non-equivalent nodes or non-isomorphic graphs to different embeddings. We validate the proposed ICGNN model for graph classification and node classification on multiple benchmark datasets including both simple graphs and attributed graphs. The experimental results demonstrate that our model achieves state-of-the-art performances on most of the benchmarks.

## Heat Kernel Graph Networks

### TL;DR

Graph neural networks with learnable multi-head heat kernels for graph classification 

### Abstract

Graph neural networks (GNNs) have shown to be useful in a variety of graph classification tasks from bioinformatics to social networks. However, most GNNs represent the graph using local neighborhood aggregation. This mechanism is inherently difficult to learn about the global structure of a graph and does not have enough expressive power to distinguish simple non-isomorphic graphs. To overcome the limitation, here we propose multi-head heat kernel convolution for graph representation. Unlike the conventional approach of aggregating local information from neighbors using an adjacency matrix, the proposed method uses multiple heat kernels to learn not only the local information, but also the global structure simultaneously. In most benchmarking problems, the proposed algorithm outperforms the competing methods, or at least shows comparable performance.

## Universal Graph Convolutional Networks

### TL;DR

None

### Abstract

Graph Convolutional Networks (GCNs), aiming to obtain the representation of a node by aggregating its neighbors, have demonstrated great power in tackling various analytics tasks on graph (network) data. The remarkable performance of GCNs typically relies on the homophily assumption of networks, while such assumption cannot always be satisfied, since the heterophily or randomness are also widespread in real-world. This gives rise to one fundamental question: whether networks with different structural properties should adopt different propagation mechanisms? In this paper, we first conduct an experimental investigation. Surprisingly, we discover that there are actually segmentation rules for the propagation mechanism, i.e., 1-hop network neighbors, 2-hop neighbors and $k$-nearest neighbor ($k$NN) are more suitable as neighborhoods of network with complete homophily, complete heterophily and randomness, respectively. However, the real-world networks are complex, and may present diverse structural properties, e.g., the network dominated by homophily may contain a small amount of randomness. So can we reasonably utilize these segmentation rules to design an universal propagation mechanism independent of the network structural assumption? To tackle this challenge, we develop a new universal GCN framework, namely U-GCN. It first introduces a multi-type convolution to extract information from 1-hop network, 2-hop network and $k$NN network simultaneously, and then designs a discriminative aggregation to sufficiently fuse them aiming to given learning objectives. Extensive experiments demonstrate the superiority of U-GCN over state-of-the-arts. 

## Krylov Graph Convolutional Networks

### TL;DR

None

### Abstract

Graph Convolutional Networks (GCN) is a pioneering model for graph-based semi-supervised learning. However, GCN does not perform well on sparsely-labeled graphs. Its two-layer version cannot effectively propagate the label information to the whole graph structure while its deep version over-smoothens and is hard to train. To solve these two issues, we propose a new model called KGCN (for Krylov Graph Convolutional Networks). KGCN aggregates the different orders of the vertex neighborhood information in a single layer. It interprets the layer-wise propagation rule of GCN from the perspective of power iteration and derives the Krylov tensor that consists of the different orders of the vertex neighborhood information. Then, the aggregation is performed in the space spanned by the Krylov tensor. The aggregation weights are learned by linear Perceptron, which are adaptive to the characteristics of datasets. Extensive experiments on various sparsely-labeled graphs verify the efficiency and effectiveness of KGCN compared to state-of-the-art approaches.

## Graph Kernel Attention Transformers

### TL;DR

We introduce Graph Kernel Attention Transformers (GKATs), leveraging the theory of graph kernels and recently proposed scalable attention methods for Transformers to efficiently model graph data.

### Abstract

We introduce a new class of graph neural networks (GNNs), by combining several concepts that were so far studied independently - graph kernels, attention-based networks with structural priors and more recently, efficient Transformers architectures applying small memory footprint implicit attention methods via low rank decomposition techniques. The goal of the paper is twofold. Proposed by us Graph Kernel Attention Transformers (or GKATs) are much more expressive than SOTA GNNs as capable of modeling longer-range dependencies within a single layer. Consequently, they can use more shallow architecture design. Furthermore, 
GKAT attention layers scale linearly rather than quadratically in the number of nodes of the input graphs, even when those graphs are dense, requiring less compute than their regular graph attention counterparts. They achieve it by applying new classes of graph kernels admitting random feature map decomposition via random walks on graphs. As a byproduct of the introduced techniques, we obtain a new class of learnable graph sketches, called graphots, compactly encoding topological graph properties as well as nodes' features. We conducted exhaustive empirical comparison of our method with nine different GNN classes on tasks ranging from motif detection through social network classification to bioinformatics challenges, showing consistent gains coming from GKATs.

## Deep Probabilistic Graph Matching

### TL;DR

We propose a deep learning framework for graph matching that works for the original QAP without compromising on the matching constraints.

### Abstract

 Most previous learning-based graph matching algorithms solve the $\textit{quadratic assignment problem} $ (QAP) by dropping one or more of the matching constraints and adopting a relaxed assignment solver to obtain sub-optimal correspondences. Such relaxation may actually weaken the original graph matching problem, and in turn hurt the matching performance.  In this paper we propose a deep learning-based graph matching framework that works for the original QAP without compromising on the matching constraints. In particular, we design an affinity-assignment prediction network to jointly learn the pairwise affinity and estimate the node assignments, and we then develop a differentiable solver inspired by the probabilistic perspective of the pairwise affinities. Aiming to obtain better matching results, the probabilistic solver refines the estimated assignments in an iterative manner to impose both discrete and one-to-one matching constraints. The proposed method is evaluated on three popularly tested benchmarks (Pascal VOC, Willow Object and SPair-71k), and it outperforms all previous state-of-the-arts on all benchmarks.

## Learning Graph Cellular Automata

### TL;DR

We study the application of graph neural networks to learn the transition rule of graph cellular automata.

### Abstract

Cellular automata (CA) are a class of computational models that exhibit rich dynamics emerging from the local interaction of cells arranged in a regular lattice. 
In this work we focus on a generalised version of typical CA, called \emph{graph} cellular automata (GCA), in which the lattice structure is replaced by an arbitrary graph. 
In particular, we extend previous work that used convolutional neural networks to learn the transition rule of conventional CA, and we use graph neural networks to learn a variety of transition rules for GCA. 
First, we present a general-purpose architecture for learning GCA, and we show that it can represent any arbitrary GCA with finite and discrete state space. 
Then, we test our approach on three different tasks: 1) learning the transition rule of a GCA on a Voronoi tassellation; 2) imitating the behaviour of a group of flocking agents; 3) learning a rule that converges to a desired target state.

## Gromov-Wasserstein Divergence for Robust Graph Classification: Structural Attacks and Certificates

### TL;DR

We propose using Gromov-Wasserstein divergence to finely measure topological attacks on graph classifers, and we develop robustness certificates based on Fenchel biconjugation.

### Abstract

Graph classifiers are vulnerable to topological attacks.  Although certificates of robustness have been recently developed, their threat model only counts local and global edge perturbations, which effectively ignores important graph structures such as isometry.  To address this issue, we propose measuring the perturbation with the Gromov-Wasserstein style discrepancy based on shortest-path distance on graphs, and building its Fenchel biconjugate to facilitate convex optimization.  Our key insight is drawn from tight relaxations in quadratic assignment problems.  The new discrepancy is shown to well characterize the difference in graph structure, and when applied to graph classification by graph convolutional networks, both our certificate and attack algorithm are demonstrated to be effective.

## Learning on Random Balls is Sufficient for Estimating (Some) Graph Parameters

### TL;DR

A graph parameter is estimable by GNNs+random sampling if and only if it is continuous in randomized Benjamini-Schramm topology.

### Abstract

Theoretical analyses for graph learning methods often assume a complete observation of the input graph. Such an assumption might not be useful for handling extremely large graphs due to the scalability issues in practice. In this work, we develop a theoretical framework for graph classification problems in the partial observation setting. Equipped with insights from graph limit theory, we propose a new graph classification model that works on a randomly sampled subgraph and a novel topology to characterize the representability of the model. Our theoretical framework leads to new learning-theoretic results on generalization bounds and size-generalizability without any assumption on the input graphs. 

## A Statistical Relational Approach to Learning Distance-based GCNs

### TL;DR

Embedding relational graph in Euclidean space for distance based Graph Convolutional Networks

### Abstract

We consider the problem of learning distance based Graph Convolutional Networks (GCNs) for relational data. Specifically, we first embed the original graph into the Euclidean space R^m using relational density estimation techniques thereby constructing a secondary Euclidean graph. The graph vertices correspond to the target triples and edges denote the Euclidean distances between the target triples. We emphasize the importance of learning the secondary Euclidean graph and the advantages of employing a distance matrix over the typically used adjacency matrix. Our comprehensive empirical evaluation demonstrates the superiority of our approach over 12 different GCN models, relational embedding techniques, rule learning techniques and relational models. 

## Edge-based Local Push for Personalized PageRank

### TL;DR

We propose EdgePush, an efficient edge-based local push method for computing approximate Personalized PageRank vector on weighted graphs.

### Abstract

Personalized PageRank (PPR) is a node proximity measure that is widely used in various graph learning tasks. A popular method for approximately computing PPR is the LocalPush algorithm. A crucial primitive operation in LocalPush is the push operation, which distributes the probability at a node $u$ to ALL $u$'s neighbors via the corresponding edges. While LocalPush works well on {\em unweighted} graphs, unfortunately, on {\em weighted} graphs, it can be extremely inefficient. In particular, when the weights of the edges incident on a node $u$ are {\em unbalanced}, in the sense that only a few of these edges take the majority of the total weight among them, the push operation would have to distribute {``insignificant''} probabilities along those edges which just take the minor weights, resulting in expensive overhead.

To resolve this issue, in this paper, we propose the EdgePush algorithm, a novel edge-based push method. EdgePush decomposes the aforementioned push operations into individual edge-based push operations. Hence, it can flexibly distribute the probabilities according to edge weights. Moreover, our EdgePush allows a fine-grained termination threshold for each individual edge, making a sub-linear (to the graph size) time complexity become achievable. Our experimental results show that when achieving the same actual $\ell_1$-error, EdgePush outperforms LocalPush on both real-world and synthetic datasets by orders of magnitude in terms of efficiency.

## Context-Aware Sparse Deep Coordination Graphs

### TL;DR

We propose a novel method for learning context-aware sparse coordination graphs, solving this long-standing problem in the literature of multi-agent learning.

### Abstract

Learning sparse coordination graphs adaptive to the coordination dynamics among agents is a long-standing problem in cooperative multi-agent learning. This paper studies this problem by proposing several value-based and observation-based schemes for learning dynamic topologies and evaluating them on a new Multi-Agent COordination (MACO) benchmark. The benchmark collects classic coordination problems in the literature, increases their difficulty, and classifies them into different types. By analyzing the individual advantages of each learning scheme on each type of problem and their overall performance, we propose a novel method using the variance of utility difference functions to learn context-aware sparse coordination topologies. Moreover, our method learns action representations that effectively reduce the influence of utility functions' estimation errors on graph construction. Experiments show that our method significantly outperforms dense and static topologies across the MACO and StarCraft II micromanagement benchmark.

## Neo-GNNs: Neighborhood Overlap-aware Graph Neural Networks for Link Prediction

### TL;DR

None

### Abstract

Graph Neural Networks (GNNs) have been widely applied to various fields for learning over graph-structured data. They have shown significant improvements over traditional heuristic methods in various tasks such as node classification and graph classification. However, since GNNs heavily rely on smoothed node features rather than graph structure, they often show poor performance than simple heuristic methods in link prediction where the structural information, e.g., overlapped neighborhoods, degrees, and shortest paths, is crucial. To address this limitation, we propose Neighborhood Overlap-aware Graph Neural Networks (Neo-GNNs) that learn useful structural features from an adjacency matrix and estimate overlapped neighborhoods for link prediction. Our Neo-GNNs generalize neighborhood overlap-based heuristic methods and handle overlapped multi-hop neighborhoods. Our extensive experiments on Open Graph Benchmark datasets (OGB) demonstrate that Neo-GNNs consistently achieve state-of-the-art performance in link prediction.


## Low-pass Graph Convolutional Network for Recommendation

### TL;DR

We proposed a new paradigm of graph convolutional network with fully trainable kernels to extract high-level features for recommendaion.

### Abstract

Spectral graph convolution is extremely time-consuming for large graphs, thus existing Graph Convolutional Networks (GCNs) reconstruct the kernel by a polynomial, which is (almost) fixed. To extract features from the graph data by learning kernels, Low-pass Collaborative Filter Network (LCFN) was proposed as a new paradigm with trainable kernels. However, there are two demerits of LCFN: (1) The hypergraphs in LCFN are constructed by mining 2-hop connections of the user-item bipartite graph, thus 1-hop connections are not used, resulting in serious information loss issue. (2) LCFN follows the general network structure of GCNs, thus the structure is suboptimal. To address these issues, we utilize the bipartite graph to define the graph space directly and explore the best network structure based on experiments. Comprehensive experiments on two real-world datasets demonstrate the effectiveness of the proposed model.

## Adversarial Attacks on Graph Classifiers via Bayesian Optimisation

### TL;DR

We present an effective black-box and query-efficient adversarial attack on graph classification models

### Abstract

Graph neural networks, a popular class of models effective in a wide range of graph-based learning tasks, have been shown to be vulnerable to adversarial attacks. While the majority of the literature focuses on such vulnerability in node-level classification tasks, little effort has been dedicated to analysing adversarial attacks on graph-level classification, an important problem with numerous real-life applications such as biochemistry and social network analysis. The few existing methods often require unrealistic setups, such as access to internal information of the victim models, or an impractically-large number of queries. We present a novel Bayesian optimisation-based attack method for graph classification models. Our method is black-box, query-efficient and parsimonious with respect to the perturbation applied. We empirically validate the effectiveness and flexibility of the proposed method on a wide range of graph classification tasks involving varying graph properties, constraints and modes of attack. Finally, we analyse common interpretable patterns behind the adversarial samples produced, which may shed further light on the adversarial robustness of graph classification models. 

## Learning Discriminative Orthogonal Graph Representation

### TL;DR

We propose a novel method named DOG, which aims to learn discriminative node representations by maximizing the principle angle of node representation space for nodes in different communities. 

### Abstract

Graph representation learning has become the dominant technique in analyzing graph-structured data. The downstream graph analysis tasks benefit from discriminative node representations learned by unsupervised learning. Through analyzing real-world graphs, we observe that nodes with similar characteristics (e.g., labels) typically are connected together in the graph and are in the same community. Based on such observation, we assume that the connections of nodes in the same community are drawn from the same distribution.  To use this information explicitly, we propose a novel method, named Deep Orthogonal Graph rEpresentation (DOGE), which aims to learn discriminative node representations in an unsupervised fashion. Specifically, DOGE first leverages graph neural network as an encoder to transform the graph to node representations, then maximizes the principle angle of representation subspaces for nodes in different communities. We theoretically and empirically validate the effectiveness of the proposed method. Besides, we also show that DOGE outperforms a variety of unsupervised baselines on node classification task, and it also performs well when compared to its supervised counterparts.

## Learning to Learn Graph Topologies

### TL;DR

None

### Abstract

Learning a graph topology to reveal the underlying relationship between data entities plays an important role in machine learning. Under the assumption that structured data vary smoothly over a graph, the problem can be formulated as a regularised convex optimisation over a positive semidefinite cone and solved by iterative algorithms. Classic methods require an explicit convex function to reflect the topological priors, e.g. the lasso term for sparsity, which limits the flexibility and expressiveness of learning specific graph structures. We propose to learn a mapping from data to the graph based on learning to optimise (L2O). Specifically, our model first unrolls an iterative primal-dual splitting algorithm into a neural network. The network is then stacked with a variational autoencoder that refines the estimated graph with enhanced structural properties. The model is trained in an end-to-end fashion with pairs of structured data and graph samples. Experiments on both synthetic and real-world data demonstrate that our model is more efficient than classic iterative algorithms in learning a graph with specific structural properties. 

## Subgraph Kernel Contrast for Self-Supervised Graph Representation Learning

### TL;DR

None

### Abstract

Self-supervised graph representation learning is a challenging problem due to the complex geometric structures of graph-structured data. Most existing methods mainly focus on employing nodes features for graph representation in pretext task, neglecting the topology structures of graph. In order to exploit the topology structures  of the graph, we propose a subgraph kernel contrast based self-supervised graph representation learning method, named Sub-KC. In our method, we first use graph neural network (GNN) to obtain the node embeddings. Based on the proposed neighborhood adaptive sampling method, we can obtain the induced subgraph of each nodes. In order to calculate the similarity between the subgraphs, we furthermore propose a subgraph kernel function that takes both the node features and structures of subgraph into account. Then we use a corruption function to get the positive samples and treat all subgraphs induced from other nodes as negative samples. Finally, we employ the contrastive loss to pretrain the network. Experiments on both transductive and inductive node classification tasks demonstrate that our our method achieves competitive performance and even surpasses the performance of supervised learning.

## Network Controllability Perspectives on Graph Representations

### TL;DR

We develop a novel graph representation method grounded in networked control system theory; achieve state-of-the-art results on Reddit-B 

### Abstract

Graph representations in fixed dimensional feature space are vital in applying learning tools and data mining algorithms to perform graph analytics. This paper employs a unique approach grounded in networked control system theory to obtain expressive graph representations with desired properties. We consider graphs as networked dynamical systems and study their \emph{controllability} properties to explore the underlying graph structure. The controllability of a networked dynamical system profoundly depends on the underlying network topology, and we exploit this relationship to design novel graph representations using controllability Gramian and related metrics. We discuss the merits of this new approach in terms of the expressiveness and stability of the proposed representations. Our evaluation of various benchmark datasets in the graph classification framework demonstrates that the proposed representations either outperform (sometimes by more than $6\%$), or give similar results to the state-of-the-art embeddings. We also report a smaller variance in our reported classification accuracies compared to other GNNs methods as evidence of the stability of our results. We further outline the potential directions offered by network control theory for graph learning research.

## Attentive Walk-Aggregating Graph Neural Network: An Application to Molecules

### TL;DR

End-to-end supervised, interpretable GNN using weighting schemes on walk aggregation with state-of-the-art molecular property prediction performance, along with provable theoretical guarantees

### Abstract

Graph neural networks have attracted increasing interest for learning over graph-structured data, such as molecules and social networks. They have been shown to possess strong representation power, which various recent studies aim to further improve for downstream prediction tasks. In this paper, we introduce a novel graph neural network model, called AWARE, by incorporating weighting schemes, such as the self-attention mechanism, for aggregating the information about the walks in the graph in a principled way. This leads to an end-to-end supervised learning method for graph-level prediction tasks. Our theoretical analysis provides provable guarantees, demonstrating how the graph information is encoded in the representation as well as how our weighting scheme affects the representation and the learning performance. Our experiments on 60 tasks from 10 molecular property prediction benchmarks show that AWARE achieves state-of-the-art performance. Our ablation study further confirms the key advantages of our weighting mechanism. We also perform an interpretation study to illustrate that AWARE can successfully learn to capture the important substructures of the input graph.

## Partial Graph Reasoning for Neural Network Regularization

### TL;DR

We re-formulate graph reasoning methods as regularization to neural networks

### Abstract

Regularizers helped deep neural networks prevent feature co-adaptations. Dropout, as a commonly used regularization technique, stochastically disables neuron activations during network optimization. However, such complete feature disposal can affect the feature representation and network understanding. Toward better descriptions of latent representations, we present \textit{DropGraph} that learns regularization function by constructing a stand-alone graph from the backbone features. DropGraph first samples stochastic spatial feature vectors and then incorporates graph reasoning methods to generate feature map distortions. This add-on graph regularizes the network during training and can be completely skipped during inference. We provide intuitions on the linkage between graph reasoning and Dropout with further discussions on how partial graph reasoning method reduces feature correlations. To this end, we extensively study the modeling of graph vertex dependencies and the utilization of the graph for distorting backbone feature maps. DropGraph was validated on four tasks with a total of 7 different datasets. The experimental results show that our method outperforms other state-of-the-art regularizers while leaving the base model structure unmodified during inference.

## Location-aware Convolutional Neural Networks for Graph Classification

### TL;DR

None

### Abstract

 Graph patterns play a critical role in various graph classification tasks, e.g., chemical patterns often determine the properties of molecular graphs. Due to the powerful pattern learning ability of convolutional neural networks (CNNs) in image classification, researchers devote to adapting CNNs to graph classification. Existing methods generally follow a \emph{heuristic ranking-based} framework, which adopts heuristic rules to rank nodes within the ego network to share convolution kernels. However, by dropping nodes to form a fixed-size receptive field, such heuristic ranking-based methods may lose important structure information and lack the ability to learn task-oriented graph patterns. In contrast, in this paper, we propose a \emph{Location learning-based Convolutional Neural Networks} (LCNN) for graph classification.  LCNN learns the location of each node according to its learnable embeddings to construct receptive fields, then general CNNs are applied to capture graph patterns. Such a location learning mechanism not only retrains the information of all nodes, but also provides an important ability of pattern learning, which is directly guided by the target graph classification task. Experimental results show the effectiveness of our proposed LCNN method, and visualization results further illustrate the valid pattern learning ability of our method for graph classification.

## Geometric Learning via Continuous and Discrete Curvature

### TL;DR

We propose to encapsulate the intrinsic manifold of graph topology into the continuous hyperbolic space to obtain a preferable representation.

### Abstract

Graphs with hierarchical latent anatomies are ubiquitous in the real world. In geometric learning, the quality of the representations is determined by how well the geometry of the embedding space matches the structure of the graph data. A manifold can be considered as a continuous approximation to a discrete graph, but distortions are inevitable as the structure of a real-world network is intricate, in which some areas are tree-like, some areas are flat and some areas are cyclic. This motivates us to encapsulate the intrinsic local graph structure into the continuous embedding space to obtain a preferable representation. Inspiringly, graph curvature can well describe the local structure where the node is located. In this work, we investigate the graph representation learning in hyperbolic geometry on account of its exponential capacity and hierarchical awareness. In specific, we propose a discrete curvature guided hyperbolic graph network, KHGN, for node embedding of graphs with non-Euclidean layouts, which utilizes the discrete graph Ricci curvature to lead message passing of the surroundings and adaptively adjust the hyperbolic curvature simultaneously. Extensive experiments on node classification and link prediction tasks demonstrate its efficacy that our method outperforms series competitive models by a large margin in both high and low hyperbolic graph data. 

## FMLGLN: Fast Multi-layer Graph Linear Network

### TL;DR

None

### Abstract

Graph Convolutional Networks (GCNs) is an efficient way to deal with graph data. However, GCN inevitably expands neighbors in downstream layers in the training and testing process, which makes GCN non-batch trained, non-inductive learned, and not suitable for large-scale graph data. Existing methods mitigate the neighbor expansion by restricting the number of neighbors or sampling sub-graphs, but they do not solve all these problems thoroughly. To this end, this work illustrates that the key point of neighbor expansion depends on the learnable matrix in the graph convolution operation, and proposes a Fast Multi-layer Graph Linear Network (FMLGLN) to solve the neighbor expansion problem thoroughly. In the implementation, FMLGLN excludes the learnable matrix from the graph convolution operation to make nodes independent of each other in the optimization process and then uses matrix multiplication to get the embedding features in different layers. Finally, FMLGLN concatenates these embedding features and uses a linear neural network to compute the final results. Obviously, the operations including matrix multiplication and neural network are linear, which brings fast training speed for FMLGLN. The features of FMLGCN are: 1) using batch training, 2) inductive learning, 3) suiting for large-scale graph data. Experimental results on large-scale graph-structured data demonstrate the effectiveness of the proposed FMLGLN.

## Adaptive Kernel Graph Neural Network

### TL;DR

A novel graph neural network with adaptive kernel learning mechanism .

### Abstract

Graph neural networks (GNNs) have demonstrated great success in representation learning for graph-structured data. The layer-wise graph convolution in GNNs is shown to be powerful at capturing graph topology. During this process, GNNs are usually guided by pre-defined kernels such as Laplacian matrix, adjacency matrix, or their variants. However, the adoptions of pre-defined kernels may restrain the generalities to different graphs: mismatch between graph and kernel would entail sub-optimal performance. For example, GNNs that focus on low-frequency information may not achieve satisfactory performance when high-frequency information is significant for the graphs, and vice versa. To solve this problem, in this paper, we propose a novel framework - i.e., namely Adaptive Kernel Graph Neural Network (AKGNN) - which learns to adapt to the optimal graph kernel in a unified manner at the first attempt. In the proposed AKGNN, we first design a data-driven graph kernel learning mechanism, which adaptively modulates the balance between all-pass and low-pass filters by modifying the maximal eigenvalue of the graph Laplacian. Through this process, AKGNN learns the optimal threshold between high and low frequency signals to relieve the generality problem. Later, we further reduce the number of parameters by a parameterization trick and enhance the expressive power by a global readout function. Extensive experiments are conducted on acknowledged benchmark datasets and promising results demonstrate the outstanding performance of our proposed AKGNN by comparison with state-of-the-art GNNs. 

## Neural PSL: Neural-Symbolic Learning using Probabilistic Soft Logic

### TL;DR

A neural-symbolic approach to integrating arbitrary neural networks with logic.

### Abstract

We introduce Neural PSL, a novel framework for integrating domain information and constraints from structured prediction with the representational power of neural networks. We provide a principled approach to training neural networks, regardless of architecture, in the presence of structured information. Neural PSL improves on prior neural-symbolic approaches by utilizing both hard and soft constraints, putting few restrictions on the number or architecture of the neural networks, performing efficient inference, and providing convergence guarantees. We demonstrate the ability of Neural PSL to use knowledge and structure to overcome data scarcity, outperforming a purely neural solution using relational information. We further demonstrate Neural PSL's capabilities by showing an 18% improvement on the emotion recognition in conversation dataset DailyDialog, and a 16% improvement on the Visual Sudoku dataset.

## Multi-task Learning of Order-Consistent Causal Graphs

### TL;DR

None

### Abstract

We consider the problem of discovering $K$ related directed acyclic graph (DAG) structures, where the involved graph nodes share a consistent causal ordering and sparse unions of supports. Under this multi-task learning setting, we formulate the joint recovery problem as an $l_1/l_2$-regularized maximum likelihood estimation (MLE) for $K$ linear structural equation models. We show that the joint estimator, by leveraging data across related tasks, can achieve a better sample complexity for recovering the causal order (or topological order) than separate estimations. Furthermore, our analysis also shows the consistency of union support recovery of the structures. To allow practical implementation, we design a continuous optimization problem whose optimizer is the same as the analyzed joint estimator and can be approximated efficiently by an iterative algorithm. We validate the theoretical analysis and the effectiveness of the joint estimator in experiments.

## cGTW: A fast and flexible approach for time-series joint alignment problem

### TL;DR

None

### Abstract

Time series in real applications are usually arranged in a structured way, which implies the potential dependency and provides extra information for the task of time-series alignment. For example, in imaging data, the intensities are recorded over time pixel by pixel, where it is natural to assume adjacent pixels tend to have similar temporal patterns. Such a property can be utilized to achieve more accurate alignment. However, many popular time-series alignment methods like dynamic time warping (DTW) are designed to only compare a single pair of time series.  The result with the application of separate alignment is usually suboptimal. As a generalized form of DTW, graphical time warping (GTW) was recently proposed to conduct a joint alignment of all time-series pairs. Both the alignment within the single pair and the dependency between adjacent pairs are considered, and the alignment problem is proved to be equivalent to a minimum-cut problem of an induced graph. However, the original GTW assumes that all the pairs fall in the same feasible time window, and thus the prior knowledge/constraints for different time-series pairs cannot be introduced. This limitation not only incurs unnecessary computation but also may lead to less accurate performance. In this paper, working in dual graph space, we lift the restriction, propose constrained graphical time warping (cGTW), and show it is a further generalized form of GTW, where the individual alignment of each pair, the dependency among pairs, and the prior knowledge/constraints are all modeled in an integrated framework.  By introducing constraints, cGTW is more flexible, controllable, efficient, and takes up less memory than GTW. The superiority is illustrated through several experiments, and more potential applications are also discussed.

## Relational Learning with Variational Bayes

### TL;DR

We propose an unsupervised learning method for addressing the relational learning problem where we learn the underlying relationship between a pair of data irrespective of the nature of those data.

### Abstract

In psychology, relational learning refers to the ability to recognize and respond to relationship among objects irrespective of the nature of those objects. Relational learning has long been recognized as a hallmark of human cognition and a key question in artificial intelligence research. In this work, we propose an unsupervised learning method for addressing the relational learning problem where we learn the underlying relationship between a pair of data irrespective of the nature of those data. The central idea of the proposed method is to encapsulate the relational learning problem with a probabilistic graphical model in which we perform inference to learn about data relationship and other relational processing tasks.


## Learning Binary Multi-Scale Games on Networks

### TL;DR

None

### Abstract

Network games provide a natural modeling framework for strategic interactions of agents whose actions have local impacts on others as captured by graphs.  Recently, a multi-scale network game model has been proposed to capture local effects at multiple network scales, such as among both individuals and groups.  We propose a framework to learn the utility functions of binary multi-scale games from agents' behavioral data.  Departing from much prior work in this area, we model agent behavior as following logit-response dynamics, rather than acting according to a Nash equilibrium.  The multi-scale game together with the logit-response dynamics define a generative time-series model of joint behavior of both agents and groups, which enables us to naturally cast the learning problem as maximizing data likelihood. The resulting problem can be solved using gradient ascent, and we show that in the important special case of linear-quadratic games, the estimation problem is convex.  Extensive experiments using both synthetic and real data demonstrate that our proposed modeling and learning approach is effective in both game parameter estimation as well as prediction of future behavior, even when we learn the game from only a single time-series behavior sequence. Finally, we show how to use our framework to develop a statistical test for the existence of multi-scale structure in the game, and use it to demonstrate that real time-series data indeed exhibits such structure.

## SLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks

### TL;DR

None

### Abstract

Graph neural networks (GNNs) work well when the graph structure is provided. However, this structure may not always be available in real-world applications. One solution to this problem is to infer a task-specific latent structure and then apply a GNN to the inferred graph. Unfortunately, the space of possible graph structures grows super-exponentially with the number of nodes and so the task-specific supervision may be insufficient for learning both the structure and the GNN parameters. In this work, we propose the Simultaneous Learning of Adjacency and GNN Parameters with Self-supervision, or SLAPS, a method that provides more supervision for inferring a graph structure through self-supervision. A comprehensive experimental study demonstrates that SLAPS scales to large graphs with hundreds of thousands of nodes and outperforms several models that have been proposed to learn a task-specific graph structure on established benchmarks.

## Pareto Graph Self-Supervised Learning

### TL;DR

In this paper, we study the problem of finding proper trade-off for graph self-supervised learning.

### Abstract

In this paper, we study the problem of finding proper trade-off for graph self-supervised learning.
Recently, various self-supervised auxiliary tasks have been proposed  
to accelerate representation learning in Graph Neural Networks (GNNs).
However, existing graph self-supervised learning ignores the task conflicts between the main task and auxiliary tasks.
In this paper, we propose Pareto graph self-supervised learning, a general learning framework that not only finds the solution where the main task rather than auxiliary tasks achieves the best performance in graph self-supervised learning, but more importantly, learns the personalized self-supervised task for different nodes.
The proposed method first formulates the graph self-supervised learning problem as a multi-objective optimization problem, and then solves the problem with the preferred vector and personalized optimization. 
Experimental results demonstrate the effectiveness of the proposed method by achieving state-of-the-art performance. 

## Simplicial Complex Convolutional Networks

### TL;DR

We propose representation learning method for higher-order interactions in complex systems with simplicial complexes. 

### Abstract

Graph-based machine learning is experiencing explosive growth, driven by impressive recent developments and wide applicability. Typical approaches for graph representation learning predominantly focus on pairwise interactions, while neglecting the patterns of higher-order interactions common to complex systems. This paper explores many body interaction models, centering on simplicial complexes. We introduce new simplex convolutional network model and simplicial complex convolutional model that offer low complexity and excellent performance. From a theoretical point of view, we offer three insights illustrating why higher-order models are necessary, why non-graph-based models generally cannot generalize well, and why graph-based, and especially higher-order models can generalize. We conduct experiments on co-citation networks, co-authorship networks and gene-disease associations and show that our framework can more efficiently capture underlying higher-order structures than current non-graph structure, regular graph, hypergraph, and simplicial complex-based learning frameworks.

## SparRL: Graph Sparsification via Deep Reinforcement Learning

### TL;DR

The first general and effective reinforcement learning-based framework for graph sparsification

### Abstract

Graph sparsification concerns data reduction where an edge-reduced graph of a similar structure is preferred. Existing methods are mostly sampling-based, which introduce high computation complexity in general and lack of flexibility for a different reduction objective. We present SparRL, the first general and effective reinforcement learning-based framework for graph sparsification. SparRL can easily adapt to different reduction goals and promise graph-size-independent complexity. To facilitate learning, we further introduce a novel expert control process by fusing RL with imitation learning. Extensive experiments show that SparRL outperforms all prevailing sparsification methods in producing high-quality sparsified graphs concerning a variety of objectives. As graph representations are very versatile, SparRL carries the potential for a broad impact.

## GLIN: Global-Local Interaction Network for Recognizing Human-Object Interaction in a Video  

### TL;DR

Modelling human-object interactions in videos by applying knowledge distillation between global to local context graphs

### Abstract

Recognizing interactions between actors and objects in videos is imperative to scene understanding and action recognition. Due to complex spatio-temporal relations among entities, namely humans and objects, learning their interactions  is very challenging, especially when multiple objects appear around a human in different time steps. Hence, capturing the context of an interaction can provide more informative cues about actions being performed. Our paper proposes the Global-Local Interaction Network (GLIN) to learn human and object interactions through space and time by exploring two-level interaction relations, including local and global contexts. GLIN encodes humans and objects into graph nodes and learns local and global relations via graph attention and graph convolution networks. The local context graphs learn the relation between objects at a frame level, whereas the global relation graph is constructed based on video-level object interactions. The local level information can capture the co-occurrence relation between humans and objects at a specific time step, while the global level helps identify long-term relations between humans and objects during the video. We also investigate how to  distill knowledge between these global and local interaction graphs for recognizing human object interactions. We evaluate our model by conducting comprehensive experiments on the Charades dataset, which comprises videos with different human object interactions in complex environments. Our model outperforms the baseline without object-interaction graphs by 3.68% mAP on the Charades dataset. 

## DiBS: Differentiable Bayesian Structure Learning

### TL;DR

A fully differentiable method for joint Bayesian inference of graphs and parameters of general Bayesian networks 

### Abstract

Bayesian structure learning allows inferring Bayesian network structure from data while reasoning about the epistemic uncertainty---a key element towards enabling active causal discovery and designing interventions in real world systems. In this work, we propose a general, fully differentiable framework for Bayesian structure learning (DiBS) that operates in the continuous space of a latent probabilistic graph representation. Building on recent advances in variational inference, we use DiBS to devise an efficient method for approximating posteriors over structural models. Contrary to existing work, DiBS is agnostic to the form of the local conditional distributions and allows for joint posterior inference of both the graph structure and the conditional distribution parameters. This makes our method directly applicable to posterior inference of nonstandard Bayesian network models, e.g., with nonlinear dependencies encoded by neural networks. In evaluations on simulated and real-world data, DiBS significantly outperforms related approaches to joint posterior inference.


## Graph Differentiable Architecture Search with Structure Learning

### TL;DR

None

### Abstract

Discovering ideal Graph Neural Networks (GNNs) architectures for different tasks is labor intensive and time consuming. To save human efforts, Neural Architecture Search (NAS) recently has been used to automatically discover adequate GNN architectures for certain tasks in order to achieve competitive or even better performance compared with manually designed architectures. However, existing works utilizing NAS to search GNN structures fail to answer the question: how NAS is able to select the desired GNN architectures? In this paper, we investigate this question to solve the problem, for the first time. We conduct a measurement study with experiments to discover that gradient based NAS methods tend to select proper architectures based on the usefulness of different types of information with respect to the target task. Our explorations further show that gradient based NAS also suffers from noises hidden in the graph, resulting in searching suboptimal GNN architectures. Based on our findings, we propose a Graph differentiable Architecture Search model with Structure Optimization (GASSO), which allows differentiable search of the architecture with gradient descent and is able to discover graph neural architectures with better performance through employing graph structure learning as a denoising process in the search procedure. The proposed GASSO model is capable of simultaneously searching the optimal architecture and adaptively adjusting graph structure by jointly optimizing graph architecture search and graph structure denoising. Extensive experiments on real-world graph datasets demonstrate that our proposed GASSO model is able to achieve state-of-the-art performance compared with existing baselines.

## Graph-based Nearest Neighbor Search in Hyperbolic Spaces

### TL;DR

We show theoretically and empirically that graph-based approaches are well suited for nearest neighbor search in hyperbolic spaces.

### Abstract

The nearest neighbor search (NNS) problem is widely studied in Euclidean space, and graph-based algorithms are known to outperform other approaches for this task. However, hyperbolic geometry is found to be very useful for data representation in various domains, including natural language processing, computer vision, and information retrieval.  In this paper, we show that graph-based approaches are also well suited for hyperbolic geometry. From a theoretical perspective, we rigorously analyze the time and space complexity of graph-based NNS, assuming that an n-element dataset is uniformly distributed within a d-dimensional ball of radius R in the hyperbolic space of curvature -1. Assuming the dense setting (d R << log(n)), we show that locally graph-based search in the hyperbolic space behaves similarly to the Euclidean case. However, there is a fundamental global difference: for small values of d, the number of search steps over the similarity graph is much smaller in the hyperbolic space. This suggests that graph-based search can be even more efficient in spaces of negative curvature, which is an additional advantage of hyperbolic geometry for data representation. From a practical perspective, we confirm this hypothesis on word embedding data: we compare graph-based NNS for GloVe and Poincar\'e GloVe word embeddings. It turns out that for the same corpus, graph-based NNS is more efficient in the hyperbolic space.

## Motif-driven Contrastive Learning of Graph Representations

### TL;DR

Learn semantically meaningful graph motifs without annotations and leverage them to pre-train Graph Neural Networks (GNNs) via contrastive learning

### Abstract

Pre-training Graph Neural Networks (GNNs) via contrastive learning has shown promising results for self-supervised graph representation learning. However, most existing works contrast graphs at node-level, which capture limited global graph information. Contrastive learning at subgraph-level naturally involves more global information and helps with learning whole graph representations, but the key challenge for this approach to work effectively is to generate semantically meaningful subgraphs. In this paper, we propose a framework Motif-drIven Contrastive leaRning Of Graph representations (MICRO-Graph) to overcome the challenge above. We first learn graph motifs, like functional groups of molecules. Then we generate meaningful motif-like subgraphs to benefit contrastive learning. For the motif learning step, we relax the definition of motifs from the traditional case, i.e. exactly-matched subgraph patterns, to a more practical case, e.g. prototypical embeddings corresponding to semantically similar subgraph patterns. Then we learn these motif embeddings as a hidden variable characterizing the distribution of GNN-encoded node embeddings. We show that MICRO-Graph is an effective framework on different domains including chemical compounds and proteins. The extracted motifs are interpretable and match real world functional groups. The pre-trained model provide significant performance enhancement outperforming contrastive methods with random subgraphs, pre-train methods based on exactly-matched motifs, and other state-of-the-art self-supervised learning methods. 


## Understanding Graph Contrastive Learning: A Need for Better Augmentations

### TL;DR

Graph contrastive learning requires careful augmentations to perform in nontrivial set-ups. GNN inductive bias masks the need for better augmentations on popular benchmarks. 

### Abstract

Leveraging large datasets and task-relevant augmentations, contrastive learning (CL) has enabled unsupervised computer vision models to compete well against supervised ones. Due to the inherent difficulty of labeling non-euclidean datasets, graph applications are fertile ground for use of CL. Interestingly, recent graph CL frameworks report high performance while diverging from the principles of visual CL: (i) these frameworks often use datasets that are orders of magnitude smaller and (ii) their employed augmentations are either random or task-irrelevant topological/feature perturbations (e.g., node-dropping, edge removal, degree-biased sampling), which can corrupt useful information. Given these discrepancies, we seek to determine: (i) How do existing graph CL frameworks perform well despite weak augmentations and limited data? and (ii) Does adhering to visual CL principles improve performance on graph classification tasks? Through reasoned, ablative experiments, we show that on small benchmark datasets, the inductive bias of graph neural networks can significantly compensate for the limitations of existing frameworks. Indeed, on relatively larger graph classification tasks, we find prior augmentations perform poorly, while adhering to principles in visual CL can significantly improve performance. For example, in graph-based document classification, we show task-relevant augmentations improve accuracy by 20$\%$. 

## Partition and Code: learning how to compress graphs

### TL;DR

We introduce a flexible, end-to-end machine learning framework for lossless graph compression based on graph partitioning and entropy coding

### Abstract

Can we use machine learning to compress graph data? The absence of ordering in graphs poses a significant challenge to conventional compression algorithms, limiting their attainable gains as well as their ability to discover relevant patterns. On the other hand, most graph compression approaches rely on domain-dependent handcrafted representations and cannot adapt to different underlying graph distributions. This work aims to establish the necessary principles a lossless graph compression method should follow to approach the entropy storage lower bound. Instead of making rigid assumptions about the graph distribution, we formulate the compressor as a probabilistic model that can be learned from data and generalise to unseen instances. Our ``"Partition and Code" framework entails three steps: first, a partitioning algorithm decomposes the graph into elementary structures, then these are mapped to the elements of a small dictionary, and finally, an entropy encoder translates the representation into bits. All three steps are parametric and can be trained with gradient descent. Our algorithms are quantitatively evaluated on diverse real-world networks obtaining significant storage gains with respect to different families of non-parametric and parametric graph compressors.

## Spatio-Temporal Joint Graph Convolutional Network for Traffic Forecasting

### TL;DR

A Spatio-Temporal Joint Graph Convolutional Network (STJGCN) for traffic forecasting

### Abstract

Recent studies focus on formulating the traffic forecasting as a spatio-temporal graph modeling problem. They typically construct a static spatial graph at each time step and then connect each node with itself between adjacent time steps to construct the spatio-temporal graph. In such a graph, the correlations between different nodes at different time steps are not explicitly reflected, which may restrict the learning ability of graph neural networks. Meanwhile, those models ignore the dynamic spatio-temporal correlations among nodes as they use the same adjacency matrix at different time steps. To overcome these limitations, we propose a Spatio-Temporal Joint Graph Convolutional Network (STJGCN) for traffic forecasting. We construct spatio-temporal joint graphs (STJG) between any two time steps with both pre-defined and adaptive adjacency matrices, which represent complete and dynamic spatio-temporal correlations. We further design dilated causal spatio-temporal joint graph convolutions on STJG to model the spatio-temporal dependencies from distinct perspectives with multiple ranges. A multi-range attention mechanism is proposed to aggregate the information of different ranges. Experiments on two public traffic datasets demonstrate that STJGCN is computationally efficient and outperforms 12 state-of-the-art baseline methods. 

## Graph Auto-Encoder via Neighborhood Wasserstein Reconstruction

### TL;DR

We propose a new graph autoencoder model for unsupervised graph representation learning that effectively captures node information regarding proximity, structure and features.

### Abstract

Graph neural networks (GNNs) have drawn significant research attention recently, mostly under the setting of semi-supervised learning. When task-agnostic representations are preferred or supervision is simply unavailable, the auto-encoder framework comes in handy with a natural graph reconstruction objective for unsupervised GNN training. However, existing graph auto-encoders are designed to reconstruct the direct links, so GNNs trained in this way are only optimized towards proximity-oriented graph mining tasks, and will fall short when the topological structures matter. In this work, we revisit the graph encoding process of GNNs which essentially learns to reduce the neighborhood information of each node into an embedding vector, and propose a novel graph decoder to reconstruct the entire neighborhood information regarding both proximity and structure via Neighborhood Wasserstein Reconstruction (NWR). Specifically, from the GNN embedding of each node, NWR jointly predicts its node degree and neighbor feature distribution, where the distribution prediction adopts optimal-transport loss based on the Wasserstein distance. Extensive experiments on both synthetic and real-world network datasets show that the unsupervised node representations learned with NWR have much more advantageous in structure-oriented graph mining tasks, while also achieving competitive performance in proximity-oriented ones.

## From Cluster Assumption to Graph Convolution: Graph-based Semi-Supervised Learning Revisited

### TL;DR

None

### Abstract

Graph-based semi-supervised learning (GSSL) has been a hot research topic for decades. Most traditional GSSL methods are shallow learners, based on the cluster assumption. Recently, graph convolutional networks (GCNs) have become the de facto methods for their promising performance. However, despite lots of studies in both directions, few have clarified the fact: why deep GCNs meet the over-smoothing problem, while traditional shallow GSSL methods do not, although they all progress through the graph in a similar iterative manner.
In this paper, we show that compared to traditional methods, GCNs consider a self-loop graph, have a better node embedding initialization, but might be suboptimal for joint learning with graph structure and labels. Motivated by this, we first propose a supervised method OGC which adopts an auxiliary Supervised EmBedding (SEB) operator to guide the convolution process with labels. Then, we propose two unsupervised methods (GGC and its multi-scale version GGCM) both of which preserve graph structure during the convolution process, by introducing another novel operator Inverse Graph Convolution (IGC). Finally, we conduct extensive experiments to demonstrate the effectiveness of our methods.

## Graph-theoretic NAS with latency prediction of arbitrary parallel computation graph

### TL;DR

None

### Abstract

  In Neural Architecture Search (NAS), the inference time of the candidate network is one of the most important evaluation metrics for deploying the network on a hardware device. However, directly measuring the inference runtime of all candidate networks on the hardware device during the search phase is computationally prohibitive.  In this regard, recent NAS methods train a latency predictor network or restrict the search space of the candidate architecture to simple linear stacks of predictable submodule networks. However, training a latency predictor network is not scalable to larger NAS search spaces, and simple linear stacks cannot cover most of the search spaces proposed by previous NAS methods. To this end, we propose a graph-theoretic, scalable, and accurate inference time prediction algorithm for arbitrary computation graphs, which is applicable to any candidate architectures in NAS search spaces. Our algorithm predicts the actual runtime in the presence of parallel GPU task scheduling, where we consider the Nimble framework which supports this scheduling. Furthermore, we propose a scalable and efficient NAS method by jointly optimizing the performance and estimated inference time using our prediction algorithm. In DARTS search space, we show that our methods achieve 30\% faster inference speed while preserving accuracy comparable to DARTS baselines.

## Transfer Learning of Graph Neural Networks with Ego-graph Information Maximization

### TL;DR

We propose a theoretically guaranteed and practically effective GNN model for graph transfer learning

### Abstract

Graph neural networks (GNNs) have achieved superior performance in various applications, but training dedicated GNNs can be costly for large-scale graphs. Some recent work started to study the pre-training of GNNs. However, none of them provide theoretical insights into the design of their frameworks, or clear requirements and guarantees towards their transferability. In this work, we establish a theoretically grounded and practically useful framework for the transfer learning of GNNs. Firstly, we propose a novel view towards the essential graph information and advocate the capturing of it as the goal of transferable GNN training, which motivates the design of EGI (Ego-Graph Information maximization) to analytically achieve this goal. Secondly,
when node features are structure-relevant, we conduct an analysis of EGI transferability regarding the difference between the local graph Laplacians of the source and target graphs. We conduct controlled synthetic experiments to directly justify our theoretical conclusions. Comprehensive experiments on two real-world network datasets show consistent results in the analyzed setting of direct-transfering, while those on large-scale knowledge graphs show promising results in the more practical setting of transfering with fine-tuning.

## Classic Graph Structural Features Outperform Graph Embedding Methods on Community Labeling

### TL;DR

We show that common network embedding methods do a poor job of recreating community structure through experiment and theory.

### Abstract

Graph representation learning (also called graph embeddings) is a popular technique for incorporating network structure into machine learning models. Unsupervised graph embedding methods aim to capture graph structure by learning a low-dimensional vector representation (the embedding) for each node. Despite the widespread use of these embeddings for a variety of downstream transductive machine learning tasks, there is little principled analysis of the effectiveness of this approach for common tasks. In this work, we provide an empirical and theoretical analysis for the performance of a class of embeddings on the common task of pairwise community labeling. This is a binary variant of the classic community detection problem, which seeks to build a classifier to determine whether a pair of vertices participate in a community. In line with our goal of foundational understanding, we focus on a popular class of unsupervised embedding techniques that learn low rank factorizations of a vertex proximity matrix (this class includes methods like GraRep, DeepWalk, node2vec and NetMF).

We perform detailed empirical analysis for community labeling over a variety of real and synthetic graphs with ground truth. In all cases we studied, the models trained from embedding features perform poorly on community labeling. In contrast, a simple logistic model with class graph structural features handily outperforms the embedding models. For a more principled understanding, we provide a theoretical analysis for the (in)effectiveness of these embeddings in capturing the community structure. We formally prove that popular low-dimensional factorization methods either cannot produce community structure, or can only produce "unstable" communities. These communities are inherently unstable under small perturbations. This theoretical result suggests that even though "good" factorizations exist, they are unlikely to be found by computational methods.

## Graph Inference Representation: Learning Graph Positional Embeddings with Anchor Path Encoding

### TL;DR

We propose Graph Inference Representation (GIR), an anchor based GNN model encoding path information related to pre-selected anchors for each node.

### Abstract

Learning node representations that incorporate information from graph structure benefits wide range of tasks on graph. The majority of existing graph neural networks (GNNs) have limited power in capturing position information for a given node. The idea of positioning nodes with selected anchors has been exploited, yet mainly relying on explicit labeling of distance information. Here we propose Graph Inference Representation (GIR), an anchor based GNN model encoding path information related to pre-selected anchors for each node. Abilities to get position-aware embeddings are theoretically and experimentally investigated on GIR and its core variants. Further, the complementarity of GIRs and typical GNNs  is demonstrated. We show that GIRs get outperformed results in position-aware scenarios, and performance on typical GNNs could be improved by fusing GIR embeddings.

## Graph-AAL: Graph Auxiliary Augmentation Learning for Semi-supervised Node Classification

### TL;DR

None

### Abstract

Graph node classification has become an important research topic in graph-structured data analysis in recent years. Since there are always a few training samples, some researchers improve the performance by properly leveraging the predictions of unlabeled nodes (pseudo-labels) during the training. However, they suffer from the model degradation resulted from the accumulative error of pseudo-labels. Therefore only a few unlabeled nodes of extremely high quality can be selected to participate in the training, which leads to limited improvement. In this paper we propose graph auxiliary augmentation learning (Graph-AAL). It boosts the performance of the primary task with the help of an auxiliary node classification task which is trained by much more unlabeled nodes of higher pseudo-label quality through a multi-task graph neural network (GNN). The targets of the auxiliary task are generated based on the primary labels and another GNN which is optimized by the primary task performance. And an auxiliary augmentation strategy is designed to enlarge the labeled set for the auxiliary task by utilizing the primary pseudo-labels of the unlabeled nodes, which alleviates the sensitivity of the model to the pseudo-label quality. Extensive experiments verify the superior performance of the Graph-AAL on different GNN architectures when compared with other state-of-the-art approaches.

## Time-aware Relational Graph Attention Network for Temporal Knowledge Graph Embeddings

### TL;DR

None

### Abstract

Embedding-based representation learning approaches for knowledge graphs (KGs) have been mostly designed for static data. However, many KGs involve temporal data, which creates the need for new representation learning approaches that can characterize and reason over time. In this work, we propose a Time-aware Relational Graph ATtention Network (TR-GAT) for temporal knowledge graph (TKG) embeddings, in which the initial feature of each entity is represented by fusing its embedding and the embeddings of its connected relations and timestamps as well as its neighboring entities. Different from the existing temporal GNN models which discretize temporal graphs into multiple snapshots, we treat timestamps as properties of links between entities. To further incorporate relation and time information into the graph structures, we utilize a self-attention mechanism which specifies different weights to different nodes according to the corresponding link features, i.e., embeddings of the relevant relations and timestamps within one neighborhood. Experimental results show that our approach achieves state-of-the-art performances regarding TKG completion and entity alignment tasks on several well-established TKG datasets due to the effective and efficient integration of time information.

## Unbiased Augmentation for Graph Learning

### TL;DR

We propose two novel approaches for graph augmentation, which make unbiased changes of graph properties.

### Abstract

Graph augmentation is an essential strategy to improve the generalizability of graph neural networks by enlarging the distribution covered by training data. However, previous works for graph augmentation rely on simple heuristics such as adding or removing random edges, changing the essential properties of graphs with little effort to preserve semantic information. In this work, we propose NodeSam (node split and merge) and SubMix (subgraph mix), two novel approaches for unbiased graph augmentation with different motivations. NodeSam augments the node set preserving the connectivity between nodes to minimize the risk of semantic change, while SubMix mixes random subgraphs of multiple graphs to make rich soft labels combining the evidences for different classes. Our experiments on seven datasets show that NodeSam and SubMix outperform baselines and previous approaches for graph augmentation in terms of the accuracy of graph classification, as a result of preserving the original properties during augmentation. Our code and datasets are available at https://github.com/graphaugment/graph_augmentation.


## GINA: Neural Relational Inference From Independent Snapshots

### TL;DR

We propose a fast, yet powerful GNN architecture for the graph inference (network reconstruction) problem for interacting systems from independent observational data.

### Abstract

Dynamical systems in which local interactions among agents give rise to complex emerging phenomena are ubiquitous in nature and society.
This work explores the problem of inferring the unknown interaction structure (represented as a graph) of such a system from measurements of its constituent agents or individual components (represented as nodes). 
We consider a setting where the underlying dynamical model is unknown and where different measurements (i.e., snapshots) may be independent (e.g., may stem from different experiments).
We propose GINA (Graph Inference Network Architecture), a graph neural network (GNN) to simultaneously learn the latent interaction graph and, conditioned on the interaction graph, the prediction of a node's observable state based on adjacent vertices. 
GINA is based on the hypothesis that the ground truth interaction graph---among all other potential graphs---allows to predict the state of a node, given the states of its neighbors, with the highest accuracy. 
We test this hypothesis and demonstrate GINA's effectiveness on a wide range of interaction graphs and dynamical processes.

## Adaptive Diffusion in Graph Neural Networks

### TL;DR

None

### Abstract

The success of graph neural networks (GNNs) largely relies on the process of aggregating information from neighbors defined by the input graph structures. Notably, message passing based GNNs, e.g., graph convolutional networks, leverage the immediate neighbors of each node during the aggregation process, and recently, graph diffusion convolution (GDC) is proposed to expand the propagation neighborhood by leveraging generalized graph diffusion. However, the neighborhood size in GDC is manually tuned for each graph by conducting grid search over the validation set, making its generalization practically limited. To address this issue, we propose the adaptive diffusion convolution (ADC) strategy to automatically learn the optimal neighborhood size from the data. Furthermore, we break the conventional assumption that all GNN layers and feature channels (dimensions) should use the same neighborhood for propagation. We design strategies to enable ADC to learn a dedicated propagation neighborhood for each GNN layer and each feature channel, making the GNN architecture fully coupled with graph structures---the unique property that differs GNNs from traditional neural networks. By directly plugging ADC into existing GNNs, we observe consistent and significant outperformance over both GDC and their vanilla versions across various datasets, demonstrating the improved model capacity brought by automatically learning unique neighborhood size per layer and per channel in GNNs. 

## Generating a Doppelganger Graph: Resembling but Distinct

### TL;DR

a new graph generative method

### Abstract

Deep generative models, since their inception, have become increasingly more capable of generating novel and perceptually realistic signals. With the emergence of deep models for graph structured data, natural interests seek extensions of these generative models for graphs. Successful extensions were seen recently in the case of learning from a collection of graphs (e.g., protein data banks), but the learning from a single graph has been largely under explored. The latter case, however, is important in practice. For example, graphs in financial and healthcare systems contain so much confidential information that their public accessibility is nearly impossible, but open science in these fields can only advance when similar data are available for benchmarking.

In this work, we propose an new approach to generate doppelganger graphs that resembles a given one in many graph properties. The approach is the first one which is an orchestration of graph representation learning, generative adversarial networks, and traditional graph realization algorithms to generate new nodes. Through comparison with several graph generative models, we demonstrate that our result closely matches the properties of origin graph. We further show that downstream tasks, such as node classification, on the generated graphs reach similar performance to the use of the original ones.

## Improving Subgraph Recognition with Variational Graph Information Bottleneck

### TL;DR

None

### Abstract

Subgraph recognition aims at discovering a compressed yet informative substructure from a graph.  It can be formulated by optimizing Graph Information Bottleneck (GIB) with a mutual information estimator.  The estimator enforces the found subgraph to capture actionable information and drop irrelevant information.  However, GIB suffers from training instability since the mutual information of graph data is intrinsically difficult to estimate.  In this paper, we introduce a noise injection method to quantify the information in subgraphs, which leads to a novel Variational Graph Information Bottleneck (VGIB) framework.  VGIB allows a tractable variational approximation to its objective under mild assumptions. Therefore, it is more stable and efficient to train – we find that VGIB converges 10 times faster than VIB in practice. Moreover, experiments on graph interpretation and classification show that VGIB finds better subgraphs than existing methods.

## Effective Design of Adaptive Graph Filters for Heterophilic Networks

### TL;DR

Effective Eigendecomposition based Graph Adaptation for Heterophilic Networks

### Abstract

Graph Neural Networks (GNNs) exhibit excellent performance when graphs have strong homophily property, i.e. connected nodes have the same labels. However, they perform poorly on heterophilic graphs. Several approaches address the issue of heterophily by proposing models that adapt the graph by optimizing task-specific loss function using labelled data. These adaptations are made either via attention or by attenuating or enhancing various low-frequency/high-frequency signals, as needed for the task at hand. More recent approaches adapt the eigenvalues of the graph. One important interpretation of this adaptation is that these models select/weigh the eigenvectors of the graph. Based on this interpretation, we present an eigendecomposition based approach and propose \eigennet~models that improve the performance of GNNs on heterophilic graphs. Performance improvement is achieved by learning flexible graph adaptation functions that modulate the eigenvalues of the graph. Regularization of these functions via parameter sharing helps to improve the performance even more. Our approach achieves up to 11\% improvement in performance over the state-of-the-art methods on heterophilic graphs.

## Robust Optimization as Data Augmentation for Large-scale Graphs

### TL;DR

None

### Abstract

Data augmentation helps neural networks generalize better by enlarging the training set, but it remains an open question how to effectively augment graph data to enhance the performance of GNNs (Graph Neural Networks). While most existing graph regularizers focus on manipulating graph topological structures by adding/removing edges, we offer a method to augment node features for better performance. We propose FLAG (Free Large-scale Adversarial Augmentation on Graphs), which iteratively augments node features with gradient-based adversarial perturbations during training. By making the model invariant to small fluctuations in input data, our method helps models generalize to out-of-distribution samples and boosts model performance at test time. FLAG is a general-purpose approach for graph data, which universally works in node classification, link prediction, and graph classification tasks. FLAG is also highly flexible and scalable, and is deployable with arbitrary GNN backbones and large-scale datasets. We demonstrate the efficacy and stability of our method through extensive experiments and ablation studies. We also provide intuitive observations for a deeper understanding of our method.

## Similarity-aware Positive Instance Sampling for Graph Contrastive Pre-training

### TL;DR

None

### Abstract

Graph instance contrastive learning has been proved as an effective task for Graph Neural Network (GNN) pre-training. However, one key issue may seriously impede the representative power in existing works: Positive instances created by current methods often miss  crucial information of graphs or even yield illegal instances (such as non-chemically-aware graphs in molecular generation). To remedy this issue,  we propose to select positive graph instances directly from existing graphs in the training set, which ultimately maintains the legality and similarity to the target graphs. Our selection is based on certain domain-specific pair-wise similarity measurements as well as sampling from a hierarchical graph encoding similarity relations among graphs. Besides, we develop an adaptive node-level pre-training method to dynamically mask nodes to distribute them evenly in the graph. We conduct extensive experiments on $13$ graph classification and node classification benchmark datasets from various domains. The results demonstrate that the GNN models pre-trained by our strategies can outperform those trained-from-scratch models as well as the variants obtained by existing methods. 

## The Infinite Contextual Graph Markov Model

### TL;DR

We propose a deep probabilistic model for graphs that automatically adjusts the size of its latent representations.

### Abstract

The Contextual Graph Markov Model is a deep, unsupervised, and probabilistic model for graphs that is trained incrementally on a layer-by-layer basis. As with most Deep Graph Networks, one inherent limitation is the lack of a mechanism to learn the size of each layer's latent representation. In this paper, we address such a limitation by extending the Contextual Graph Markov Model with Hierarchical Dirichlet Processes. The result is the first unsupervised and probabilistic model for graphs that can automatically adjust the size of its latent representations up to a possibly infinite number of states. Furthermore, we introduce a novel approximated inference procedure to better scale to larger graph topologies. The model is evaluated across a set of eight graph classification tasks, showing competitive performances against end-to-end supervised methods. The analysis is complemented by studies on the importance of depth, hyper-parameters, and compression of the graph embeddings. We believe this to be an important step towards the theoretically grounded and automatic construction of deep probabilistic architectures for graphs.

## DynG2G: An Efficient Stochastic Graph Embedding Method for Temporal Graphs

### TL;DR

None

### Abstract

Dynamic graph embedding has gained great attention recently due to its capability of learning low-dimensional and meaningful graph representations for complex temporal graphs with high accuracy. However, recent advances mostly focus on learning node embeddings as deterministic ``vectors for static graphs, hence disregarding the key graph temporal dynamics and the evolving uncertainties associated with node embedding in the latent space. In this work, we propose an efficient stochastic dynamic graph embedding method (DynG2G) that applies an inductive feed-forward encoder trained with node triplet energy-based ranking loss. Every node per timestamp is encoded as a time-dependent probabilistic multivariate Gaussian distribution in the latent space. Our experiments based on four large dynamic graph benchmarks demonstrate that DynG2G achieves new state-of-the-art performance in capturing the underlying temporal node embeddings. Additionally, the obtained graph Gaussian embedding variance plays a central role in uncertainty quantification for node embedding, and reveals the optimum embedding size at each timestamp, which is related to the intrinsic dimension in latent space.

## MLP is All You Need: Node Classification  without Message Passing in Graph

### TL;DR

MLP is All You Need: Node Classification  without Message Passing in Graph

### Abstract

Graph Neural Network (GNN) has been demonstrated its effectiveness in dealing with non-Euclidean structural data. Both spatial-based and spectral-based GNNs are relying on adjacency matrix to guide message passing among neighbors during feature aggregation. Recent works have mainly focused on powerful message passing modules, however, in this paper, we show that none of the message passing modules is necessary.  Instead, we propose a pure multilayer-perceptron-based framework, Graph-MLP with the supervision signal leveraging graph structure, which is sufficient for learning discriminative node representation. In model-level, Graph-MLP only includes multi-layer perceptrons, activation function, and layer normalization. In the loss level, we design a neighboring contrastive (NContrast) loss to bridge the gap between GNNs and MLPs by utilizing the adjacency information implicitly. This design allows our model to be lighter and more robust when facing large-scale graph data and corrupted adjacency information. Extensive experiments prove that even without adjacency information in testing phase, our framework can still reach comparable and even superior performance against the state-of-the-art models in the graph node classification task.

## Bringing Your Own View: Graph Contrastive Learning without Prefabricated Data Augmentations

### TL;DR

None

### Abstract

Self supervision is recently surging at its new frontier of graph learning.  It facilitates graph representations beneficial to downstream tasks; but its success could hinge on domain knowledge for handcraft or the often expensive trials and errors.  Even its state-of-the-art representative, graph contrastive learning (GraphCL), is not completely free of those needs as GraphCL uses a prefabricated prior reflected by the ad-hoc manual selection of graph data augmentations. Our work aims at advancing GraphCL by answering the following questions: How to represent the space of graph augmented views? What principle can be relied upon to learn a prior in that space? And what framework can be constructed to learn the prior in tandem with contrastive learning? Accordingly we have extended the prefabricated discrete prior in the augmentation set, to a learnable continuous prior in the parameter space of graph generators, assuming that graph priors \textit{per se}, similar to the concept of image manifolds, can be learned by data generation. Furthermore, to form contrastive views without collapsing to trivial solutions due to the prior learnability, we have leveraged both principles of information minimization (InfoMin) and information bottleneck (InfoBN) to regularize the learned priors. Eventually, contrastive learning, InfoMin, and InfoBN are incorporated organically into one framework of bi-level optimization. Our principled and automated approach has proven to be competitive against the state-of-the-art graph self-supervision methods, including GraphCL, on benchmarks of small graphs; and shown even better generalizability on large-scale graphs, without resorting to human expertise or downstream validation. Codes will be released.

## Pure Exploration in Multi-armed Bandits with Graph-Side Information

### TL;DR

None

### Abstract

In this paper, we study pure exploration in multi-armed bandits with graph side-information. In particular, we consider the fixed confidence setting under the assumption that the arm rewards are smooth with respect to a given graph. This captures a range of real world pure-exploration scenarios where one often has information about the similarity of the options under consideration. We propose a novel algorithm GECO (Graph Exploration via COnfidence-based elimination) in this scenario and provide a theoretical characterization of its performance. Our theory elicits the benefit of the graph-side information in terms of a novel complexity measure that captures the difficulty of the underlying problem. We complement our theory with  experimental results that show that using graph side information, when available, yields significant improvements over pure exploration methods that are unable to capitalize on this information.

## Compact Graph Neural Networks via Intra-Group Sparse Aggregation

### TL;DR

None

### Abstract

Graph Neural Networks (GNNs) have been demonstrated very effective in many graph learning tasks. Existing GNNs usually conduct the layer-wise message propagation via the aggregation of all neighborhood information which thus are often sensitive to the structural noises in the graph, such as incorrect or undesired redundant edge connections. To overcome this drawback, inspired by the field of sparse representation/coding, we propose a novel GNN to deal with noisy graph learning. The proposed GNN method adaptively selects the reliable neighbors for each graph node by leveraging an intra-group LASSO and aggregates the message of selected neighbors to learn robust representation for the downstream learning tasks, e.g., node classification, clustering etc. An effective algorithm is derived to optimize/train the proposed GNN model. Experimental results on several datasets demonstrate the effectiveness and robustness of our model.

## RQG: A Ring-based and Quantization-oriented Graph for the Maximum Inner Product Search

### TL;DR

We proposed a new graph strucure for accelerating the approximate maximum inner product search.

### Abstract

\emph{Maximum inner product search}(MIPS) is a fundamental problem which appears in various applications of data mining and machine learning. In this paper, we propose a novel proximity-graph structure called \emph{Ring-based and Quantization-oriented Graph}(RQG) to solve the approximate MIPS problem. RQG has a hierarchical ring structure and is built over the base layer of any existing graph structure. All graph nodes in RQG are represented by quantizers which are generated by a novel quantization technique called \emph{Weighted Product Quantization}(WPQ). In the query phase, by means of two pruning rules, RQG can reach some promising entering points for the base-layer search efficiently and accurately. Experimental results show that, by the combination with RQG, the recall rates of existing graph-based methods can be improved by 4\%-20\% generally.

## NN-Baker: A Neural-network Infused Algorithmic Framework for Optimization Problems on Geometric Intersection Graphs

### TL;DR

We propose a new framework for solving geometric intersection graphs using neural networks and provide experimental evidence

### Abstract

Recent years have witnessed a surge of approaches to use neural networks to help tackle combinatorial optimization problems, including graph optimization problems. However, theoretical understanding of such approaches remains limited. In this paper, we consider the geometric setting, where graphs are induced by points in a fixed dimensional Euclidean space. We show that several graph optimization problems can be approximated by an algorithm that is polynomial in graph size n via a framework we propose, call the Baker-paradigm. More importantly, a key advantage of the Baker-paradigm is that it decomposes the input problem into (at most linear number of) small sub-problems of fixed sizes (independent of the size of the input). For the family of such fixed-size sub-problems, we can now design neural networks with universal approximation guarantees to solve them. This leads to a mixed algorithmic-ML framework, which we call NN-Baker that has the capacity to approximately solve a family of graph optimization problems (e.g, maximum independent set and minimum vertex cover) in time linear to input graph size, and only polynomial to approximation parameter. We instantiate our NN-Baker by a CNN version and GNN version, and demonstrate the effectiveness and efficiency of our approach via a range of experiments.

## T-LoHo: A Bayesian Regularization Model for Structured Sparsity and Smoothness on Graphs

### TL;DR

A Bayesian shrinkage and clustering prior is proposed to simultaneously detect structured sparsity and smoothness on graphs.

### Abstract

 Many modern complex data can be represented as a graph. In models dealing with graph-structured data, multivariate parameters are not just sparse but have structured sparsity and smoothness in the sense that both zero and non-zero parameters tend to cluster together. We propose a new prior for high dimensional parameters with graphical relations, referred to as a Tree-based Low-rank Horseshoe(T-LoHo) model, that generalizes the popular univariate Bayesian horseshoe shrinkage prior to the multivariate setting to detect structured sparsity and smoothness simultaneously. The prior can be embedded in many hierarchical high-dimensional models. To illustrate its utility, we apply it to regularize a Bayesian high-dimensional regression problem where the regression coefficients are linked on a graph. The resulting clusters have flexible shapes and satisfy the cluster contiguity constraint with respect to the graph. We design an efficient Markov chain Monte Carlo algorithm that delivers full Bayesian inference with uncertainty measures for model parameters including the number of clusters. We offer theoretical investigations of the clustering effects and posterior concentration results. Finally, we illustrate the performance of the model with simulation studies and real data applications such as anomaly detection in road networks. The results indicate substantial improvements over other competing methods such as sparse fused lasso.

## SADGA: Structure-Aware Dual Graph Aggregation Network for Text-to-SQL

### TL;DR

None

### Abstract

The Text-to-SQL task, aiming to translate the natural language of the questions into SQL queries, has drawn much attention recently.  One of the most challenging problems of Text-to-SQL is how to generalize the trained model to the unseen databases schema, also known as the cross-domain Text-to-SQL task. The key lies in the generalizability of (i) the encoding method to model the question and the database schema and (ii) the question-schema linking method to learning the mapping between the words in the question and the tables/columns in the database schema. Focusing on the above two key issues, we propose a \emph{Structure-Aware Dual Graph Aggregation Network} (SADGA) for the cross-domain Text-to-SQL task. In SADGA, we adopt the graph structure to provide a unified encoding model for both the natural language question and database schema. Based on the proposed unified modeling, we further devise a structure-aware aggregation method to learn the mapping between the question-graph and schema-graph. The structure-aware aggregation method is featured with \emph{Global Graph Linking}, \emph{Local Graph Linking} and \emph{Dual-Graph Aggregation Mechanism}.  We not only study the performance of our proposal empirically but also achieve 3rd place on the challenging Text-to-SQL benchmark Spider.

## RGP: Neural Network Pruning through Its Regular Graph Structure

### TL;DR

A graph-guided pruning framework is proposed to use a specific regular graph structure  to prune neural networks.

### Abstract

 Lightweight model design has become an important direction in the application of deep learning technology, pruning is an effective mean to achieve a large reduction in model parameters and FLOPs. The existing neural network pruning methods mostly start from the importance of parameters, and design parameter evaluation metrics to perform parameter pruning iteratively. These methods are not studied from the perspective of model topology, may be effective but not efficient, and requires completely different pruning for different datasets. In this paper, we study the graph structure of the neural network, and propose regular graph based pruning (RGP) to perform a one-shot neural network pruning. We generate a regular graph, set the node degree value of the graph to meet the pruning ratio, and reduce the average shortest path length of the graph by swapping the edges to obtain the optimal edge distribution. Finally, the obtained graph is mapped into a neural network structure to realize pruning. Experiments show that the average shortest path length of the graph is negatively correlated with the classification accuracy of the corresponding neural network, and the proposed RGP shows a strong precision retention capability with extremely high parameter reduction (more than 90%) and FLOPs reduction (more than 90%). The code that reproduces the results of this paper is available at https://anonymous.4open.science/r/Neural-Network-Pruning-through-its-RegularGraph-Structure-BB71

## Geometric Random Walk Graph Neural Networks via Implicit Layers

### TL;DR

None

### Abstract

Graph neural networks (GNNs) have recently attracted a lot of attention and have been applied with great success to several important graph problems. The Random Walk Graph Neural Network (RWNN) model was recently proposed as a more intuitive alternative to the well-studied family of message passing neural networks. This model compares each input graph against a set of latent ``hidden graphs'' using a kernel that counts common random walks up to some length. In this paper, we propose a new architecture, called Geometric Random Walk Graph Neural Network (GRWNN), that generalizes the above model such that it can count common walks of infinite length in two graphs. The proposed model retains the transparency of RWNN since its first layer also consists of a number of trainable ``hidden graphs'' which are compared against the input graphs using the geometric random walk kernel. To compute the kernel, we employ a fixed-point iteration approach involving implicitly defined ``state'' vectors. Then, we capitalize on implicit differentiation to derive an efficient training scheme which requires only constant memory, regardless of the number of fixed-point iterations. The employed random walk kernel is differentiable, and therefore, the proposed model is end-to-end trainable. Experiments on standard graph classification and graph regression datasets demonstrate the effectiveness of the proposed approach in comparison with state-of-the-art methods.

## Graph Sylvester Embeddings for Network Analysis

### TL;DR

None

### Abstract

We introduce Graph Sylvester Embedding (GSE), an unsupervised graph representation of local similarity, connectivity, and global structure. GSE uses the solution of the Sylvester equation to capture both network structure and neighborhood proximity in a single representation. Unlike embeddings based on the eigenvectors of the Laplacian, Graph Sylvester Embeddings incorporate two or more basis functions, for instance using the Laplacian and affinity matrix. Such basis functions are constructed not from the original graph, but from one whose weights measure the centrality of an edge (the fraction of the number of shortest paths that pass through that edge) in the original graph. This allows more flexibility and control to represent complex network structure and shows significant improvements over the state of the art when used for data analysis tasks such as predicting failed edges in material science and network alignment in the human-SARS CoV-2 protein-protein interactome. 

## Semi-Parametric Contextual Bandits with Graph-Laplacian Regularization

### TL;DR

A novel contextual bandit for non-stationary, socially connected users.

### Abstract

Non-stationarity is ubiquitous in human behavior and addressing it in the contextual bandits is challenging.  Several works have addressed the problem by investigating semi-parametric contextual bandits and warned that ignoring non-stationarity could harm performances.  Another prevalent human behavior is social interaction which has become available in a form of a social network or graph structure. As a result, graph-based contextual bandits have received much attention.  In this paper, we propose a novel contextual Thompson-sampling algorithm for a graph-based semi-parametric reward model.  Our algorithm is the first to be proposed in this setting. We derive an upper bound of the cumulative regret that can be expressed as a multiple of a factor depending on the graph structure and the order for the semi-parametric model a without graph.  We evaluate the proposed and existing algorithms via simulation and two real data examples.

## LASA: Label-aware Spectral Batching for Training Large-Scale Graph Neural Networks

### TL;DR

None

### Abstract

Graph batching has become the most efficient way for  training  large-scale graph neural networks (GNNs) with stochastic optimizers. The main idea is to construct subgraphs as minibatches and then conduct forward-backward propagation within each batch for parameter update. Using graph clustering to construct minibatch has drawn much attention for speeding up the GNN training. In this paper, we point out existing graph clustering based batching methods do not consider label imbalance issue, which may lead to inaccurate gradient estimations and slow down the convergence of GNN training significantly.
 To resolve this problem, we propose a novel label-aware  clustering objective for graph batching, and develop an efficient algorithm for optimizing this objective with a combination of spectral clustering, randomized SVD and hierarchical balanced kmeans. This new graph batching algorithm introduces limited overhead, while significantly improves the training speed across many large-scale datasets. For instance, on Amazon2M dataset with more than 2 million nodes, our proposed batching algorithm can achieve 89.11% accuracy with only 62.6 seconds of model training, while Cluster-GCN, a state-of-the-art fast GNN training algorithm takes 232.6 seconds for model training achieving 89.06% accuracy.

## NeuroSED: Learning Subgraph Similarity via GraphNeural Networks

### TL;DR

None

### Abstract

Subgraph similarity search is one of the fundamental operators in graph analysis. In this framework, given a query graph and a database of graphs, the goal is to identify all subgraphs of the database graphs that are structurally similar to the query. Subgraph edit distance (SED) is one of the most expressive mechanisms to quantify subgraph similarity. In this work, we study the problem of learning SED from a train set of graph pairs and their corresponding SED values. Towards that end, we design a novel siamese graph neural network called NEUROSED, which embeds graphs in an embedding space with a rich structure reminiscent of SED. Furthermore, with the help of a specially crafted inductive bias,  NEUROSED not only enables high accuracy but also transfers triangle inequality from the original space to the embedding space. The architecture of  NEUROSED is generic enough to model graph edit distance (GED) while ensuring that the predicted GED space remains metric. Extensive experiments on real graph datasets establish that the RMSE obtained by  NEUROSED is, on average, 2 times lower than the state of the art. In addition,  NEUROSED is up to 34 times faster than the fastest baseline in computing SED.

## Scalable Deep Graph Clustering With Bidirectional Propagation

### TL;DR

this paper proposes a new scalable graph clustering method based on bidirectional propagation and deep learning

### Abstract

In the clustering domain,  a popular trend has been to  apply deep learning approaches to boost clustering performance. For example, multiple efforts have  combined  the node attributes (features) and graph structural information by applying GCN (Graph Convolutional Networks) in the training process.  However, GCNs currently  suffer from scalability issues, and thus the GCN based graph clustering methods cannot handle  large  graphs. In this paper,  inspired by GBP (Graph neural network via Bidirectional Propagation)  that  decouples prediction and propagation processes, we build  a highly scalable deep clustering framework GBP-SDCN,  on top of SDCN (Structural Deep Clustering Network). GBP precomputation can reweigh  node attributes into a powerful representation by incorporating the structural information from the entire  graph.  Subsequently, such a  representation is used as the input for a multi-layer neural network with mini-batch training for clustering purpose. Based on extensive empirical studies  using  6  datasets and  6 clustering metrics, we show that GBP-SDCN achieved improved results over   several most recent baselines. Most notably  we demonstrate that GBP-SDCN, unlike all other state-of-art deep clustering  frameworks, could  handle a 1.8 billion edge graph on a single GPU. 

## Contrastive Graph Poisson Networks: Semi-Supervised Learning with Extremely Limited Labels

### TL;DR

None

### Abstract

		Graph Neural Networks (GNNs) have achieved remarkable performance in the task of semi-supervised node classification. However, most existing GNN models require sufficient labeled data for effective network training. Their performance can be seriously degraded when labels are extremely limited. To address this issue, we propose a new framework termed Contrastive Graph Poisson Networks (CGPN) for node classification under extremely limited labeled data. Specifically, our CGPN derives from variational inference; integrates a newly designed Graph Poisson Network (GPN) to effectively propagate the limited labels to the entire graph and a normal GNN, such as Graph Attention Network, that flexibly guides the propagation of GPN; applies a contrastive objective to further exploit the supervision information from the learning process of GPN and GNN models. Essentially, our CGPN can enhance the learning performance of GNNs under extremely limited labels by contrastively propagating the limited labels to the entire graph. We conducted extensive experiments to demonstrate the superiority of CGPN. Specifically, when given only one label per category, our method significantly outperforms the second best competitors by 9.2\%, 23.5\%, and 6.7\% on Cora, CiteSeer, and PubMed datasets, respectively. Codes are provided in the supplemental materials and will be released on GitHub.

## Message Passing in Graph Convolution Networks via Adaptive Filter Banks

### TL;DR

None

### Abstract

Graph convolution networks have been a powerful tool in representation learning of networked data. However, most popular message passing graph convolution networks (MPGCNs) are limited to employing a single strategy to handle multi-channel graph signals that usually encode multifaceted information, and to exploiting low-frequency information.   In this paper, we present a novel graph convolution operator, termed BankGCN, which keeps benefits of MP (message passing), but extends capabilities beyond `low-pass' features. It decomposes multi-channel signals on graphs into subspaces and handles particular information in each subspace with an adaptive filter. These filters of all subspaces have different frequency responses and together form a filter bank. Furthermore,  each filter in the spectral domain corresponds to an MP scheme, and  a group of diverse MP schemes are employed via the filter bank. Importantly, the filter bank and the signal decomposition are jointly learned to adapt to the spectral characteristics of data and target applications. This is implemented almost without extra parameters in comparison with most existing MPGCNs. Experimental results show that the proposed convolution operator achieves excellent performance in graph classification on a collection of benchmark graph datasets.

## Self-Supervised Graph Learning with Proximity-based Views and Channel Contrast

### TL;DR

We propose a simple but powerful self-supervised graph representation learning model with novel graph augmentation and less contrastive pairs.

### Abstract

We consider graph representation learning in a self-supervised manner. Graph neural networks (GNNs) use neighborhood aggregation as a core component that results in feature smoothing among nodes in proximity. While successful in various prediction tasks, such a paradigm falls short of capturing nodes' similarities over a long distance, which proves to be important for high-quality learning. To tackle this problem, we strengthen the graph with two additional graph views, in which nodes are directly linked to those with the most similar features or local structures. Not restricted by connectivity in the original graph, the generated views allow the model to enhance its expressive power with new and complementary perspectives from which to look at the relationship between nodes. Following a contrastive learning approach, We propose a method that aims to maximize the agreement between representations across generated views and the original graph. We also propose a channel-level contrast approach that greatly reduces computation cost, compared to the commonly used node level contrast, which requires computation cost quadratic in the number of nodes. Extensive experiments on seven assortative graphs and four disassortative graphs demonstrate the effectiveness of our approach. 

## Semantic Consistency Guided Feature Fusion for Graphs

### TL;DR

None

### Abstract

It has been reported in prior work that graph neural networks(GNNs) suffer from the heterophily of class label distribution in semi-supervised node classification task, the phenomenon that nodes belonging to different classes are more likely to link to each other. In this work, we discuss the limitation of the heterophily in distinguishing the properties of graph data, and claim that a measurable intrinsic factor is the semantic consistency of node attributes with graph structure. Motivated by this observation, we
 introduce a new feature fusion scheme without convolution on the neighbors, by considering both the semantic differences between node attributes and graph structure, and the correlation of the two sides. Specifically, we translate the correlation of node attributes with its local structure into the distribution of clustering labels on the ego-graph with a certain radius, where the higher-order interaction information among neighbors is captured. Then the ego-graph features are further fused with the original node attributes by concatenation. Experiments on real-world datasets show that linear classifier (e.g. multilayer perceptrons) based on our feature fusion achieves strong performance across the datasets, especially on the datasets with low semantic consistency, compared to vanilla GNNs.

## Beltrami Flow and Neural Diffusion on Graphs

### TL;DR

A generalisation of Graph Neural Networks using Beltrami Flow

### Abstract

We propose a novel class of graph neural networks based on the discretized Beltrami flow, a non-Euclidean diffusion PDE. 
In our model, node features are supplemented with  positional encodings derived from the graph topology and jointly evolved by the Beltrami flow,  producing simultaneously continuous feature learning, topology evolution. 
The resulting model generalizes many popular graph neural networks and achieves state-of-the-art results on several benchmarks. 

## Slow Learning and Fast Inference: Efficient Graph Similarity Computation via Knowledge Distillation

### TL;DR

None

### Abstract

Graph Similarity Computation (GSC) is essential to wide-ranging graph applications such as retrieval, plagiarism/anomaly detection, etc. The exact computation of graph similarity, e.g., Graph Edit Distance (GED), is an NP-hard problem that cannot be exactly solved within an adequate time given large graphs. Thanks to the strong representation power of graph neural network (GNN), a variety of GNN-based inexact methods emerged. To capture the subtle difference across graphs, the key success is designing the dense interaction, which, however, is a trade-off between speed and accuracy. For Slow Learning of graph similarity, this paper proposes a novel early-fusion approach by designing a co-attention-based feature fusion network on multilevel GNN features. To further improve the speed without much accuracy drop, we introduce an efficient GSC solution by distilling the knowledge from the slow early-fusion model to the student one for Fast Inference. Such a student model also enables the offline collection of individual graph embeddings, speeding up the inference time in orders. To address the instability through knowledge transfer, we decompose the dynamic joint embedding into the static pseudo individual ones for precise teacher-student alignment. The experimental analysis on the real-world datasets demonstrates the superiority of our approach over the state-of-the-art methods on both accuracy and efficiency. Particularly, we speed up the prior art by 65x on the benchmark AIDS data.

## Data driven semi-supervised learning

### TL;DR

We present a novel data driven approach for learning the graph with strong formal guarantees in both the distributional and online learning formalizations - this overcomes the major drawback of classic graph-based semi-supervised learning techniques.

### Abstract

We consider a novel data driven approach for designing semi-supervised learning algorithms that can effectively learn with only a small number of labeled examples. We focus on graph-based techniques, where the unlabeled examples are connected in a graph  under the implicit assumption that similar nodes likely have similar labels. Over the past two decades, several elegant graph-based semi-supervised learning algorithms for inferring the labels of the unlabeled examples given the graph and a few labeled examples have been proposed. However, the problem of how to create the graph (which impacts the practical usefulness of these methods significantly) has been relegated to heuristics and domain-specific art, and no general principles have been proposed. In this work we present a  novel data driven approach for learning the graph and provide strong formal guarantees in both the distributional and online learning formalizations. We show how to leverage problem instances coming from an underlying problem domain to learn the graph hyperparameters for commonly used parametric families of graphs that provably perform well on new instances from the same domain. We obtain low regret and efficient algorithms in the online setting, and generalization guarantees in the distributional setting. We also show how to combine several very different similarity metrics and learn multiple  hyperparameters, our results hold for large classes of problems. We expect some of the tools and techniques we develop along the way to be of independent interest, for data driven algorithms more generally.

## Is Solving Graph Neural Tangent Kernel Equivalent to Training Graph Neural Network?

### TL;DR

None

### Abstract

Graph Neural Networks (GNNs) have become the de facto method for machine learning on graph data, including suggesting people to connect in a social network, predicting biological properties with protein structures, and finding software bugs using source code's abstract syntax tree.
Meanwhile, a rising trend in theoretical deep learning is to understand why deep learning works through Neural Tangent Kernel (NTK)~\cite{jgh18}, a kernel method which is equivalent to using gradient descents to train a multi-layer infinite-wide neural network. NTK is a major step forward in the theoretical deep learning community because it allows researchers to use traditional mathematical tools to analyze properties about deep neural networks and to explain various neural network techniques from a theoretical view. A natural extension of NTK on graph learning is \textit{Graph Neural Tangent Kernel (GNTK)}, and GNTK has shown promising accuracy compared to GNNs empirically in various bioinformatics datasets~\cite{dhs+19}. The remaining question now is whether solving GNTK regression is equivalent to training an infinite-wide multi-layer GNN using gradient descent.
In this paper, we provide an affirmative answer to this question.
Answering this question is more challenging than the equivalence result for NTK: a GNN's input is much more complex, consisting of a graph with feature vectors on each node in the graph, and the training process of a GNN is also more complicated than training a typical neural network. This requires us to develop new proof techniques to establish the equivalence between GNN and GNTK.

## Representing Long-Range Context for Graph Neural Networks with Global Attention

### TL;DR

We generalize the Transformer structure to improve long-range context for graph neural networks to achieve a new SOTA on the OpenGraphBenchmark.

### Abstract

Graph neural networks are powerful architectures for structured datasets. However, current methods struggle to represent long-range dependencies. Scaling the depth or width of GNNs is insufficient to broaden receptive fields as larger GNNs encounter optimization instabilities such as vanishing gradients and representation oversmoothing, while pooling-based approaches have yet to become as universally useful as in computer vision. In this work, we propose the use of Transformer-based self-attention to learn long-range pairwise relationships, with a novel "readout" mechanism to obtain a global graph embedding. Inspired by recent computer vision results that finds position-invariant attention performant in learning long-range relationships, our method, which we call GraphTrans, applies a permutation-invariant Transformer module after a standard GNN module. This simple architecture leads to state-of-the-art results on several graph classification tasks, outperforming methods that explicitly encode graph structure. Our results suggest that purely-learning-based approaches without graph structure may be suitable for learning high-level, long-range relationships on graphs.

## Graph Structure Enhanced Exploration for Goal-conditioned Reinforcement Learning

### TL;DR

In this paper, we propose GSRL, a new goal-conditioned RL that leverages the state-transition graph for effective exploration and efficient training.

### Abstract

Goal-conditioned Reinforcement Learning (RL) is a promising approach for scaling up RL techniques on sparse reward environments requiring long horizon planning. Recent works attempt to build suitable abstraction graph of the environment and enhance RL with classical graphical methods such as shortest path searching; however, these approaches mainly focus on graph construction or agent exploitation but leave the exploration lack of study.  This paper introduces Graph Structured RL (GSRL), a new goal-conditioned RL algorithm for effective exploration and efficient training. GSRL mainly builds upon attention mechanism with hindsight supervision that leverages state-transition graph information for efficient goal9generation  and policy learning.  GSRL  learns to generate goals from a  state-transition graph of seen data such that it leads to an effective and informative11exploration strategy. In addition to our theoretical results regarding optimal goal1 generation,  our empirical results on standard discrete and continuous  control13benchmarks show that GSRL outperforms the state-of-the-art methods.

## Iterative Causal Discovery in the Possible Presence of Latent Confounders and Selection Bias

### TL;DR

We present a sound and complete algorithm for iteratively recovering causal graphs in the possible presence of latent confounders and selection bias.

### Abstract

We present a sound and complete algorithm, called iterative causal discovery (ICD), for recovering causal graphs in the possible presence of latent confounders and selection bias. ICD relies on the causal Markov and faithfulness assumptions and recovers the equivalence class of the underlying causal graph. It starts with a complete graph, and consists of a single iterative stage that gradually refines this graph by identifying conditional independence (CI) between connected nodes. Independence and causal relations entailed after any iteration are correct, rendering ICD anytime. Essentially, we tie the size of the CI conditioning set to its distance on the graph from the tested nodes, and increase this value in the successive iteration. Thus, each iteration refines a graph that was recovered by previous iterations having smaller conditioning sets---a higher statistical power---which contributes to stability. We demonstrate empirically that ICD requires significantly fewer CI tests and smaller conditioning sets compared to FCI, FCI+, and RFCI algorithms.

## TransCamP: Graph Transformer for 6-DoF Camera Pose Estimation

### TL;DR

This paper presents a camera relocalization network leveraging graph transformers.

### Abstract

Camera pose estimation or camera relocalization is the centerpiece in numerous computer vision tasks such as visual odometry, structure from motion (SfM) and SLAM. 
In this paper we propose a neural network approach with a graph transformer backbone, namely TransCamP, to address the camera relocalization problem.
In contrast with prior work where the pose regression is mainly guided by photometric consistency, TransCamP effectively fuses the image features, camera pose information and inter-frame relative camera motions into encoded graph attributes and is trained towards the graph consistency and accuracy instead, yielding significantly higher computational efficiency. 
By leveraging graph transformer layers with edge features and enabling tensorized adjacency matrix, TransCamP dynamically captures the global attention and thus endows the pose graph with evolving structures to achieve improved robustness and accuracy. 
In addition, optional temporal transformer layers actively enhance the spatiotemporal inter-frame relation for sequential inputs. 
Evaluation of the proposed network on various public benchmarks demonstrates that TransCamP outperforms state-of-the-art approaches.

## GraphPiece:  Efficiently Generating High-Quality Molecular Graph with Substructures

### TL;DR

None

### Abstract

Molecular graph generation is a fundamental but challenging task in various applications such as drug discovery and material science, which requires generating valid molecules with desired properties. Auto-regressive models, which usually construct graphs following sequential actions of adding nodes and edges at the atom-level, have made rapid progress in recent years. However, these atom-level models ignore high-frequency subgraphs that not only capture the regularities of atomic combination in molecules but also are often related to desired chemical properties. In this paper, we propose a method to automatically discover such common substructures, which we call graph pieces, from given molecular graphs. Based on graph pieces, we leverage a variational autoencoder to generate molecules in two phases: piece-level graph generation followed by bond completion. Experiments show that our graph piece variational autoencoder achieves better performance over state-of-the-art baselines on property optimization and constrained property optimization tasks with higher computational efficiency. 

## Controlling Epidemic Spread under Probabilistic Diffusion through Stochastic Networks

### TL;DR

None

### Abstract

The spread of an epidemic is often modeled by an SIR random process on a social network graph. The MinInf problem involves minimizing the expected number of infections, when the disease starts at a designated vertex and we are allowed to break at most $B$ edges  (or at most $B$ vertices, in the case of vaccination) of the graph. This type of intervention naturally corresponds to implementing social distancing, and as the COVID-19 pandemic has shown, it is critical in mitigating the infection spread. Although this problem is fundamental in epidemiology, it has remained generally open. In this paper, we study MinInf under the Chung-Lu random graph model, and develop a Sample Average Approximation (SAA) scheme for it. We further show that for certain parameters of the random graph model affecting the number of paths in a randomly drawn graph, our framework yields rigorous  bicriteria approximation algorithms. Finally, we complement the latter by providing cases demonstrating the limits of our SAA approach.

## A Unified Framework for Convolution-based Graph Neural Networks

### TL;DR

We propose a unified framework for convolution-based graph neural networks and provide a general methodology for evaluating and relating different graph learning modules.

### Abstract

Graph Convolutional Networks (GCNs) have attracted a lot of research interest in machine learning, and many variants have been proposed recently. In this paper, we take a step forward to establish a unified framework for convolution-based graph neural networks, aiming to provide a systematic view of different GCN variants and deep understanding of the relations among them. Our key idea is formulating the basic graph convolution operation as an optimization problem in the graph Fourier space. Under this framework, a variety of popular GCN models, including vanilla-GCNs, attention-based GCNs and topology-based GCNs, can be interpreted as a similar optimization problem but with different regularizers. This novel perspective enables a better understanding of the similarities and differences among many widely used GCNs, and may inspire new model designs. As a showcase, we present a novel regularization technique under the proposed framework to tackle the oversmoothing problem in graph convolution. The effectiveness of newly designed model is validated empirically.

## Generalization Bounds for Graph Embedding Using Negative Sampling: Linear vs Hyperbolic

### TL;DR

We derived generalization bound for graph embedding with negative sampling in inner-product space and hyperbolic space.

### Abstract

Graph embedding, which represents real-world entities in a mathematical space, has enabled numerous applications such as analyzing natural languages, social networks, biochemical networks, and knowledge bases.
It has been experimentally shown that graph embedding in hyperbolic space can represent hierarchical tree-like data more effectively than embedding in linear space, owing to hyperbolic space's exponential growth property. 
However, since the theoretical comparison has been limited to ideal noiseless settings, the potential for the hyperbolic space's property to worsen the generalization error for practical data has not been analyzed.
In this paper, we provide a generalization error bound applicable for graph embedding both in linear and hyperbolic spaces under various negative sampling settings that appear in graph embedding. 
Our bound states that error is polynomial and exponential with respect to the embedding space's radius in linear and hyperbolic spaces, respectively, which implies that hyperbolic space's exponential growth property worsens the error.
Using our bound, we clarify the data size condition on which graph embedding in hyperbolic space can represent a tree better than in Euclidean space by discussing the bias-variance trade-off.
Our bound also shows that imbalanced data distribution, which often appears in graph embedding, can worsen the error.


## Metric Learning on Temporal Graphs via Few-Shot Examples

### TL;DR

None

### Abstract

Graph metric learning methods aim to learn the distance metric over graphs such that similar graphs are closer and dissimilar graphs are farther apart. This is of critical importance in many graph classification applications such as drug discovery and epidemics categorization. In many real-world applications, the graphs are typically evolving over time; labeling graph data is usually expensive and also requires background knowledge. However, state-of-the-art graph metric learning techniques consider the input graph as static, and largely ignore the intrinsic dynamics of temporal graphs. Furthermore, most of these techniques require abundant labeled examples for training in the representation learning process. To address the two aforementioned problems, we aim to learn a distance metric over temporal graphs, which should not only help accurately categorize seen temporal graphs but also adapt fast to unseen temporal graphs. In this paper, we first propose the streaming-snapshot model to characterize temporal graphs in different time scales. Then we propose the MetaTag framework to learn the metric over a limited number of temporal graphs, and adapt the learned metric to unseen temporal graphs via a few examples. Finally, we demonstrate the performance of MetaTag in comparison with state-of-the-art baseline algorithms for temporal graph classification problems.

## Weisfeiler and Lehman Go Cellular: CW Networks

### TL;DR

We propose a message passing scheme on CW complexes, study its properties and expressive power and apply it to molecular graphs

### Abstract

Graph Neural Networks (GNNs) are limited in their expressive power, struggle with long-range interactions and lack a principled way to model higher-order structures. These problems can be attributed to the strong coupling between the computational graph and the input graph structure. The recently proposed Message Passing Simplicial Networks naturally decouple these elements by performing message passing on the clique complex of the graph. Nevertheless, these models are severely constrained by the rigid combinatorial structure of Simplicial Complexes (SCs). In this work, we extend recent theoretical results on SCs to regular Cell Complexes, topological objects that flexibly subsume SCs and graphs. We show that this generalisation provides a powerful set of graph ``lifting'' transformations, each leading to a unique hierarchical message passing procedure. The resulting methods, which we collectively call CW Networks (CWNs), are strictly more powerful than the WL test and, in certain cases, not less powerful than the 3-WL test. In particular, we demonstrate the effectiveness of one such scheme, based on rings, when applied to molecular graph problems. The proposed architecture benefits from provably larger expressivity than commonly used GNNs, principled modelling of higher-order signals and from compressing the distances between nodes. We demonstrate that our model achieves state-of-the-art results on a variety of molecular datasets.

## Edge Proposal Sets for Link Prediction

### TL;DR

We show that adding a "good" set of edges to a graph serves as an effective pre-processing step for link prediction.

### Abstract

Graphs are a common model for complex relational data such as social networks and protein interactions, and such data can evolve over time (e.g., new friendships) and be noisy (e.g., unmeasured interactions). Link prediction aims to predict future edges or infer missing edges in the graph, and has diverse applications in recommender systems, experimental design, and complex systems. Even though link prediction algorithms strongly depend on the set of edges in the graph, existing approaches typically do not modify the graph topology to improve performance. Here, we demonstrate how simply adding a set of edges, which we call a \emph{proposal set}, to the graph as a pre-processing step can improve the performance of many link prediction algorithms. The underlying idea is that if the edges in the proposal set generally align with the structure of the graph, link prediction algorithms are further guided towards predicting the right edges; in other words, adding a proposal set of edges is a signal-boosting pre-processing step. We show how to use existing link prediction algorithms to generate effective proposal sets and evaluate this approach on various synthetic and empirical datasets. We find that proposal sets consistently improve the accuracy of link prediction algorithms based on both neighborhood heuristics and graph neural networks.

## Relaxed Marginal Consistency for Differentially Private Query Answering

### TL;DR

We propose a post-processing technique that boosts utility by enforcing (local) consistency constraints.  Our method is scalable to far more general settings than prior work

### Abstract

Many differentially private algorithms for answering database queries involve a
step that reconstructs a discrete data distribution from noisy measurements. This
provides consistent query answers and reduces error, but often requires space that
grows exponentially with dimension. PRIVATE-PGM is a recent approach that uses
graphical models to represent the data distribution, with complexity proportional to
that of exact marginal inference in a graphical model with structure determined by
the co-occurrence of variables in the noisy measurements. PRIVATE-PGM is highly
scalable for sparse measurements, but may fail to run in high dimensions with dense
measurements. We overcome the main scalability limitation of PRIVATE-PGM
through a principled approach that relaxes consistency constraints in the estimation
objective. Our new approach works with many existing private query answering
algorithms and improves scalability or accuracy with no privacy cost.

## Context-Specific Causal Discovery for Categorical Data Using Staged Trees

### TL;DR

None

### Abstract

Causal discovery algorithms aims at untangling complex causal relationships using observational data only. Here, we introduce new causal discovery algorithms based on staged tree models, which can represent complex and non-symmetric causal effects. To demonstrate the efficacy of our algorithms, we introduce a new distance, inspired by the widely used structural interventional distance, to quantify the closeness between two staged trees in terms of their corresponding causal inference statements. A simulation study highlights the efficacy of staged trees in uncovering complex, asymmetric causal relationship from data and a real-world data application illustrates their use  in a practical causal analysis.

## Deep Causal Reasoning for Recommender Systems

### TL;DR

None

### Abstract

Recommender systems aim to estimate a user's rating to an item based on observed ratings from the population. As with all observational studies, hidden confounders, which affect both the item's exposure and the user's ratings, lead to a systematic bias in the estimation. Consequently, a new trend in recommender system research is to negate the influence of confounders from a causal reasoning perspective. Observing that confounders in recommendations are mainly attributes shared among items (and are therefore multi-cause confounders), we model the recommendation as a multi-cause multi-outcome (MCMO) inference problem. Specifically, to remedy the bias, we use the exposure data to estimate latent user factors that render the item exposures independent Bernoulli trials, where the generative distribution is parameterized by a deep neural network with factorized logistic likelihood and the intractable posteriors are estimated by variational inference. Controlling these factors as substitute confounders, under mild assumptions, is proved to be able to eliminate the bias. Furthermore, we argue that the direct utilization of the above model for rating predictions leads to high variance due to the scarce observations associated with the high-dimensional causal vectors, where we theoretically demonstrate that introducing user features as pre-treatment variables can substantially improve sample efficiency and alleviate overfitting. Empirical studies on semi-simulated and real-world datasets show that the proposed method is more robust to unobserved confounders and consistently improves recommendation quality. Codes are released at https://www.github.com/caslrec/deep_deconf.

## Mitigating Cold Start in E-Commerce by Predicting Unbiased Matching of Rare Products and Queries

### TL;DR

None

### Abstract

In this work, we propose a technique to mitigate cold start for product search,
i.e., the inability of a ranker to draw inference for products and queries with
which customer have not sufficiently interacted. The proposed method involves two
models: A probabilistic debiasing graphical model and a deep neural network.
The debiasing model estimates the unbiased matching quality of
observed query-products and scores a dataset used to train the neural network.
The neural network predicts the matching quality of rare queries to rare products by transferring 
what learned on known queries and products.
We demonstrate the efficacy of our combined models against cold start
on a test set of 5M query-products collected from a popular e-commerce website.

## Beware of the Simulated DAG! Causal Discovery Benchmarks May Be Easy to Game

### TL;DR

Many causal discovery benchmarks can easily be gamed by exploiting patterns in the way DAGs are simulated.

### Abstract

Simulated DAGs may exhibit properties that inadvertently render their structure identifiable and unexpectedly affect structure learning algorithms. Here, we show that marginal variance tends to increase along the causal order for generically sampled additive noise models. We introduce \emph{varsortability} as a measure of the agreement between the order of increasing marginal variance and the causal order. For common sampling schemes, we show that the remarkable performance of some continuous structure learning algorithms can be explained by high varsortability and matched by a simple baseline method. Yet, this performance may not transfer to real-world data where varsortability may be absent or dependent on the choice of measurement scales. On standardized data, the same algorithms no longer identify the ground-truth DAG or its Markov equivalence class. While standardization removes the pattern in marginal variance, we show that data generating processes that incur high varsortability also leave a distinct covariance pattern that may be exploited even after standardization. Our findings challenge the significance of generic benchmarks with independently drawn parameters.

## Molecular Graph Generation by Decomposition and Reassembling

### TL;DR

A new molecular generation method based on graph decomposition by graph mining and reassembling by Monte Carlo tree search.

### Abstract

Designing molecular structures with desired chemical properties is an essential task in drug discovery and material design. However, finding molecules with the optimized desired properties is still a challenging task due to combinatorial explosion of candidate space of molecules. Here we propose a novel decomposition-and-reassembling based approach, which does not include any optimization in hidden space and our generation process is highly interpretable. Our method is a two-step procedure: In the first decomposition step, we apply frequent subgraph mining to a molecular database to collect smaller size of subgraphs as building blocks of molecules. In the second reassembling step, we search desirable building blocks via Monte Carlo tree search (MCTS) and combine them to generate new molecules. Our experiments show that our method can find competitive or better molecules in terms of two standard criteria, the penalized log P and drug-likeness, compared to the state-of-the-art molecular graph generation methods.


## Continuous–Depth Neural Models for Dynamic Graph Prediction

### TL;DR

We introduce Neural Graph Differential Equations (Neural GDEs), continuous-depth counterparts to to GNNs. Neural GDEs are evaluated in dynamic graph settings, including traffic forecasting and prediction of stochastic genetic regulatory networks.

### Abstract

We introduce the framework of continuous-depth graph neural networks (GNNs). Neural graph differential equations (Neural GDEs) are formalized as the counterpart to GNNs where the input-output relationship is determined by a continuum of GNN layers, blending discrete topological structures and differential equations. The proposed framework is shown to be compatible with static GNN models and is extended to dynamic and stochastic settings through hybrid dynamical system theory. Here, Neural GDEs improve performance by exploiting of the underlying dynamics geometry, further introducing the ability to accommodate irregularly sampled data. Results prove the effectiveness of the proposed models across applications, such as traffic forecasting or prediction in genetic regulatory networks.































## Probabilistic Entity Representation Model for Chain  Reasoning over Knowledge Graphs

### TL;DR

We propose probabilistic embedding model for entity representation in knowledge graphs to improve reasoning over first-order logical queries. 

### Abstract

Logical reasoning is a fundamental challenge in knowledge graphs that limits its application on large and incomplete databases. Current approaches employ spatial geometries such as boxes to learn query representations that encompass the answer entities and model the logical operations of projection and intersection.   However, their geometry is restrictive and leads to multiple issues.Furthermore, previous works propose transformation tricks to handle union which results in non-closure and, thus, cannot be chained in a stream. In this paper, we propose Probabilistic Entity Representation Model (PERM) to encode entities as a Multivariate Gaussian density with parameters mean and covariance to capture its semantic position and continuous decision boundary, respectively. Additionally, we also define the closed10logical operations of projection, intersection ($\cap$) and union ($\cup$) that can be chained in an end-to-end objective function. We demonstrate the performance of PERM on the logical query reasoning problem and demonstrate that it outperforms the state-of-the-art methods on four standard KG datasets by 6.2% in HITS@3 and 12.6% in MRR. Furthermore, we evaluate PERM’s competence in the COVID-19 drug-repurposing problem and show that we are able to recommend drugs with 8.2% better F1 than current methods. Finally, we demonstrate the comprehension of PERM’s query answering process by observing a low-dimensional visualization of the Gaussian embeddings through the complex query process.

## SPARE: A Single-Pass Neural Model for Multi-Relational Databases

### TL;DR

We describe an architecture for relational databases offering significant speedups without compromises on accuracy.

### Abstract

While there has been extensive work on deep neural networks for images and text, deep learning for multi-relational databases (RDBs) is still a rather unexplored field. One direction that recently gained traction is to apply Graph Neural Networks (GNNs) to RBDs. However, training GNNs on large multi-relational databases (i.e., data stored in multiple database tables) is rather inefficient due to multiple rounds of training and potentially large and inefficient representations.
Hence, in this paper we propose Spare (Single-Pass Multi-Relational models), a new class of neural models that can be trained efficiently on RDBs. To this end, Spare makes use of the fact that data in RDBs has a regular structure, which allows one to train these models in a single pass while exploiting symmetries and, thus, avoiding redundant sub-graphs that typically arise when using GNNs. Our extensive empirical evaluation demonstrates that Spare can significantly speedup both training and inference while offering competitive predictive performance over numerous baselines.

## Storchastic: A Framework for General Stochastic Automatic Differentiation

### TL;DR

We present a framework for gradient estimation in stochastic computation graphs that incorporates many estimators and extends to any-order differentiation.

### Abstract

Modelers use automatic differentiation (AD) of computation graphs to implement complex Deep Learning models without defining gradient computations. Stochastic AD extends AD to stochastic computation graphs with sampling steps, which arise when modelers handle the intractable expectations common in Reinforcement Learning and Variational Inference. However, current methods for stochastic AD are limited: They are either only applicable to continuous random variables and differentiable functions, or can only use simple but high variance score-function estimators. To overcome these limitations, we introduce Storchastic, a new framework for AD of stochastic computation graphs. Storchastic allows the modeler to choose from a wide variety of gradient estimation methods at each sampling step, to optimally reduce the variance of the gradient estimates. Furthermore, Storchastic is provably unbiased for estimation of any-order gradients, and generalizes variance reduction techniques to higher-order gradient estimates. Finally, we implement Storchastic as a PyTorch library.

## Multiscale Graph Comparison via the Embedded Laplacian Distance

### TL;DR

We propose a simple and fast method to quantify the structural similarity between graphs of vastly different sizes. 

### Abstract

We introduce a simple and fast method for comparing graphs of different sizes. Existing approaches are often either limited to comparing graphs with the same number of vertices or are computationally unscalable. We propose the Embedded Laplacian Distance (ELD) for comparing graphs of potentially vastly different sizes. Our approach first projects the graphs onto a common, low-dimensional Laplacian embedding space that respects graphical structure. This reduces the problem to that of comparing point clouds in a Euclidean space. A distance can then be computed efficiently via a natural sliced Wasserstein approach. We show that the ELD is a pseudo-metric and is invariant under graph isomorphism. We provide intuitive interpretations of the ELD using tools from spectral graph theory. We test the efficacy of the ELD approach extensively on both simulated and real data. Results obtained are very promising.

## Graph Optimal Transport with Transition Couplings of Random Walks

### TL;DR

We extend optimal transport techniques for Markov chains to define a new approach to optimal transport for graphs.

### Abstract

We present a novel approach to optimal transport between graphs from the perspective of stationary Markov chains. A weighted graph may be associated with a stationary Markov chain by means of a random walk on the vertex set with transition distributions depending on the edge weights of the graph. After drawing this connection, we describe how optimal transport techniques for stationary Markov chains may be used in order to perform comparison and alignment of the graphs under study. In particular, we propose the graph optimal transition coupling problem, referred to as GraphOTC, in which the Markov chains associated to two given graphs are optimally synchronized to minimize an expected cost. The joint synchronized chain yields an alignment of the vertices and edges in the two graphs, and the expected cost of the synchronized chain acts as a measure of distance or dissimilarity between the two graphs. We demonstrate that GraphOTC performs equal to or better than existing state-of-the-art techniques in graph optimal transport for several tasks and datasets. Finally, we also describe a generalization of the GraphOTC problem, called the FusedOTC problem, from which we recover the GraphOTC and OT costs as special cases.

## Adaptive Graph Neural Networks via Fisher Regularization-Guided Filter Integration

### TL;DR

Proposed a unified GNN framework (AGNN) that goes beyond the message passing framework and does not rely on any underlying data assumptions.

### Abstract

Graph Neural Networks (GNNs) have received tremendous attention due to their power in handling graph data for different downstream tasks across different application domains. However, some recent work raised concerns about GNNs' limitations. First, most of the existing GNNs have a data-dependency assumption, which assumes that node feature and node label are dependent, and graph structure and node label are also dependent. Therefore, for graph data that does not satisfy these dependencies, GNNs may fail to optimally leverage node features and graph topological structures during embedding learning. Second, most of the existing GNNs have the underlying homophily assumption, which states that similar nodes are having closer connections. Then for heterophilic graphs, GNNs may suffer performance degradation. In this paper, we tackle the aforementioned challenges by proposing a unified GNN framework called Adaptive Graph Neural Networks (AGNN), which go beyond the classic message passing framework and can handle all types of graphs properly. We provide discussions to connect existing GNN models with our AGNN framework, and provide a simple but effective instantiation of AGNN. We also propose to use Fisher Score, a widely used evaluation metric in Linear Discriminant Analysis that can quantify the separability of two sets of features, as a regularization to guide the training and boost the quality of the learned embeddings. Experiments have demonstrated that our proposed model has the flexibility in learning a model that can be adaptive to any given graph data and can consistently provide competitive performance across all the datasets.



## More Powerful Graph Neural Networks with Nesting

### TL;DR

None

### Abstract

Graph neural network (GNN)'s success in graph classification is closely related to the Weisfeiler-Lehman (1-WL) algorithm. By iteratively aggregating neighboring node features to a center node, both 1-WL and GNN obtain a node representation that encodes a rooted subtree around the center node. These rooted subtree representations are then pooled into a single representation to represent the whole graph. However, rooted subtrees are of limited expressiveness to represent a non-tree graph. To address it, we propose Nested Graph Neural Networks (NGNNs). NGNN represents a graph with rooted subgraphs instead of rooted subtrees, so that two graphs sharing many identical subgraphs (rather than subtrees) tend to have similar representations. The key is to make each node representation encode a subgraph around it more than a subtree. To achieve this, NGNN extracts a local subgraph around each node and applies a base GNN to each subgraph to learn a subgraph representation. The whole-graph representation is then obtained by pooling these subgraph representations. We provide a rigorous theoretical analysis showing that NGNN is strictly more powerful than 1-WL. In particular, we proved that NGNN can discriminate almost all $r$-regular graphs, where 1-WL always fails. Moreover, unlike other more powerful GNNs, NGNN only introduces a constant factor in time complexity compared to standard GNNs. NGNN is a plug-and-play framework that can be combined with various base GNNs. We test NGNN with different base GNNs on several benchmark datasets. NGNN uniformly improves their performance and shows highly competitive performance on all datasets.

## Permutation-sensitive Neural Networks Express More on Graph

### TL;DR

This paper designs a new permutation-sensitive aggregation function via permutation group to develop more powerful graph neural networks, provding better expressivity than the 2-dimensional Weisfeiler-Lehman (2-WL) graph isomorphism test.

### Abstract

Global invariance is an overarching requirement for graph neural networks (GNNs), especially on graph-level tasks. Traditionally this has been achieved by invariant operations over node permutations when aggregating messages, which, however, limits the expressivity of GNNs. In this study, we consider designing a permutation-sensitive aggregation function via permutation group, capturing implicit patterns among neighboring nodes and meanwhile ensuring linear computational accessibility. We prove that our method goes beyond the 2-dimensional Weisfeiler-Lehman (2-WL) graph isomorphism test due to its ability of counting incidence triangles, which helps to distinguish non-isomorphic regular graphs unable to be handled by MPNNs and 2-WL test. Extensive experiments on multiple real-world datasets for graph-level tasks have demonstrated the superiority of our models.

## Robust Contrastive Graph Representation Learning

### TL;DR

None

### Abstract

Despite the widespread success of unsupervised graph representation learning via contrastive learning algorithms, optimizing contrastive graph representation learning by theoretical understanding is still missing. This paper investigates unsupervised graph representation learning by (1) maximizing the mutual information between the semantic information and the structural information of the data; (2) reducing the influence of false positive samples and false negative samples; and (3) transferring mutual information maximization to the minimization of a triplet loss plus an upper bound loss rather than conventional contrastive learning. As a result, our method can output robust low-dimensional representations to conduct downstream tasks. Experimental results  on eight real datasets demonstrate that our proposed method is superior over several state-of-the-art methods of graph representation learning in terms of node classification.

## High-Dimensional Sparse Graph Learning Under Laplacian-Related Constraints

### TL;DR

We learn a sparse graph under graph Laplacian-related constraints, given high-dimensional  data.

### Abstract

We consider the problem of learning a sparse undirected graph underlying a given set of multivariate data. We focus on graph Laplacian-related constraints on the sparse precision matrix that encodes conditional dependence between the random variables associated with the graph nodes. Under these constraints the off-diagonal elements of the precision matrix are non-positive (total positivity), and the precision matrix may not be full-rank. We investigate  modifications to widely used penalized log-likelihood approaches to enforce total positivity but not the Laplacian structure. The graph Laplacian can then be extracted from the off-diagonal precision matrix. An alternating direction method of multipliers (ADMM) algorithm is presented and analyzed for constrained optimization under Laplacian-related constraints and lasso as well as adaptive lasso penalties. Numerical results based on synthetic data show that the proposed constrained adaptive lasso approach significantly outperforms existing Laplacian-based approaches. We also evaluate our approach on real financial data.

## Sketch-Based Streaming Anomaly Detection in Dynamic Graphs

### TL;DR

We extend count-min sketch to higher-order preserving the dense subgraph structure, and propose four online algorithms. This is the first streaming approach that incorporates dense subgraph search to detect graph anomalies in constant memory/time.

### Abstract

Given a stream of graph edges from a dynamic graph, how can we assign anomaly scores to edges and subgraphs in an online manner, for the purpose of detecting unusual behavior, using constant time and memory? For example, in intrusion detection, existing work seeks to detect either anomalous edges or anomalous subgraphs, but not both. In this paper, we first extend the count-min sketch data structure to a higher-order sketch. This higher-order sketch has the useful property of preserving the dense subgraph structure (dense subgraphs in the input turn into dense submatrices in the data structure). We then propose four online algorithms that utilize this enhanced data structure, which (a) detect both edge and graph anomalies; (b) process each edge and graph in constant memory and constant update time per newly arriving edge, and; (c) outperform state-of-the-art baselines on four real-world datasets. Our method is the first streaming approach that incorporates dense subgraph search to detect graph anomalies in constant memory and time.

## Node Feature Kernels Increase Graph Convolutional Network Robustness

### TL;DR

None

### Abstract

The robustness of the much used Graph Convolutional Networks (GCNs) to perturbations of their input is becoming a topic of increasing importance. In this paper the \textit{random} GCN is introduced for which a random matrix theory analysis is possible. This analysis suggests that if the graph is sufficiently perturbed, or in the extreme case random, then the GCN fails to benefit from the node features. It is furthermore observed that enhancing the message passing step in GCNs by adding the node feature kernel to the adjacency matrix of the graph structure solves this problem. An empirical study of a GCN utilised for node classification on six real datasets further confirms the theoretical findings and demonstrates that perturbations of the graph structure can result in GCNs performing significantly worse than Multi-Layer Perceptrons run on the node features alone. In practice, adding a node feature kernel to the message passing of perturbed graphs results in a significant improvement of the GCN's performance, thereby rendering it more robust to graph perturbations. 

## Efficient Non-parametric DAG StructureLearning

### TL;DR

A Efficient method to perform DAG learning for nonparameteric Structural Equation Model.

### Abstract

Directed acyclic graph (DAG) structure learning can be formulated as a constrained optimization with a continuous acyclicity constraint, which is often solved iteratively through sub-problem optimization. A recent breakthrough shows that the set of DAGs are equivalent to the set of weighted gradients of graph potential functions, and hence one may perform DAG learning by searching in the equivalent space of DAGs. However, the previous work, called DAG-NoCurl, is limited to linear structural equation models (SEM) with explicit weighted adjacency matrix defined. We theoretically derive a nonparametric projection formulation and propose an efficient two-steps nonparametric DAG learning method, which we coined DAG-NCMLP, to estimate DAG from observational data. First, we learn a non-acyclic graph through few iterations of soft constrained optimization. Then, we project the non-acyclic graph to the equivalent space of DAGs and obtain an acyclic graph. Experimental studies on benchmark datasets demonstrate that our proposed method provides comparable accuracy, if not better, compared to state-of-the-art nonparametric DAG learning methods with hard constrained optimization, while substantially reduces the computational time. 

## Hypergraph Embedding via Spectral Connection: Cut, Weighted Kernel $k$-means, and Heat Kernel

### TL;DR

We propose a theoretical founded hypergraph embedding method.

### Abstract

We propose a theoretical framework of multi-way similarity to embed real-valued data into hypergraphs for spectral clustering.
For graph cut based spectral clustering, it is common to embed real-valued data into graph by modeling pairwise similarities using kernel function.
This is because the kernel function has a theoretical connection to the graph cut.
For problems where using multi-way similarities are more suitable than pairwise ones, it is natural to embed into a hypergraph, which is generalization of a graph.
However, although the hypergraph cut is well-studied, there is not yet established a hypergraph cut based framework to model multi-way similarity.
In this paper, we formulate multi-way similarities by exploiting the theoretical foundation of kernel function.
We show a theoretical connection between our formulation and hypergraph cut in two ways, generalizing both weighted kernel $k$-means and the heat kernel, by which we justify our formulation.
We also provide a fast algorithm for spectral clustering.
Our algorithm empirically shows better performance than existing graph and other heuristic modeling methods.

## Spectral Graph Networks with Constrained Polynomials

### TL;DR

We revisit the basic formulation of spectral GNNs and propose a model which scales and performs better than existing spectral nets.

### Abstract

Spectral graph neural networks (GNNs) marry ideas from spectral graph theory and signal processing to extend convolutional nets to graph domains. Despite their principled formulation, spectral GNNs have faded away from the spotlight with the surge of high-performing GNNs designed from a message-passing perspective.  This work revisits spectral GNNs and builds on their basic formulation to introduce Polynomial Subspace Net (PSN) --- a simple yet effective spectral graph network. PSN convolutions exploit multi-hop graph neighborhoods while controlling model complexity by sharing parameters (filter coefficients) across feature channels. We combine this idea with other simple design choices to further enhance the performance of PSNs. First, we restrict the coefficients to a closed interval to obtain numerical stability.  Second, we derive inner-layer residual connections from a spectral perspective. Despite its simplicity, PSN outperforms spectral GNNs and rivals more expressive GNNs on relevant benchmarks. We show that PSN scales asymptotically better than other popular spectral nets. We also demonstrate that our design alleviates oversmoothing and enables leveraging deeper architectures.

## Learning to solve Minimum Cost Multicuts efficiently using Graph Convolutional Neural Networks

### TL;DR

None

### Abstract

Graph partitioning can be expressed by the minimum cost multicut problem (MP). This combinatorial optimization problem is usually defined by a constrained integer linear program. The MP is known to be NP-hard as well as APX-hard, making it difficult to solve in practice. Therefore, the need for efficient, learnable MP solvers is emergent. Graph convolutional neural networks have recently proven to be promising in classical combinatorial problems. In this context, the MP is particularly challenging to address because (i) its cost function is solely defined by real-valued edge weights and (ii) the sole enumeration of constraints is often infeasible in practice. In this paper we extend various GNN architectures including Graph Convolutional Networks, Signed Graph Convolutional Networks and Graph Isomorphic Networks to facilitate the efficient encoding of real-valued edge costs. We further provide a loss function that promotes feasible MP solutions in a scalable way. Our findings support that GNN approaches can produce good solutions in practice while providing lower computation times and largely improved scalability compared to LP solvers and optimized heuristics, especially when considering large instances.

## Path-Aware Graph Attention for HD Maps in Motion Prediction

### TL;DR

We propose a novel attention mechanism for heterogeneous graphs based on paths which achieves state of the art at the motion prediction task.

### Abstract

The success of motion prediction for autonomous driving relies on integration
of information from the HD maps. As maps are naturally graph-structured,
investigation on graph neural networks (GNNs) for encoding HD maps is
burgeoning in recent years. However, unlike many other applications where
GNNs have been straightforwardly deployed, HD maps are heterogeneous
graphs where vertices (lanes) are connected by edges (lane-lane interaction
relationships) of various nature, and most graph-based models are not
designed to understand the variety of edge types which provide crucial cues
for predicting how the agents would travel the lanes. To overcome this challenge,
we propose Path-Aware Graph Attention, a novel attention architecture that 
infers the attention between two vertices by parsing the sequence of edges
forming the paths that connect them. Our analysis illustrates how the proposed
attention mechanism can facilitate learning in a didactic problem where existing
graph networks like GCN struggle. We also demonstrate that the proposed
model improves HD map encoding and surpasses previous state of the art on
the Argoverse Motion Forecasting dataset.

## Beyond GNNs: A Sample-Efficient Architecture for Graph Problems

### TL;DR

We propose a provably efficient architecture for solving graph problems

### Abstract

Despite their popularity for graph structured data, existing Graph Neural Networks (GNNs) have inherent limitations for fundamental graph problems such as shortest paths, $k$-connectivity, minimum spanning tree and minimum cuts. In these instances, it is known that one needs GNNs of high depth, scaling at a polynomial rate with the number of nodes $n$, to provably encode the solution space, in turn affecting their statistical efficiency.

In this work we propose a new hybrid architecture to overcome this limitation. Our proposed architecture that we call as GNN+ networks involve a combination of multiple parallel low depth GNNs along with simple pooling layers involving low depth fully connected networks. We provably demonstrate that for many graph problems, the solution space can be encoded by GNN+ networks using depth that scales only poly-logarithmically in the number of nodes. This also has statistical advantages that we demonstrate via generalization bounds for GNN+ networks.

We empirically show the effectiveness of our proposed architecture for a variety of graph problems and real world classification problems.





## Regret-efficient graph-structured bandits

### TL;DR

Optimal strategy for a plurality of structured bandit problems including unimodal, Lipschitz and linear bandits

### Abstract

We consider a structured variant of the multi-armed bandit problem when  the difference of means between any pairs of arms is constrained not to exceed some value. This graph-structure is flexible enough to encompass as special cases classical structures such as unimodal, Lipschitz, or linear assumptions, plus other models.
We derive the asymptotic lower bound on the cumulative regret for this structure, and introduce an extension of the popular Indexed Minimum Empirical Divergence (IMED) strategy from Honda and Takemura (Non-asymptotic analysis of a new bandit algorithm for semi-bounded rewards, 2015) to such structured configurations.
In order to show asymptotic optimality in a graph-structured scenario, we further add a tracking step to the IMED approach whose aim is to ensure the sub-optimal arms are played with correct asymptotic frequencies. Interestingly, this tracking step that requires solving an optimization problem is only triggered when the IMED index suggests exploration, which provably happens no more than $O(\log(T))$ times within $T$ rounds. Moreover, we carefully handle the rounds when the structure cannot be exploited due to large uncertainty (e.g. initial rounds), by combining structured and unstructured versions of IMED. Our analysis yields an explicit finite-time regret bound emphasizing the role of the second-order terms. Along the way, we provide novel concentration inequalities related to stochastic orderings that are of independent interest. Last, we illustrate the benefit of IMED-GS over alternative structured bandit strategies on numerical experiments. 


## Adaptive Graph Capsule Convolutional Networks

### TL;DR

None

### Abstract

In recent years, many studies utilize Convolutional Neural Networks (CNNs) to deal with non-grid graph data, known as Graph Convolutional Networks (GCNs). However, there exist two main limitations of the prevalent GCNs. First, GCNs have a latent information loss problem since they use scalar-valued neurons rather than vector-valued ones to iterate through graph convolutions. Second, GCNs are presented statically with fixed architectures during training, which would limit their representation power. To tackle these two issues, based on a GNN model (CapsGNN) which encodes node embeddings as vectors, we propose Adaptive Graph Capsule Convolutional Networks (AdaGCCN) to adaptively adjust the model architecture at runtime. Specifically, we leverage Reinforcement Learning (RL) to design an assist module for continuously selecting the optimal modification to the model structure through the whole training process. To mitigate the computation overhead brought by the assist module, we then deploy multiple workers to compute in parallel on GPU. Evaluations show that AdaGCCN obtains better classification accuracy than CapsGNN and other SOTA methods almost on all datasets in both bioinformatics and social fields. We also conduct experiments to indicate the efficiency of the paralleling strategy.

## Graph Adversarial Self-Supervised Learning

### TL;DR

None

### Abstract

This paper studies a long-standing problem of learning the representations of a whole graph without human supervision. The recent self-supervised learning methods train models to be invariant to the transformations (views) of the inputs. However, designing these views requires the experience of human experts. Inspired by adversarial training, we propose an adversarial self-supervised learning (\texttt{GASSL}) framework for learning unsupervised representations of graph data without any handcrafted views. \texttt{GASSL} automatically generates challenging views by adding perturbations to the input and are adversarially trained with respect to the encoder. Our method optimizes the min-max problem and utilizes a gradient accumulation strategy to accelerate the training process. Experimental on ten graph classification datasets show that the proposed approach is superior to state-of-the-art self-supervised learning baselines, which are competitive with supervised models.

## Text as Heterogenous Graph Data

### TL;DR

None

### Abstract

The advances of self-attention mechanisms in recent years have outperformed traditional neural network architectures in almost every field in natural language processing. However, a self-attention module is mathematically and conceptually equivalent to a Graph Attentional Network (GAT) which treats all tokens from a sentence as nodes and connects every pair of these token nodes. In this article, we want to further explore this line and formulate natural languages completely by the data structure of graphs. In particular, we propose a unified iterated-refinement framework that combines and transfers several NLP tasks into one heterogeneous graph. At the starting point of each refinement round, the model receives a graph from the previous iteration round and receives a signal via gradient descent to improve upon it. At the output stage of each iteration, we formulate all these NLP tasks as link prediction. Experiments demonstrate promising results of this method.

## Geometry Interaction Knowledge Graph Embeddings

### TL;DR

we propose the Geometry Interaction Knowledge Graph Embedding to capture a more reliable internal structure by interactively learning informative spatial structures in an adaptive way.

### Abstract

Knowledge graph (KG) embeddings have shown great power in learning representations of entities and relations for the link prediction task. The related work embeds KGs into either Euclidean or hyperbolic spaces to adapt to the needs of knowledge graphs embedding with specific geometric structures. However, with multiple relations, the structure embraced in a KG is usually not single and exhibits intrinsic heterogeneous features. They usually contain varied types of geometric structures, such as hierarchy, chain and ring typed structures. Embedding KGs in the single Euclidean or hyperbolic space, cannot capture their complex structures of KGs accurately and adequately, since it will ignore the intrinsic heterogeneous features. To address this challenge and embrace a richer set of relational information, we propose a novel method called Geometry Interaction Knowledge Graph Embeddings (GIE). Our GIE method can capture a more reliable internal structure by interactively learning informative spatial structures in an adaptive way. Finally, experimental results demonstrate that our method achieves state-of-the-art performance with fewer parameters on three well-established knowledge graphcompletion benchmarks.

## Decompositional Quantum Graph Neural Network

### TL;DR

None

### Abstract

Quantum machine learning is a fast emerging field that aims to tackle machine learning using quantum algorithms and quantum computing. Due to the lack of physical qubits and an effective means to map real-world data from Euclidean space to Hilbert space, most of these methods focus on quantum analogies or process simulations rather than devising concrete architectures based on qubits. In this paper, we propose a novel hybrid quantum-classical algorithm for graph-structured data, which we refer to as Decompositional Quantum Graph Neural Network (DQGNN). DQGNN replaces the Euclidean weight matrix of the classical GNN with quantum mechanical unitary weight matrices. When controlled by a classical computer, DQGNN can accommodate arbitrarily sized graphs by  processing substructures from  the input graph using a moderately-sized quantum device. The architecture is based on a novel mapping from real-world data to a Hilbert space. This mapping maintains the distance relations present in the data and reduces information loss. Experimental results show that the proposed method outperforms competitive state-of-the-art models and uses significantly fewer model parameters.

## Graph Alignment with Noisy Supervision

### TL;DR

We propose a robust graph alignment model via non-sampling and curriculum learning.

### Abstract

Despite achieving remarkable performance, prevailing graph alignment models still suffer from noisy supervision, yet how to mitigate the impact of noise in labeled data is still under-explored. The negative sampling based discrimination model has been a feasible solution to detect the noisy data and filter them out. However, due to its sensitivity to the sampling distribution, the negative sampling based discriminator would lead to an inaccurate decision boundary. Furthermore, it is difficult to find an abiding threshold to separate the potential positive (benign) and negative (noisy) data in the whole training process. To address these important issues, in this paper, we design a non-sampling discrimination model resorting to unbiased risk estimation of positive-unlabeled learning to circumvent the impact of negative sampling. We also propose to select the appropriate potential positive data at different training stages by an adaptive filtration threshold enabled by curriculum learning, for maximally improving the performance of the alignment model and discrimination model. Extensive experiments conducted on several real-world datasets validate the effectiveness of our proposed method.


## Semi-Riemannian Graph Convolutional Networks

### TL;DR

A novel graph convolution network in semi-Riemannian manifold of constant nonzero curvature. 

### Abstract

Graph Convolutional Networks (GCNs) are typically studied through the lens of Euclidean geometry. Non-Euclidean Riemannian manifolds provide specific inductive biases for embedding hierarchical or spherical data, but cannot align well with data of mixed topologies. We consider a larger class of semi-Riemannian manifolds with indefinite metric that generalize hyperboloid and sphere as well as their submanifolds. We develop new geodesic tools that allow for extending neural network operations into geodesically disconnected semi-Riemannian manifolds. As a consequence, we derive a principled Semi-Riemannian GCN that first models data in semi-Riemannian manifolds of constant nonzero curvature in the context of graph neural networks. Our method provides a geometric inductive bias that is sufficiently flexible to model mixed heterogeneous topologies like hierarchical graphs with cycles. Empirical results demonstrate that our method outperforms Riemannian counterparts when embedding graphs of complex topologies. 

## Spatiotemporal Graph Generative Representation Learning

### TL;DR

None

### Abstract

Spatiotemporal graphs represent crucial data structures where the nodes and edges are embedded in a geometric space and their attribute values can evolve dynamically over time. Nowadays, spatiotemporal graph data is becoming increasingly popular and important, ranging from microscale (e.g. protein folding), to middle-scale (e.g. dynamic functional connectivity), to macro-scale (e.g. human mobility network). Although disentangling and understanding the correlations among spatial, temporal, and network aspects have been a long-standing key topic in network science, they typically rely on network processes hypothesized by human knowledge. They usually fit well towards the properties that the predefined principles are tailored for, but usually cannot do well for the others, especially for many key domains where the human has yet very limited knowledge such as protein folding and biological neuronal networks. In this paper, we aim at pushing forward the modeling and understanding of spatiotemporal graphs via new disentangled deep generative models. Specifically, a new Bayesian model is proposed that factorizes spatiotemporal graphs into spatial, temporal, and graph factors as well as the factors that explain the interplay among them. A variational objective function and new mutual information thresholding algorithms driven by information bottleneck theory have been proposed to maximize the disentanglement among the factors with theoretical guarantees. Qualitative and quantitative experiments on both synthetic and real-world datasets demonstrate the superiority of the proposed model over the state-of-the-arts by up to 69.2\% for graph generation and 41.5\% for interpretability.

## Deoscillated Adaptive Graph Collaborative Filtering

### TL;DR

The paper solves the oscillation problem, varying locality and fixed propagation pattern problems to improve current GNN models in recommender systems.

### Abstract

Collaborative Filtering~(CF) signals are crucial for a Recommender System~(RS) model to learn user and item embeddings. High-order information can alleviate the cold-start issue of CF-based methods, which is modeled through propagating the information over the user-item bipartite graph. Recent Graph Neural Networks~(GNNs) propose to stack multiple aggregation layers to propagate high-order signals. 
However, there are three challenges, the oscillation problem, varying locality of bipartite graphs, and the fixed propagation pattern, which spoil the ability of the multi-layer structure to propagate information.
In this paper, we theoretically prove the existence and boundary of the oscillation problem, and empirically study the varying locality and layer-fixed propagation problems. We propose a new RS model, named as \textbf{D}eoscillated adaptive \textbf{G}raph \textbf{C}ollaborative \textbf{F}iltering~(DGCF), which is constituted by stacking multiple CHP layers and LA layers.
We conduct extensive experiments on real-world datasets to verify the effectiveness of DGCF. Detailed analyses indicate that DGCF solves oscillation problems, adaptively learns local factors, and has layer-wise propagation patterns. 

## Symmetry-driven graph neural networks

### TL;DR

We build new graph neural networks that are equivariant to several types of transformations affecting the node coordinates

### Abstract

Exploiting symmetries and invariance in data is a powerful, yet not fully exploited, way to achieve better generalisation with more efficiency. In this paper, we introduce two graph network architectures that are equivariant to several types of transformations affecting the node coordinates. First, we build equivariance to any transformation in the coordinate embeddings that preserves the distance between neighbouring nodes, allowing for equivariance to the Euclidean group. Then, we introduce angle attributes to build equivariance to any angle preserving transformation - thus, to the conformal group. Thanks to their equivariance properties, the proposed models can be vastly more data efficient with respect to classical graph architectures, intrinsically equipped with a better inductive bias and better at generalising. We demonstrate these capabilities on a synthetic dataset composed of $n$-dimensional geometric objects. Additionally, we provide examples of their limitations when (the right) symmetries are not present in the data.

## Understanding Bandits with Graph Feedback

### TL;DR

The improved upper and lower bounds for graph feedback bandits.

### Abstract

The bandit problem with graph feedback, proposed in [Mannor and Shamir, NeurIPS 2011], is modeled by a directed graph $G=(V,E)$ where $V$ is the collection of bandit arms, and once an arm is triggered, all its incident arms are observed. A fundamental question is how the structure of the graph affects the min-max regret. We propose the notions of the fractional weak domination number $\delta^*$ and the $k$-packing independence number capturing upper bound and lower bound for the regret respectively.  We show that the two notions are inherently connected via aligning them with the linear program of the weakly dominating set and its dual --- the fractional vertex packing set respectively. Based on this connection, we utilize the strong duality theorem to prove a general regret upper bound $O\left(\left(\delta^*\log  |V|\right)^{\frac{1}{3}}T^{\frac{2}{3}}\right)$ and a lower bound $\Omega\left(\left(\delta^*/\alpha\right)^{\frac{1}{3}}T^{\frac{2}{3}}\right)$ where $\alpha$ is the integrality gap of the dual linear program. Therefore, our bounds are tight up to a $\left(\log |V|\right)^{\frac{1}{3}}$ factor on graphs with bounded integrality gap for the vertex packing problem including trees and graphs with bounded degree. Moreover, we show that for several special families of graphs, we can get rid of the $\left(\log |V|\right)^{\frac{1}{3}}$ factor and establish optimal regret.

## SpreadGNN: Serverless Multi-task Federated Learning for Graph Neural Networks

### TL;DR

A Multi-task framework to train GNNs in a serverless federated setting.

### Abstract

Graph Neural Networks (GNNs) are the first choice methods for graph machine learning problems thanks to their ability to learn state-of-the-art level representations from graph-structured data. However, centralizing a massive amount of real-world graph data for GNN training is prohibitive due to user-side privacy concerns, regulation restrictions, and commercial competition. Federated Learning is the de-facto standard for collaborative training of machine learning models over many distributed edge devices without the need for centralization. Nevertheless, training graph neural networks in a federated setting is vaguely defined and brings statistical and systems challenges. This work proposes \texttt{SpreadGNN}, a novel federated GNN training framework capable of operating on decentralized and serverless topologies for the first time in the literature. \texttt{SpreadGNN} extends federated multi-task learning to realistic serverless settings for GNNs, and utilizes a novel optimization algorithm, \textit{Decentralized Periodic Averaging SGD (DPA-SGD)}, to collaboratively train GNN models over decentralized clients for graph-level machine learning tasks. We empirically demonstrate the efficacy of our framework on a variety of molecular property prediction tasks. SpreadGNN can match or even slightly outperform GNN models trained over a central server-dependent federated learning system, even in constrained topologies. We also provide a convergence analysis to understand why \textit{DPA-SGD} is a suitable optimization algorithm.

## Permute Me Softly: Learning Soft Permutations for Graph Representations

### TL;DR

None

### Abstract

Graph neural networks (GNNs) have recently emerged as a dominant paradigm for machine learning with graphs. Research on GNNs has mainly focused on the family of message passing neural networks (MPNNs). Similar to the Weisfeiler-Leman (WL) test of isomorphism, these models follow an iterative neighborhood aggregation procedure to update vertex representations, and they next compute graph representations by aggregating the representations of the vertices. Although very successful, MPNNs have been studied intensively in the past few years. Thus, there is a need for novel architectures which will allow research in the field to break away from MPNNs. In this paper, we propose a new graph neural network model, so-called $\pi$-GNN which learns a ''soft'' permutation (i.e., doubly stochastic) matrix for each graph, and thus projects all graphs into a common vector space. The learned matrices impose a ''soft'' ordering on the vertices of the input graphs, and based on this ordering, the adjacency matrices are mapped into vectors. These vectors can be fed into fully-connected or convolutional layers to deal with supervised learning tasks. In case of large graphs, to make the model more efficient in terms of running time and memory, we further relax the doubly stochastic matrices to row stochastic matrices. We empirically evaluate the model on graph classification and graph regression datasets and show that it achieves competitive performance.

## Reinforcement Learning Enhanced Explainer for Graph Neural Networks

### TL;DR

We generate explanations for graph neural networks by a reinforcement learning enhanced method.

### Abstract

Graph neural networks (GNNs) have recently emerged as revolutionary technologies for machine learning tasks on graphs. In GNNs, the graph structure is generally incorporated with node representation via the message passing scheme, making the explanation much more challenging. Given a trained GNN model, a GNN explainer aims to identify a most influential subgraph to interpret the prediction of an instance (e.g., a node or a graph), which is essentially a combinatorial optimization problem over graph. The existing works solve this problem by continuous relaxation or search-based heuristics. But they suffer from key issues such as violation of message passing and hand-crafted heuristics, leading to inferior interpretability. To address these issues, we propose a RL-enhanced GNN explainer, RG-Explainer, which consists of three main components: starting point selection, iterative graph generation and stopping criteria learning. RG-Explainer could construct a connected explanatory subgraph by sequentially adding nodes from the boundary of the current generated graph, which is consistent with the message passing scheme. Further, we design an effective seed locator to select the starting point, and learn stopping criteria to generate superior explanations. Extensive experiments on both synthetic and real datasets show that RG-Explainer outperforms state-of-the-art GNN explainers. Moreover, RG-Explainer can be applied in the inductive setting, demonstrating its better generalization ability. 

## Distribution Knowledge Embedding for Graph Pooling

### TL;DR

None

### Abstract

Graph-level representation learning is the pivotal step for downstream tasks that operate on the whole graph. The most common approach to this problem heretofore is graph pooling, where node features are typically averaged or summed to obtain the graph representations. However, pooling operations like averaging or summing inevitably cause massive information missing, which may severely downgrade the final performance. In this paper, we argue what is crucial to graph-level downstream tasks includes not only the topological structure but also the distribution from which nodes are sampled. Therefore, powered by existing Graph Neural Networks (GNN), we propose a new plug-and-play pooling module, termed as Distribution Knowledge Embedding (DKEPool), where graphs are rephrased as distributions on top of GNNs and the pooling goal is to summarize the entire distribution information instead of retaining a certain feature vector by simple predefined pooling operations. A DKEPool network de facto disassembles representation learning into two stages, structure learning and distribution learning. Structure learning follows a recursive neighborhood aggregation scheme to update node features where structure information is obtained. Distribution learning, on the other hand, omits node interconnections and focuses more on the distribution depicted by all the nodes.  Extensive experiments demonstrate that the proposed DKEPool significantly and consistently outperforms the state-of-the-art methods. Code and models will be made publicly available soon.

## Distributed Online Learning for Joint Regret with Communication Constraints

### TL;DR

None

### Abstract

We consider distributed online learning for joint regret with
  communication constraints. In this setting, there are multiple agents
  that are connected in a graph. Each round, an adversary first
  activates one of the agents to issue a prediction and provides a
  corresponding gradient, and then the agents are allowed to send a
  $b$-bit message to their neighbors in the graph. All agents cooperate
  to control the joint regret, which is the sum of the losses of the
  activated agents minus the losses evaluated at the best fixed common
  comparator parameters $\mathbf u$. We observe that it is suboptimal for
  agents to wait for
  gradients that take too long to arrive. Instead, the graph should
  be partitioned into local clusters that communicate among
  themselves. 
  Our main result is a new method that can adapt to the optimal graph
  partition for the adversarial activations and gradients, where the
  graph partition is selected from a set of candidate partitions. A
  crucial building block along the way is a new algorithm for online
  convex optimization with delayed gradient information that is
  comparator-adaptive, meaning that its joint regret scales with the
  norm of the comparator $\|\mathbf  u\|$. We further provide near-optimal
  gradient compression schemes depending on the ratio of $b$ and the
  dimension times the diameter of the graph.

## Hash-Net$^2$: Hashing Deep Neural Network Models by Graph Neural Networks

### TL;DR

This method generate perceptual binary hash codes for deep neural network models by graph neural networks.

### Abstract

  In recent years, the advances of deep learning have boosted the practical development, distribution and implementation of deep neural networks (DNNs). Massive DNN models are diverse in category, quantity and open source frameworks for implementation. Inspired by the demand of neural networks intellectual protection and image hashing technique, the paper presents a new problem called ``DNN model retrieval''. To address this issue, we introduce Hash-Net$^2$, a multi-framework-compatible learning-based hashing approach for retrieving similar network structures in the form of distinct deep learning frameworks such as PyTorch, TensorFlow, Keras, etc. Hash-Net$^2$ is a novel DNN hashing scheme based on graph neural networks, consisting of two stages: graph generator and graph hashing. In graph generator, the target DNN model is first converted and optimized into a graph according to a set of operations. Then it is assigned with additional information extracted from the execution of the original model.  In graph hashing, it learns to construct a compact binary hash code, and further classify for DNN models. The constructed hash function can well preserve the similarities of both the topology structure and the semantics information among diverse neural networks models. Experimental results demonstrate that the proposed scheme is effective to learn a hash function for DNN model retrieval, exhibiting its generalizability and efficiency on a variety of deep models.

## Program and Dependency-enhanced Graph Attention Networks for Explainable Fact Verification

### TL;DR

None

### Abstract

Performing fact verification based on structured data such as tables possesses heavy demand and is also a challenging task. The current research is usually to construct linguistic inference in statement-table graph networks, and symbolic reasoning in program-table graph networks to capture evidence for explainable fact verification. However, the existing methods still have several drawbacks: 1) Statement-table graph networks constructed by them in linguistic inference are severely sparse and ignore the dependency of statements; and 2) There is a lack of effective communication and fusion between the two types of networks, which leads to the failure to cooperatively capture the valuable evidence between them. In this paper, we propose Program and dEpendency-enhanced Graph Attention Networks (PEGAN) to capture associated and logical evidence for explainable fact verification. Specifically, in linguistic inference stage, we propose a tight-fitting statement-table graph network to alleviate the sparsity between statement dependency and table subgraphs, and design local association modeling on this network to infer deep association evidence. In symbolic reasoning stage, we construct global consistent reasoning on the program-table graph network to capture logical evidence. To establish reasonable communication and fusion between the above two stages, we devise a pyramid interaction fusion network to make the two captured evidence interact and collaborate. Experiments on TABFACT dataset reveal the superiority of our model.

## Inductive Structural Role Embedding on Large-scale Graphs

### TL;DR

We propose InSuRE, an inductive structural embedding method, which utilizes a local diffusion kernel to embed nodes' structural roles.

### Abstract

Graph representation learning approaches have been proposed to extract the structural identities of the nodes in a graph. However, most of existing structural role embedding methods are transductive, in the sense that the embedding cannot be generalized to unseen nodes. Here we introduce InSuRE, an inductive method to embed nodes' structural roles. Instead of leveraging a diffusion process on the entire graph, we characterize a local diffusion kernel with two learnable parameters, the local neighborhood radius and corresponding diffusion scale. The embedding of unseen nodes can be efficiently generated based on their neighborhood topology and learned parameters. The time complexity of our algorithm is linear with the number of edges in the graph. Experiments demonstrate that InSuRE outperforms baseline methods in an inductive experiment setting, and is capable of embedding large graphs efficiently. 

## Graph Attention Transformer Network for Multi-Label Image Classification

### TL;DR

We proposed Graph Attention Transformer Network for multi-label classification and achieve state-of-the-art performance. 

### Abstract

Multi-label classification aims to recognize multiple objects or attributes from images by using graph neural networks to exploit the underlying inter-label dependencies. However, it is a challenging issue to learn proper label graphs to characterize such inter-label correlations or dependencies effectively.  In this paper, we propose a Graph Attention Transformer Network (GATN), a general framework for multi-label image classification that can effectively mine complex inter-label relationships. In our proposed model, we adopt several self-attention branches to generate multi-hop connections and enhance the model's ability to learn new graph structures. In addition, we use label embedding to calculate the inter-label distances and generate an initial graph adjacency matrix. Our extensive experiments have demonstrated that our proposed methods can achieve state-of-the-art performance on many datasets.

## Efficient Distribution of Deep Learning on Large Graphs

### TL;DR

We distribute graph neural network computation on CPUs and GPUs using distributed graph analytics systems.

### Abstract

Graph neural networks (GNN) are compute intensive; thus, they are attractive for1acceleration on distributed platforms. We present DeepGalois, a GNN framework targeting distributed CPUs and GPUs. DeepGalois is designed for efficient communication of high-dimensional feature vectors used in GNN. The graph partitioning engine supports different partitioning policies and helps the user make tradeoffs among task division, memory usage, and communication overhead, leading to fast feature learning without compromising accuracy.  The communication engine minimizes communication overhead by exploiting partitioning invariants and communication bandwidth in modern clusters. Evaluation on a CPU production cluster for the representative reddit and ogbn-products datasets demonstrates that DeepGalois on 32 machines is 2.5× and 2.3× faster than that on 1 machine in average epoch time and time to accuracy, respectively. On 32 machines, Deep-Galois outperforms DistDGL by 4× and 8.9× in average epoch time and time to accuracy, respectively. For the reddit graph on GPUs, DeepGalois outperforms both CAGNET and Roc by 1.7× on average for varying numbers of GPUs

## Simpler Graph Neural Networks for Optimization on Graphs

### TL;DR

None

### Abstract

There is a recent trend towards developing simplified graph neural networks (GNNs) for efficiency and interpretability. Some work has shown the excellent performance of simplified GNNs on semi-supervised and supervised tasks such as node classification and graph classification. In this study, we explore the performance of simplified GNN models for unsupervised graph optimization problems. We proposed a simple and general framework SGNN that integrates one message-passing layer and a single-layer perceptron followed by a softmax operation for node assignments. SGNN is a differentiable and pure end-to-end method that allows direct optimization on graphs. Experimental results show that the proposed SGNN tackles graph optimization problems with high quality and efficiency, superior to the previous state-of-the-art methods. Moreover, SGNN has an excellent performance in solving the optimization tasks in which clusters are size-constrained.

## Community Channel-Net: Community Graph Topology for Structured Channel-wise Interactions

### TL;DR

A dynamic and structured channel interaction framework based on community-graph topology that boosts the DNNs performances via encouraging diverse interactions.

### Abstract

Deep neural networks (DNNs) achieve superior performances on various intelligence tasks. However, the layer-wise structure of DNNs isolates the channel interactions between successive layers, which has significantly impeded efficient learning of DNNs. Several existing methods encourage channel-wise information exchange and verify their effectiveness in a heuristic and empirical manner. In this paper, we unify these methods from the graph view, where they can be regarded as \emph{information exchange on the channel graph}.  We have also identified that the performance gain has a positive correlation with channel diversity. This new insight has driven us to develop a novel channel interaction method, which constructs a \emph{sparse}, \emph{structured}, and \emph{dynamic} channel graph with community topology, increasing the channel diversity. In the channel graph, each community contains a set of channels with semantic commonalities. Information exchange, including message passing and aggregation, is conducted on the constructed channel graph. Thus, the channels from the same community can gain more complementary features through high-density interactions, and those important channels from distinct communities can learn more diverse feature representations from each other. Our proposed method consistently outperforms other channel interaction methods on image classification tasks across various backbone models. 

## Video Inpainting with Dynamic Graph Memory Bank

### TL;DR

None

### Abstract

Video inpainting aims to generate plausible contents for the missing regions and faces the challenge of aggregating spatial and temporal information in the corrupted video effectively. In this paper, we propose a dynamic graph memory bank to settle this challenge. To model the long-range temporal dependency, a memory bank is built and updated dynamically with the input visual information flow, and the relationships among the memory items are modeled through a graph-based message propagation. Benefiting from the dynamic graph memory bank, both contents and their relationships in the corrupted video are well exploited. Furthermore, considering the spatial misalignment across different frames may degrade the quality of features in the dynamic graph memory bank, we propose a motion-guided feature alignment module to alleviate this problem. The proposed module cooperates with the dynamic graph memory bank to improve the network’s information aggregation ability in spatial and temporal dimensions. In this way, more details of the missing regions are recovered and the inpainting results are also more plausible. Extensive experiments on YouTube-VOS and DAVIS datasets demonstrate the superiority of our approach when compared with the state-of-the-arts. The code will be released upon publication. 

## Learned Low Precision Graph Neural Networks

### TL;DR

None

### Abstract

Deep Graph Neural Networks (GNNs) show promising performance on a range of graph tasks, yet their run-time performance is tightly coupled to the large input graph sizes. We show, for the first time, how to systematically quantise GNNs with minimal or no loss in performance using Network Architecture Search (NAS). We investigate the novel quantisation search space of GNNs. The proposed NAS mechanism, named Low Precision Graph NAS (LPGNAS), constrains both architecture and quantisation choices to be differentiable. LPGNAS learns the optimal architecture coupled with the best quantisation strategy for different components in the GNN automatically using back-propagation in a single search round. On eight different datasets, solving the task of classifying unseen nodes in a graph, LPGNAS generates quantised models with significant reductions in computational requirements but with similar or even higher accuracy compared to manually designed networks and other NAS results. In particular, on the Pubmed dataset, LPGNAS offers a $0.4\%$ increase in accuracy compared to the best NAS competitor. Comparing to the best-performing manual baseline, LPGNAS shows a $3.5\%$ increase in accuracy while using around $130 \times$ less bitwise operations.

## Graph Variational Transformer for Retrosynthesis Prediction

### TL;DR

We develop a deep conditional generative model for reactant prediction based on hierarchical graph representations for each molecule.

### Abstract

Retrosynthesis prediction refers to transforming a given target molecule into a set of commercially available building blocks. It is fundamental to organic chemistry. Existing computer-assisted retrosynthesis methods either have difficulty modeling \textit{multi-route} retrosynthesis scenarios, or suffer from limited coverage of reaction space. To tackle these problems, we propose a graph variational Transformer for retrosynthesis, which implicitly models reaction types with hierarchical graph representations to direct the reactant generation. Specifically, we treat retrosynthesis as a sequence-to-sequence translation task and develop a deep conditional generative model for reactant prediction using Gaussian latent variables. We design a graph neural network to derive both atom-level and molecule-level embeddings, which are used to enhance the encoder module and decoder module of Transformer, respectively.  Atom-level embeddings enrich chemical symbol representations with molecular structure information, while molecule-level embeddings imply potential reaction types and are used to modulate the prior on Gaussian latent variables. Experiments demonstrate that our graph variational Transformer can generate diverse synthesis routes for an identical target product. Moreover, our graph variational Transformer outperforms all sequence-based baselines for retrosynthesis prediction on the benchmark data set USPTO-50k. 

## Joint Modeling of Visual Objects and Relations for Scene Graph Generation

### TL;DR

This work proposes a principled model to predict a whole scene graph by jointly capturing all the label dependency within it.

### Abstract

An in-depth scene understanding usually requires recognizing all the objects and their relations in an image, encoded as a scene graph. Most existing approaches for scene graph generation first independently recognize each object and then predict their relations independently. Though these approaches are very efficient, they ignore the dependency between different objects as well as between their relations. In this paper, we propose a principled approach to jointly predict the entire scene graph by fully capturing the dependency between different objects and between their relations. Specifically, we establish a unified conditional random field (CRF) to model the joint distribution of all the objects and their relations in a scene graph. We carefully design the potential functions to enable relational reasoning among different objects according to knowledge graph embedding methods. We further propose an efficient and effective algorithm for inference based on mean-field variational inference, in which we first provide a warm initialization by independently predicting the objects and their relations according to the current model, followed by a few iterations of relational reasoning. Experimental results on both the relationship retrieval and zero-shot relationship retrieval tasks prove the efficiency and efficacy of our proposed approach.

## GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph

### TL;DR

This work is highlighted the GNN-nested Transformers, a new model which outperforms the existing GNN+PLM methods with equally competitive efficiency and scalability.

### Abstract

The representation learning on textual graph is to generate low-dimensional embeddings for the nodes based on the individual textual features and the neighbourhood information. Recent breakthroughs on pretrained language models and graph neural networks push forward the development of corresponding techniques. The existing works mainly rely on the cascaded model architecture: the textual features of nodes are independently encoded by language models at first; the textual embeddings are aggregated by graph neural networks afterwards. However, the above architecture is limited due to the independent modeling of textual features. In this work, we propose GraphFormers, where layerwise GNN components are nested alongside the transformer blocks of language models. With the proposed architecture, the text encoding and the graph aggregation are fused into an iterative workflow, making each node's semantic accurately comprehended from the global perspective. In addition, a progressive learning strategy is introduced, where the model is successively trained on manipulated data and original data to reinforce its capability of integrating information on graph. Extensive evaluations are conducted on three large-scale benchmark datasets, where GraphFormers outperform the SOTA baselines with comparable running efficiency.

## Deep Graph Matching for Partial Label Learning

### TL;DR

None

### Abstract

Partial Label Learning (PLL) aims to learn a robust classifier from the training data, where each instance is associated with a set of candidate labels, among which only one is correct. The key to deal with such problem lies in how to disambiguate each candidate label set and obtain the correct matching between each instance and its ground-truth label. Accordingly, in this paper, we formulate the task of PLL problem as an ``instance-label'' matching selection problem, and propose a DeepGNN-based graph matching PLL approach to solve it. Specifically, we first construct all instances and labels as graph nodes into two different graphs respectively, and then integrate them into a unified matching graph by connecting each instance to its candidate labels. Afterwards, the graph attention mechanism is adopted to aggregate and update all nodes state on the instance graph to form structural representations for each instance. Finally, each candidate label is embedded into its corresponding instance and derives a matching affinity score for each instance-label correspondence with a progressive cross-entropy loss. Extensive experiments on both synthetic and real-world data sets have demonstrated the superiority of our proposed method.

## Learning Knowledge Graph-based World Models of Textual Environments

### TL;DR

We teach agents to simultaneously model and act in interactive, situated, textual worlds by mapping them with knowledge graphs.

### Abstract

World models improve a learning agent's ability to efficiently operate in interactive and situated environments. This work focuses on the task of building world models of text-based game environments. Text-based games, or interactive narratives, are reinforcement learning environments in which agents perceive and interact with the world using textual natural language. These environments contain long, multi-step puzzles or quests woven through a world that is filled with hundreds of characters, locations, and objects. Our world model learns to simultaneously: (1) predict changes in the world caused by an agent's actions when representing the world as a knowledge graph; and (2) generate the set of contextually relevant natural language actions required to operate in the world. We frame this task as a Set of Sequences generation problem by exploiting the inherent structure of knowledge graphs and actions and introduce both a transformer-based multi-task architecture and a loss function to train it. A zero-shot ablation study on never-before-seen textual worlds shows that our methodology significantly outperforms existing textual world modeling techniques as well as the importance of each of our contributions.

## A Faster Maximum Cardinality Matching Algorithm with Applications in Machine Learning

### TL;DR

None

### Abstract

Maximum cardinality bipartite matching is an important graph optimization problem with several applications. For instance, maximum cardinality matching in a $\delta$-disc graph can be used in the computation of several distance metrics between distributions that are popular in machine learning and statistics. These metrics include the bottleneck distance, the Wasserstein distance and the Levy-Prokhorov distance. For any point sets $A, B \subset \mathbb{R}^2$, the $\delta$-disc graph is a bipartite graph formed by connecting every pair of points $(a,b) \in A\times B$ by an edge if the Euclidean distance between them is at most $\delta$. Using the classical Hopcroft-Karp algorithm, a maximum-cardinality matching on any $\delta$-disc graph can be found in $\tilde{O}(n^{3/2})$ time. In this paper, we present a simplification of a recent algorithm (Lahn and Raghvendra, JoCG 2021) for the maximum cardinality matching problem and describe how a maximum cardinality matching in a $\delta$-disc graph can be computed asymptotically faster than $O(n^{3/2})$ time for moderately dense point sets. As applications, we show that if $A$ and $B$ are point sets drawn uniformly at random from a unit square, an exact bottleneck distance can be computed in $\tilde{O}(n^{4/3})$ time. On the other hand, experiments suggest that the Hopcroft-Karp algorithm seems to take roughly $\Theta (n^{3/2})$ time for this case. This translates to substantial improvements in execution time for larger inputs.

## Learning Dynamic Graph Representation of Brain Connectome with Spatio-Temporal Attention

### TL;DR

We propose a method for learning dynamic graph representation of brain connectome that achieves state-of-the art performance while providing spatio-temporal explainability.

### Abstract

Functional connectivity (FC) between regions of the brain can be assessed by the degree of temporal correlation measured with functional neuroimaging modalities. Based on the fact that these connectivities build a network, graph-based approaches for analyzing the brain connectome have provided insights into the functions of the human brain. The development of graph neural networks (GNNs) capable of learning representation from graph structured data has led to increased interest in learning the graph representation of the brain connectome. Although recent attempts to apply GNN to the FC network have shown promising results, there is still a common limitation that they usually do not incorporate the dynamic characteristics of the FC network which fluctuates over time. In addition, a few studies that have attempted to use dynamic FC as an input for the GNN reported a reduction in performance compared to static FC methods, and did not provide temporal explainability. Here, we propose STAGIN, a method for learning dynamic graph representation of the brain connectome with spatio-temporal attention. Specifically, a temporal sequence of brain graphs is input to the STAGIN to obtain the dynamic graph representation, while novel READOUT functions and the Transformer encoder provide spatial and temporal explainability with attention, respectively. Experiments on the HCP-Rest and the HCP-Task datasets demonstrate exceptional performance of our proposed method. Analysis of the spatio-temporal attention also provide concurrent interpretation with the neuroscientific knowledge, which further validates our method. Code is available at (anonymized)

## Fast Graph Neural Tangent Kernel via Kronecker Sketching

### TL;DR

We propose an iterative sketching algorithm to accelerate the graph neural tangent kernel regression.

### Abstract

Many deep learning tasks need to deal with graph data (e.g., social networks, protein structures, code ASTs). Due to the importance of these tasks, people turned to Graph Neural Networks (GNNs) as the de facto method for machine learning on graph data. GNNs have become widely applied due to their convincing performance. Unfortunately, one major barrier to using GNNs is that GNNs require substantial time and resources to train. Recently, a new method for learning on graph data is \textit{Graph Neural Tangent Kernel (GNTK)}~\cite{dhs+19}. GNTK is an application of Neural Tangent Kernel (NTK)~\cite{jgh18} (a kernel method) on graph data, and solving NTK regression is equivalent to using gradient descent to train an infinite-wide neural network. The key benefit of using GNTK is that, similar to any kernel method, GNTK's parameters can be solved directly in a single step, avoiding time-consuming gradient descent. Meanwhile, sketching has become increasingly used in speeding up various optimization problems, including solving kernel regression. Given a kernel matrix of $n$ graphs, using sketching in solving kernel regression can reduce the running time to $o(n^3)$. But unfortunately such methods usually requires extensive knowledge about the kernel matrix beforehand, while in the case of GNTK we find that the construction of the kernel matrix is already $O(n^2N^4)$, assuming each graph has $N$ nodes. The kernel matrix construction time can be a major performance bottleneck when the size of graphs $N$ increases. A natural question to ask is thus whether we can speed up the kernel matrix construction to improve GNTK regression's end-to-end running time. This paper provides the first algorithm to construct the kernel matrix in $o(n^2N^3)$ running time.

## PipeGCN: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication

### TL;DR

None

### Abstract

Graph Convolutional Networks (GCNs) is the state-of-the-art method for learning graph-structured data. Training large-scale GCNs requires distributed training across multiple accelerators such that each accelerator is able to hold a partitioned subgraph. However, distributed GCN training incurs prohibitive overhead of communicating node features and gradients among partitions for every GCN layer in each training iteration, limiting the achievable training efficiency and model scalability. To this end, we propose PipeGCN, a simple-yet-effective scheme that hides the communication overhead by pipelining inter-partition communication with intra-partition computation. It is non-trivial to pipeline for efficient GCN training, as communicated node features/gradients will become stale and thus can harm the convergence, negating the pipeline benefit. Notably, little is known regarding the convergence rate of GCN training with stale features. This work not only provides a theoretical convergence guarantee but also finds the convergence rate of PipeGCN to be close to that of the vanilla distributed GCN training without pipeline. Furthermore, we develop a smoothing method to further improve PipeGCN's convergence. Extensive experiments show that PipeGCN can largely boost training throughput (up to 2.2×) while achieving the same accuracy as its vanilla counterpart and that PipeGCN also outperforms existing full-graph training methods. All code will be released publicly upon acceptance.

## Does Graph Neural Network Require Training? A Training-Free Framework for Graph Matching

### TL;DR

We present a framework to boost the performance of untrained GNNs for Graph Matching

### Abstract

We present TFGM (Training Free Graph Matching), a framework to provide both theoretical and empirical foundations for Graph Neural Network (GNN) based graph matching without training. It involves two types of training-free GNNs, i.e., weight-free GNN (without weights) and random-weight GNN (relying on randomly initialized weights). Theoretically, we prove that either of them is a linear relaxation to the NP-hard quadratic assignment problem of graph matching. This guarantees an efficient polynomial complexity. Empirically, we further improve the basic TFGM by handcrafting two types of matching priors into the architecture of GNNs: neighborhood information of different localities and annotation data if available. For evaluation, we have conducted extensive experiments on a broad set of settings, including supervised keypoint matching between images, semi-supervised entity alignment between knowledge graphs, and unsupervised alignment between protein interaction networks. The results of applying TFGM on various types of GNNs show promising improvements over baselines. Further ablation studies also demonstrate the effective yet efficient training-free procedure of our proposed method. Our code will be released later.

## Representing Hierarchical Structure By Using Cone Embedding

### TL;DR

A method for embedding graph which has a hierarchical structure using metric cone.

### Abstract

Graph embedding has become an important technique with applications in various fields such as social networks and knowledge graph completion. In particular, embedding methods into non-Euclidean spaces, such as Poincaré embedding, have been proposed to capture the hierarchical structure of graphs, and their effectiveness has been reported. However, most of the existing methods have isometric maps in the embedding space, and the indicator of hierarchy is not invariant for isometric maps. In this paper, we propose a method of graph embedding into metric cones, cone embedding, which solves this problem, and furthermore has the following advantages: 1) it provides a indicator of hierarchical information that is both geometrically and intuitively natural to interpret, 2) we can extract the hierarchical structure from a graph embedding output of other methods by learning additional one-dimensional parameters, and 3) we can change the curvature of the embedding space via a hyperparameter.

## QPGCN: Graph Convolutional Network with a Quadratic Polynomial Filter for Overcoming Over-smoothing

### TL;DR

We propose a gragh convolutional network with a novel quadratic polynomial filter to better alleviate over-smoothing.

### Abstract

Graph convolutional networks have developed rapidly these years. Over-smoothing is an important factor that makes it difficult to deepen the networks, affecting the further development of graph convolutional networks. There have been some studies to solve the over-smoothing issue. However, the shapes of the filters in the graph convolutional networks are fixed, and the convolution operations in some networks require eigendecomposition. Starting from the relationship between filters and over-smoothing, we propose a novel way  to overcome over-smoothing in this paper.  In this way, a novel quadratic polynomial filter (QPF) is proposed, and then a quadratic polynomial graph convolutional network (QPGCN) is derived. Without increasing the complexity of the network, QPGCN  can adaptively learn the shape of QPF and does not require eigendecomposition, which can better alleviate over-smoothing.  The extensive experiments on Cora, Citeseer, Pubmed and DBLP datasets show that QPGCN achieves state-of-the-art performance.

## Understanding Pooling in Graph Neural Networks

### TL;DR

We propose a new framework to unify graph pooling operators, introduce a taxonomy, and run several experiments.

### Abstract

Inspired by the conventional pooling layers in convolutional neural networks, many recent works in the field of graph machine learning have introduced pooling operators to reduce the size of graphs. 
The great variety in the literature stems from the many possible strategies for coarsening a graph, which may depend on different assumptions on the graph structure or the specific downstream task.
In this paper we propose a formal characterization of graph pooling based on three main operations, called selection, reduction, and connection, with the goal of unifying the literature under a common framework. 
Following this formalization, we introduce a taxonomy of pooling operators and categorize more than thirty pooling methods proposed in recent literature. 
We propose criteria to evaluate the performance of a pooling operator and use them to investigate and contrast the behavior of different classes of the taxonomy on a variety of tasks.

## Partition-Based Active Learning for Graph Neural Networks

### TL;DR

We develop a partition-based active learning method for graph neural networks by leveraging the local and global smoothness properties of the graph-structured data.

### Abstract

We study the problem of semi-supervised learning with Graph Neural Networks (GNNs) in an active learning setup, and propose GraphPart, a novel partition-based active learning approach for GNNs. GraphPart first split the graph into disjoint partitions and then selects representative nodes within each partition to annotate. The proposed method is motivated by an analysis of the classification error under various types of smoothness assumptions over the graph and the node features. Extensive experiments on multiple benchmark datasets demonstrate that the proposed method outperforms existing active learning methods for GNNs for a wide range of annotation budgets.

## Online Bipartite Matching with Predicted Degrees

### TL;DR

We empirically and theoretically demonstrate that a degree predictor (potentially learned from past data) can be used to improve results for online bipartite matching.

### Abstract

We propose a model for online graph problems where algorithms are given access to an oracle that predicts the degrees of nodes in the graph (e.g., based on past data). Within this model, we study the classic problem of online bipartite matching. An extensive empirical evaluation shows that a greedy algorithm called MinPredictedDegree compares favorably to state-of-the-art online algorithms for this problem. We also initiate the theoretical study of MinPredictedDegree on a natural random graph model with power law degree distribution and demonstrate that MinPredictedDegree produces matchings almost as large as the maximum matching on such graphs.

## Graph skeletonization of high-dimensional point cloud data via topological method

### TL;DR

None

### Abstract

Geometric graphs form an important family of hidden structures behind data. In this paper, we develop an efficient and robust algorithm to infer a hidden graph skeleton behind a point cloud data (PCD) embedded in high dimensional space. Previously, there has been much work to recover a hidden graph from a low-dimensional density field, or relatively clean high-dimensional PCD (in the sense that the input points are within a small bounded distance to a true hidden graph). 
Our work closes this gap. Our proposed approach builds upon the recent line of work on using a persistence-guided discrete Morse (DM) theory based approach to reconstruct a geometric graph from a density field defined over a triangulation of low-dimensional Euclidean domain. In particular, we first give a simple generalization of this DM-based algorithm from a density-function perspective to a general filtration perspective. On the theoretical front, we show that the output of the generalized algorithm contains a so-called lexicographic-optimal persistent cycle basis w.r.t the input filtration, justifying that the output is indeed meaningful. On the algorithmic front, this generalization allows us to use the idea of sparsified weighted Rips filtration (developed by Buchet et al) to develop a new graph reconstruction algorithm for noisy point cloud data (PCD) (which do not need to be embedded). The new algorithm is robust to background noise and non-uniform distribution of input points. We provide various experimental results to show the efficiency and effectiveness of our new graph reconstruction algorithm for PCDs.

## Graph Confident Learning for Software Vulnerability Detection

### TL;DR

None

### Abstract

Code vulnerability is the main factor of software being attacked. An increasing vulnerabilities identified in public reports and proprietary codes result in greater threats to software security, which leads to millions in financial losses to software companies. In order to reduce the losses, there are many effective and reliable vulnerability detection methods proposed by researchers to automatically detect software vulnerability. However, the effectiveness of these methods on defect finding is still limited, due to the missing syntactic structural information of source code limitation and existing characterization noise and labeling errors limitation. To solve these limitations, we propose a novel method, named GSL4SVD to detect software vulnerability in the development phase, which can make the software more secure and less vulnerable. GSL4SVD comprises two components: code graph embedding and graph confident learning denoising. To address the missing syntactic structural information of the source code limitation, code graph embedding component extracts the structure and semantic information of the source code based on a sliding window mechanism, and then encodes the source code into a graph structure to capture the patterns and characteristics of code vulnerabilities. Additionally, graph confident learning denoising component identify the characterization noise and labeling errors to improve the quality of datasets. Finally, the Gated Graph sequence Neural Networks (GGNNs) model is retrained to learn the characteristics of the source code in high-quality datasets and detect code vulnerabilities. The experimental results show that our method outperforms the state-of-the-art on three open widely used datasets by 9.4%, 19.1%, 16.9% in terms of Accuracy.

## Propagate, Correct, and Learn: A Meta-learning Approach for Low-resource Representation Learning on Graphs

### TL;DR

None

### Abstract

Inspired by the extensive success of deep learning, graph neural networks (GNNs) have been proposed to learn expressive node representations and demonstrated promising performance on different graph learning tasks. However, existing endeavors predominately focus on the conventional semi-supervised setting where relatively abundant gold-labeled nodes are provided. Such an assumption is often impractical due to the fact that data labeling is unbearably laborious and requires intensive domain knowledge, especially when considering the heterogeneity of graph-structured data. While under the low-resource semi-supervised setting, most of the existing GNNs built upon the message-passing scheme have two major limitations, which can largely undermine the model performance in practice: (i) the notorious oversmoothing issue when increasing the message-passing range; and (ii) the over-reliance on the homophily assumption about the input graph. In this paper, we propose a new architecture that is able to learn simple yet effective GNN models with limited labeled data. Specifically, we first propagate the labels of training data along the graph to generate abundant pseudo labels, then progressively correct the mislabeled nodes during propagation and learn the prediction model within a unified meta-learning algorithm. Extensive experiments demonstrate that our approach offers easy and substantial performance gains compared to existing techniques on different benchmark datasets.  

## Bootstrapped Representation Learning on Graphs

### TL;DR

None

### Abstract

Current state-of-the-art self-supervised learning methods for graph neural networks (GNNs) are based on contrastive learning. 
As such, they heavily depend on the construction of augmentations and negative examples. In many cases, achieving peak performance requires computation and memory quadratic in the number of nodes, which can be prohibitively expensive.
Inspired by BYOL, a recently introduced method for self-supervised learning that does not require negative pairs, we present Bootstrapped Graph Latents, BGRL, a self-supervised graph representation method that gets rid of this potentially quadratic bottleneck. We show that this enables BGRL to achieve state-of-the-art results on a large Open Graph Benchmark dataset to which previous methods do not naively scale. Further, we show that BGRL outperforms or matches previous methods on several smaller established benchmark datasets, while consuming 2-10x less memory.
Moreover, it enables the effective usage of graph attentional (GAT) encoders, allowing us to further improve the state of the art.
In particular on the PPI dataset, using GAT as an encoder we achieve state-of-the-art 70.49\% Micro-F1, using the linear evaluation protocol. On all other datasets under consideration, our model is competitive with the equivalent supervised GNN results, often exceeding them.

## Graph Representation Learning with Individualization and Refinement

### TL;DR

Improving the expressivity of GNNs with techniques from exact graph isomorphism solvers, i.e., Individualization and Refinement. 

### Abstract

Graph Neural Networks (GNNs) have emerged as prominent models for representation learning on graph structured data. GNNs follow an approach of message passing analogous to 1-dimensional Weisfeller Leman (1-WL) test for graph isomorphism and consequently are limited by the distinguishing power of 1-WL. More expressive higher-order GNNs which operate on k-tuples of nodes need increased computational resources in order to process higher-order tensors. Instead of the WL approach, in this work, we follow the classical approach of Individualization and Refinement (IR), a technique followed by most practical isomorphism solvers. Individualization refers to artificially distinguishing a node in the graph and refinement is the propagation of this information to other nodes through message passing. We learn to adaptively select nodes to individualize and to aggregrate the resulting graphs after refinement to help handle the complexity. Our technique lets us learn richer node embeddings while keeping the computational complexity manageable. Theoretically, we show that our procedure is more expressive than 1-WL test. Experiments on benchmark synthetic and real datasets show that our method outperforms prominent 1-WL GNN models as well as competitive higher-order baselines. Furthermore, our method opens new doors for exploring the paradigm of learning on graph structures with individualization and refinement.

## Position and Structure-aware Graph Learning

### TL;DR

None

### Abstract

Various graph node representation learning methods have been proposed in recent years. However, most of these methods merely consider the local information around the nodes, meanwhile the global relative position information of each node is not fully taken into account. Recently, Position-aware Graph Neural Network has been proposed to take the global information into consideration, which aggregates position information from the randomly sampled anchor set nodes to the given target node. With such randomness, some neighbor nodes which may contribute more are likely to be lost. In order to solve the above problem, we propose a Position and Structur-aware Graph Learning framework (\textbf{PSGL}), which consists of three modules: (1) The structure representation module embeds the graph node into a latent space acordding to the local topology. (2) The node position module first pools the graph into some key points, and then propagates the position information from anchor-set which is consist of the key points, to the given target node. (3) The feature fusion module employs attention mechanism to adaptively balance the local structure and global position informations. Experiments on five real-world datasets show that \textbf{PSGL} can achieve state-of-the-art. Our code is publicly available at https://github.com/leaf-ygq.

## A Bi-Level Framework for Learning to Solve Combinatorial Optimization on Graphs

### TL;DR

We propose a hybrid reinforcement learning and traditional heuristics framework based on bi-level optimization to solve combinatorial optimization on graphs.

### Abstract

Combinatorial Optimization (CO) has been a long-standing challenging research topic featured by its NP-hard nature. Traditionally such problems are approximately solved with heuristic algorithms which are usually fast but may sacrifice the solution quality. Currently, machine learning for combinatorial optimization (MLCO) has become a trending research topic, but most existing MLCO methods treat CO as a single-level optimization by directly learning the end-to-end solutions, which are hard to scale up and mostly limited by the capacity of ML models given the high complexity of CO. In this paper, we propose a hybrid approach to combine the best of the two worlds, in which a bi-level framework is developed with an upper-level learning method to optimize the graph (e.g. add, delete or modify edges in a graph), fused with a lower-level heuristic algorithm solving on the optimized graph. Such a bi-level approach simplifies the learning on the original hard CO and can effectively mitigate the demand for model capacity. The experiments and results on several popular CO problems like Directed Acyclic Graph scheduling, Graph Edit Distance and Hamiltonian Cycle Problem show its effectiveness over manually designed heuristics and single-level learning methods.


## Knowledge-inspired 3D Scene Graph Prediction in Point Cloud

### TL;DR

None

### Abstract

Prior commonsense knowledge integration helps identify semantic entities and their relationships in a graphical representation, however, its meaningful abstraction and intervention remain elusive. This paper advocates a knowledge-inspired 3D scene graph prediction method solely based on point clouds. At the mathematical modeling level, we formulate the task as two sub-problems: commonsense learning and scene graph prediction with learned prior knowledge. Unlike conventional methods that learn knowledge embedding and regular patterns from encoded visual information, we propose to suppress the misunderstandings caused by appearance similarities and other perceptual confusion. At the network design level, we devise a graph auto-encoder to automatically extract class-dependent representations and topological patterns from the one-hot class labels and their intrinsic graphical structures, so that the prior knowledge can avoid perceptual errors and noises. We further devise a scene graph prediction model to predict credible relationship triplets by incorporating the related prototype knowledge with perceptual information. Comprehensive experiments confirm that, our method can successfully learn representative knowledge embedding, and the obtained prior knowledge can effectively enhance the accuracy of relationship predictions. Our thorough evaluations indicate the new method can achieve the state-of-the-art performance compared with other scene graph prediction methods.

## Contrastive Learning of Global and Semantic Representations with Heterogeneous GraphSelf-Supervised Networks

### TL;DR

A self-supervised framework for heterogeneous graph

### Abstract

Unsupervised representation learning in heterogeneous graphs has arouse great interest. The heterogeneity of graphs contains rich information but it also raises difficult questions to design unsupervised or self-supervised models. The methods based on random walk are known to be mainly dependent on proximity information of neighbors and lack of the ability to integrate node features. Meta-path has been demonstrated to provide semantic information in heterogeneous graphs, however, it is insufficient to represent global characteristics of the whole graph especially in graph-level tasks. Therefore, it is critical to develop an unsupervised framework that can better capture global properties of heterogeneous graphs and uniformly utilize  both node features and graph structure. Here we propose the concept of global representations derived from multi-hop subgraphs. Compared with meta-paths, multi-hop subgraphs provide a different view to encode the complex relations in heterogeneous graphs. We propose a novel Heterogeneous graph Self-supervised Network (HSN) based on contrastive learning. It learns node embeddings through maximizing mutual information between global and semantic representations leveraging multi-hop subgraphs and meta-paths, respectively. Experiments on both node-level and graph-level tasks show the superiority of the HSN over other unsupervised and supervised models.

## Robust Self-Supervised Structural Graph Neural Network

### TL;DR

None

### Abstract

Recent initial studies have shown the feasibility of transferable graph representation learning to use diverse graph data. However, prior works suffer from two problems. Firstly, the overwhelming routine in graph representation-learning utilizes the node-wise similarity metric defined on embedding vectors that cannot exactly capture the subtle local structure and the network proximity. Secondly, existing works implicitly assume a universal distribution across datasets, which presumably leads to sub-optimal models considering the potential distribution shift.  To address these problems, in this paper, we learn structural embeddings in which the proximity is characterized by 1-Wasserstein distance. We propose a distributionally robust self-supervised graph neural network framework to learn the representations robust to the data distributions and have better generalization abilities. More specifically, the embeddings are based on subgraphs centering at the node of interest, which better preserves the local structure of nodes. To make our model end-to-end trainable, we adopt a deep implicit layer to compute the Wasserstein distance. Extensive experiments demonstrate that the graph encoder learned by our approach can be utilized for various downstream analyses. The results show our algorithm outperforms state-of-the-art baselines, and the ablation study validates the effectiveness of our design.

## A Theoretical Framework For Graph Prediction Problems

### TL;DR

We develop a theoretical framework for analyzing graph prediction problems, where only a single graph is available. Theoretical findings are empirically supported.

### Abstract

We develop a theoretical framework for analyzing graph prediction problems, such as node classification, link prediction, and clustering. In our framework, we assume that there is an underlying distribution over graphs, and the goal is to predict an unknown vector from the input graph sampled from the distribution, where the quality of the prediction is measured by a suitably defined loss function. The challenge here is that we only have a single graph sampled from the underlying distribution. To resolve this issue, we propose to reduce the variance of algorithms. More specifically, we show that, if the variance of an algorithm is small, then the problem of minimizing the loss with respect to the input graph can be reduced to a problem of minimizing the expected loss with respect to the graph sampled from the distribution, and that the latter admits bias-variance decomposition. Subsequently, we show that the variance of an algorithm can be bounded by its (average) sensitivity, which is more amenable to analysis. Finally, we apply our framework to specific algorithms, such as spectral methods for node classification and clustering and matrix factorization for link prediction,  to obtain some insights on the appropriate parameters to be used for these algorithms.

## IGLU: Efficient GCN Training via Lazy Updates

### TL;DR

IGLU is a novel lazy update-based optimization technique for accelerated GCN training with provable convergence guarantees

### Abstract

Graph Convolution Networks (GCN) are used in numerous settings involving a large underlying graph as well as several layers. Standard SGD-based training scales poorly here since each descent step ends up updating node embeddings for a large portion of the graph. Recent methods attempt to remedy this by sub-sampling the graph which does reduce the compute load, but at the cost of biased gradients which may offer suboptimal performance. In this work we introduce a new method IGLU that caches forward-pass embeddings for all nodes at various GCN layers. This enables IGLU to perform lazy updates that do not require updating a large number of node embeddings during descent which offers much faster convergence but does not significantly bias the gradients. Under standard assumptions such as objective smoothness, IGLU provably converges to a first-order saddle point. We validate IGLU extensively on a variety of benchmarks, where it offers up to 1.2% better accuracy despite requiring up to 88% less wall-clock time.

## Heterogeneous Graph Pooling Neural Network for Sleep Stage Classification

### TL;DR

None

### Abstract

Sleep stage classification based on physiological time-series is essential for sleep quality evaluation and the diagnosis of sleep disorders in clinical practice. Existing machine learning studies have achieved adequate results in sleep stage classification. However, those methods neglect the significance of simultaneously capturing the interactivity and heterogeneity of physiological signals and cannot capture the local-to-global features comprehensively. In this paper, we propose a novel Heterogeneous Graph Pooling Neural Network (HGPNN) to employ these essential features. The HGPNN is a deep graph network composed of Heterogeneous Graph Transformer layers and Heterogeneous Graph Pooling layers. One for simultaneously encoding the interactivity and heterogeneity of physiological signals and the other for capturing the local-to-global features of the signals. The experiments conducted on the two benchmark datasets show that the HGPNN outperforms the state-of-the-art models on the sleep stage classification task for both healthy subjects and subjects with sleep disorders.

## PointWavelet: Learning in Spectral Domain for Point Clouds

### TL;DR

None

### Abstract

With the great success of deep learning in 2D visual recognition, it has received increasing attention for 3D point cloud applications, especially due to the rapid development of the autonomous driving technology. However, recent methods usually either transform point cloud to 3d voxels or directly learn point features in an isolated manner, leaving the local structure of neighborhood points poorly investigated. In this paper, we address the above issues by using the spectral graph wavelet transform on local graphs, which is then referred to as the PointWavelet. Specifically, the spectral graph convolution is carried out on the local graph constructed from the point set to learn effective local structure representations. Furthermore, to avoid the high computational complexity of spectral decomposition, we introduce a trainable orthogonal matrix to represent the learned local graph, which significantly accelerates the training process. Extensive experiments on several popular point cloud datasets demonstrate the  effectiveness of the proposed method for point cloud classification and segmentation.

## SIGMA: A Structural Inconsistency Reducing Graph Matching Algorithm

### TL;DR

None

### Abstract

Graph matching finds the correspondence of nodes across two correlated graphs and lies at the core of many applications. When graph side information is not available, the node correspondence is estimated on the sole basis of network topologies. In this paper, we propose a novel criterion to measure the graph matching accuracy, structural inconsistency (SI), which is defined based on the network topological structure. Specifically, SI incorporates the heat diffusion wavelet to accommodate the multi-hop structure of the graphs. Based on SI, we propose a Structural Inconsistency reducing Graph Matching Algorithm (SIGMA), which improves the alignment scores of node pairs that have low SI values in each iteration. Under suitable assumptions, SIGMA can reduce SI values of true counterparts. Furthermore, we demonstrate that SIGMA can be derived by using a mirror descent method to solve the Gromov-Wasserstein distance with a novel K-hop-structure-based matching costs. Extensive experiments show that our method outperforms state-of-the-art methods.

## CORGI: Content-Rich Graph Neural Networks with Attention

### TL;DR

We present CoRGi, a GNN model that considers the rich content data within nodes in the context of their neighbors by assigning user-item-specific attention scores.

### Abstract

Graph representations of a target domain often project it to a set of entities (nodes) and their relations (edges). However, such projections often miss important and rich information. For example, in graph representations used in missing value imputation, items --- represented as nodes --- may contain rich textual information. However, when processing graphs with graph neural networks (GNN), such information is either ignored or summarized into a single vector representation used to initialize the GNN. Towards addressing this, we present CoRGi, a GNN that considers the rich data within nodes in the context of their neighbors. This is achieved by endowing CoRGi's message passing with a personalized attention mechanism over the content of each node. This way, CoRGi assigns user-item-specific attention scores with respect to the words that appear in items. We evaluate CoRGi on two edge-value prediction tasks and show that \modelName is better at making edge-value predictions, especially on sparse regions of the graph.

## Topology-aware Tensor Decomposition for Subgraph Search in Machine Learning

### TL;DR

None

### Abstract

Many machine learning problems can be formulated as subgraph search in a multi-graph. For example, neural architecture search (NAS) involves picking the best operation (edge) for each node pair (multi-edge) in the supernet (multi-graph). To make the discrete selection easier, a common approach as exhibited by DARTS is to perform continuous relaxation that mixes the edges in each multi-edge together. However, existing approaches then recover the edges from the continuous optimization solution independently, thus ignoring topological dependencies among the edges. To address this issue, we propose imposing the multi-graph structure on the tensor of mixing weights. This topology-aware decomposition is parsimonious and can be trained together with the rest of the machine learning model. Empirical results on three diverse tasks, namely, NAS, meta-path discovery in heterogeneous information networks, and logic inference from knowledge graph, demonstrate that the proposed method outperforms the state-of-the-arts for all these tasks.

## Graph Embedding Network for Few-Shot Point Cloud Classification

### TL;DR

We propose a graph matching based method for few-shot point cloud classification.

### Abstract

Most existing approaches for point cloud classification heavily rely on numerous labeled examples of the target categories. In this paper, we propose to study the few-shot point cloud classification problem, where a point cloud classifier must generalize to new categories given only a few labeled examples. Towards this task, we propose a simple yet effective method, named Graph Embedding Network (GENet). Specifically, to better capture the spatial and geometric information of  point cloud data, we firstly design an axis-aware approach to construct a graph for the point cloud and employ a graph embedding module to encode the graph into a feature vector. Then, we obtain the prototype vector of each category by averaging the representations of the examples and predict the label for every testing point cloud by finding its most similar prototype. Our experiments on three few-shot point cloud classification benchmark datasets show that our proposed GENet outperforms the baselines with remarkable performance advantages, which demonstrates its effectiveness.

## Fair Contrastive Learning on Graphs

### TL;DR

None

### Abstract

Node representation learning has demonstrated its effectiveness for various applications on graphs. Particularly, recent developments in contrastive learning have led to promising results in unsupervised node representation learning for a number of tasks. Despite the success of graph contrastive learning and consequent growing interest, fairness is largely under-explored in the field. To this end, this study addresses fairness issues in graph contrastive learning with fairness-aware graph augmentation designs, through adaptive feature masking and edge deletion. In the study, different fairness notions on graphs are introduced, which serve as guidelines for the proposed graph augmentations. Furthermore, theoretical analysis is provided to quantitatively prove that the proposed feature masking approach can reduce intrinsic bias. Experimental results on real social networks are presented to demonstrate that the proposed augmentations can enhance fairness in terms of statistical parity and equal opportunity, while providing comparable classification accuracy to state-of-the-art contrastive methods for node classification.

## Algorithmic Concept-based Explainable Reasoning

### TL;DR

We introduce concept-bottlenecked GNNs and show how they can be used to explain some classical algorithms.

### Abstract

Recent research on graph neural network (GNN) models successfully applied GNNs to classical graph algorithms and combinatorial optimisation problems. This has numerous benefits, such as allowing applications of algorithms when preconditions are not satisfied, or reusing learned models when sufficient training data is not available or can't be generated.  Unfortunately, a key hindrance of these approaches is their lack of explainability, since GNNs are black-box models that cannot be interpreted directly. In this work, we address this limitation by applying existing work on concept-based explanations to GNN models. We introduce concept-bottlenecked GNNs, which rely on a modification to the GNN readout mechanism. Using three case studies we demonstrate that: (i) our proposed model is capable of accurately learning concepts and extracting propositional formulas based on the learned concepts for each target class; (ii) our concept-based GNN models achieve comparative performance with state-of-the-art models; (iii) we can derive global graph concepts, without explicitly providing any supervision on graph-level concepts. 


## \title{Bi-directional Graph Neural Network for Session-based Recommendation}

### TL;DR

None

### Abstract

\begin{abstract}
  Graph neural networks (GNNs) are recently used for session-based recommendation (SBR), which have achieved state-of-the-art performance. However, most GNN based recommendations represent sessions into the homogeneous graph that ignore node or edge type diversity and the potential correlation between directional features. In this paper, we propose a novel Bi-directional Graph Neural Network (BiGNN) model to learn the session representation, which involves identifying useful connections between nodes on directed graphs. For each session graph, BiGNN can capture the forward and backward node hidden vectors respectively, while learning a soft selection of edge types and composite relations to obtain the effective session representation. Experiments show that our model achieved the best performance in all three benchmark SBR tasks against the state-of-the-art methods. We further demonstrate that the learned session representation is very robust to the length of the session, and show the best performance in both long and short sessions.\\
\end{abstract}

## Edge-featured Graph Neural Architecture Search

### TL;DR

We propose edge-featured graph neural architecture search with novel edge-featured search space to learn the optimal GNN architecture using gradient-based search strategy. 

### Abstract

Graph neural networks (GNNs) have been successfully applied to learning representation on graphs in many relational tasks. Recently, researchers study neural architecture search (NAS) to reduce the dependence of human expertise and explore better GNN architectures, but they over-emphasize entity features and ignore latent relation information concealed in the edges. To solve this problem, we incorporate edge features into graph search space and propose Edge-featured Graph Neural Architecture Search (EGNAS) to find the optimal GNN architecture. Specifically, we design rich entity and edge updating operations to learn high-order representations which convey more generic message passing mechanisms. Moreover, the architecture topology in our search space allows to explore complex feature dependence of both entities and edges, which can be efficiently optimized by differentiable search strategy. Experiments at three graph tasks on six datasets show EGNAS can search better GNNs with higher performance than current state-of-the-art human-designed and searched-based GNNs. 

## Exponential Graph is Provably Efficient for Decentralized Deep Training

### TL;DR

We theoretically establish that static and one-peer exponential graphs endow decentralized SGD with fast and high-quality training performance

### Abstract

Decentralized SGD is an emerging training method for deep learning known for its much less (thus faster) communication per iteration, which relaxes the averaging step in parallel SGD to inexact averaging. The less exact the averaging is, however, the more the total iterations the training needs to take. Therefore, the key to making decentralized SGD efficient is to realize nearly-exact averaging using little communication. This requires a skillful choice of communication topology, which is an under-studied topic in decentralized optimization.

In this paper, we study so-called exponential graphs where every node is connected to $O(\log(n))$ neighbors and $n$ is the total number of nodes. This work proves such graphs can lead to both fast communication and effective averaging simultaneously. We also discover that a sequence of $\log(n)$ one-peer exponential graphs, in which each node communicates to one single neighbor per iteration, can together achieve exact averaging. This favorable property enables one-peer exponential graph to average as effective as its static counterpart but communicates more efficiently. We apply these exponential graphs in decentralized (momentum) SGD to obtain the state-of-the-art balance between per-iteration communication and iteration complexity among all commonly-used topologies. Experimental results on a variety of tasks and models demonstrate that decentralized (momentum) SGD over exponential graphs promises both fast and high-quality training.

## Collaborative Causal Discovery with Atomic Interventions

### TL;DR

Introduces a new model for collaborative casual learning of multiple causal graphs and provide efficient intervention-based algorithms for it.

### Abstract

We introduce a new Collaborative Causal Discovery problem, through which we model a common scenario in which we have multiple independent entities each with their own causal graph, and the goal is to simultaneously learn all these causal graphs. We study this problem without the causal sufficiency assumption, using Maximal Ancestral Graphs (MAG) to model the causal graphs, and assuming that we have the ability to actively perform independent single vertex (or atomic) interventions on the entities. If the $M$ underlying (unknown) causal graphs of the entities satisfy a natural notion of clustering, we give algorithms that leverage this property and recovers all the causal graphs using roughly logarithmic in $M$ number of atomic interventions per entity. These are significantly fewer than $n$ atomic interventions per entity required to learn each causal graph separately, where $n$ is the number of observable nodes in the causal graph. We complement our results with a lower bound and discuss various extensions of our collaborative setting.

## Explainability in Graph Convolutional Network for Representation Learning

### TL;DR

None

### Abstract

There has been a rising interest in graph convolutional networks (GCNs) for representation learning including social graphs and recommender systems. These GCNs have shown powerful performance in many tasks such as node classification, link prediction, and knowledge graph learning applications. However, there are only a handful of studies around the explainability in GCNs. Most of the existing methods focus on applying the explainability methods in the traditional convolutional neural network (CNN) to graph convolutional networks. In this work, we introduce a new explainability method for representation learning in GCNs, called EdgeGrad. We show that this new method helps us to identify the most influential edges for node representations.

## Constraining Linear-chain CRFs to Regular Languages

### TL;DR

CRFs can be efficiently constrained to arbitrary regular languages, enforcing nonlocal constraints on their outputs.

### Abstract

In structured prediction, a major challenge for models is to represent the
interdependencies within their output structures.  For the common case where
outputs are structured as a sequence, linear-chain conditional random fields
(CRFs) are a widely used model class which can learn local dependencies in
output sequences.  However, the CRF's Markov assumption makes it impossible for
these models to capture nonlocal dependencies, and standard CRFs are unable to
respect nonlocal constraints of the data (such as global arity constraints on
output labels).  We present a generalization of CRFs that can enforce a broad
class of constraints, including nonlocal ones, by specifying the space of
possible output structures as a regular language $\mathcal{L}$. The resulting
regular-constrained CRF (RegCCRF) has the same formal properties as a standard
CRF, but assigns zero probability to all label sequences not in $\mathcal{L}$.
Notably, RegCCRFs can incorporate their constraints during training, while
related models only enforce constraints during decoding.  We prove that
constrained training is never worse than constrained decoding, and show using
synthetic data that it can be substantially better in practice.  Additionally,
we demonstrate a practical benefit on downstream tasks by incorporating a
RegCCRF into a deep neural model for semantic role labeling, exceeding
state-of-the-art results on a standard dataset.

## Demystifying Topology Optimization in DAG-based Neural Architecture Search

### TL;DR

We demystify the topology optimization in DAG-based NAS and provide insights on designing cell structures, which inspired our design of GRFNAS, a topology-aware NAS method to carry extensive topology optimization in a meta-graph search space.

### Abstract

Directed Acyclic Graphs (DAG) are powerful abstractions of Convolutional Neural Networks (CNN).
Searching promising CNN architectures may benefit from composing multiple DAGs into a meta-graph and allowing more flexible cell topology fabrication.  A predictor-based method has the potential to fully explore the meta-graph search space. Unfortunately, existing predictor-based NAS are topology-agnostic: they can only sample a limited number of candidate architectures and thus difficult to accurately model the large meta-graph search space. In addition, existing predictor-based NAS methods are mostly powered by evolutionary search, which greedily preserves the best child architectures and ends up with locally optimal CNN architectures. In this paper, we conduct extensive studies to demystify the topology optimization in DAG-based NAS within the same search budget as gradient-based methods. Inspired by our study, we propose GRFNAS, a topology-aware predictor-based NAS method that can flexibly explore promising CNN architectures in a large meta-graph search space. 
GRFNAS incorporates topological information to augment training samples for the performance predictor via Graph Augmentation (GraphAug) thus reduces prediction errors for cell topologies in the search space. In addition, GRFNAS proposes Metropolis-Hastings Evolutionary Search (MH-ES) that accepts weaker child architectures with lower probabilities to improve existing evolutionary search and avoid the discovery of locally optimal architectures. Experimental results show that GRFNAS can achieve at least 0.7\% performance gain compared to existing DAG-based NAS methods on ImageNet-1K with similar Multiply-Accumulates (MACs) under the same search cost. GRFNAS also reveals some key insights within the meta-graph search space that serve as future guidance for designing CNN architectures.

## Towards Best-of-All-Worlds Online Learning with Feedback Graphs

### TL;DR

We provide the first best-of-all-worlds regret guarantee for online learning with graph-structured feedback via a novel Tsallis-Shannon regularization.

### Abstract

We study the online learning with feedback graphs framework introduced by Mannor and Shamir (2011), in which the feedback received by the online learner is specified by a graph $G$ over the available actions.  We develop an algorithm that simultaneously achieves regret bounds of the form: $O(\sqrt{\theta(G) T})$ with adversarial losses; $O(\theta(G)\mathrm{polylog}{T})$ with stochastic losses; and $O(\theta(G)\mathrm{polylog}{T} + \sqrt{\theta(G) C})$ with stochastic losses subject to $C$ adversarial corruptions.  Here, $\theta(G)$ is the $clique~covering~number$ of the graph $G$.  Our algorithm is an instantiation of Follow-the-Regularized-Leader with a novel regularization that can be seen as a product of a Tsallis entropy component (inspired by Zimmert and Seldin (2019)) and a Shannon entropy component (analyzed in the corrupted stochastic case by Amir et al. (2020)), thus subtly interpolating between the two forms of entropies.  One of our key technical contributions is in establishing the convexity of this regularizer and controlling its inverse Hessian, despite its complex product structure.

## Incentive Compatible Pareto Alignment for  Multi-Source Large Graphs

### TL;DR

None

### Abstract

In this paper, we focus on learning effective entity matching models over multi-source large-scale data. For real applications, we relax typical assumptions that data distributions/spaces, or entity identities are shared between sources, and propose a Relaxed Multi-source Large-scale Entity-matching (RMLE) problem. Challenges of the problem include 1) how to align large-scale entities between sources to share information and 2) how to mitigate negative transfer from joint learning multi-source data. What's worse, one practical issue is the entanglement between both challenges. Specifically, incorrect alignments may increase negative transfer; while mitigating negative transfer for one source may result in poorly learned representations for other sources and then decrease alignment accuracy. To handle the entangled challenges, we point out that the key is to optimize information sharing first based on Pareto front optimization, by showing that information sharing significantly influences the Pareto front which depicts lower bounds of negative transfer. Consequently, we proposed an Incentive Compatible Pareto Alignment (ICPA) method to first optimize cross-source alignments based on Pareto front optimization, then mitigate negative transfer constrained on the optimized alignments. This mechanism renders each source can learn based on its true preference without worrying about deteriorating representations of other sources. Specifically, the Pareto front optimization encourages minimizing lower bounds of negative transfer, which optimizes whether and which to align. In detail, we adopt graph neural networks to handle data sparsity in each source and a scalable alignment based on sliced graph matching. Comprehensive empirical evaluation results on four large-scale datasets are provided to demonstrate the effectiveness and superiority of ICPA. Online A/B test results at a search advertising platform also demonstrate the effectiveness of ICPA in production environments. We also release an International Entity Graph (IEG) dataset to facilitate future research. 

## Online Matching in Sparse Random Graphs: Non-Asymptotic Performances of Greedy Algorithm

### TL;DR

None

### Abstract

Motivated by sequential budgeted allocation problems, we investigate  online matching problems where connections between vertices are not i.i.d., but they have fixed degree distributions -- the so-called configuration model. We estimate the competitive ratio of the simplest algorithm, GREEDY, by approximating some relevant stochastic discrete processes by their continuous counterparts, that are solutions of an explicit system of partial differential equations. This technique gives precise bounds on the estimation  errors,  with arbitrarily high probability as the problem size increases. In particular, it allows the formal comparison between different configuration models. We also prove that, quite surprisingly,  GREEDY can have  better performance guarantees than RANKING, another celebrated algorithm for online matching that usually outperforms the former.

## ConE: Cone Embeddings for Multi-Hop Reasoning over Knowledge Graphs

### TL;DR

A geometry-based query embedding model that can handle all first-order logical queries.

### Abstract

Query embedding (QE)---which aims to embed entities and first-order logical (FOL) queries in low-dimensional spaces---has shown great power in multi-hop reasoning over knowledge graphs. Recently, embedding entities and queries with geometric shapes becomes a promising direction, as geometric shapes can naturally represent answer sets of queries and logical relationships among them. However, existing geometry-based models have difficulty in modeling queries with negation, which significantly limits their applicability. To address this challenge, we propose a novel query embedding model, namely \textbf{Con}e \textbf{E}mbeddings (ConE), which is the first geometry-based QE model that can handle all the FOL operations, including conjunction, disjunction, and negation. Specifically, ConE represents entities and queries as Cartesian products of two-dimensional cones, where the intersection and union of cones naturally model the conjunction and disjunction operations. By further noticing that the closure of complement of cones remains cones, we design geometric complement operators in the embedding space for the negation operations. Experiments demonstrate that ConE significantly outperforms existing state-of-the-art methods on benchmark datasets.

## Learning Theory Can (Sometimes) Explain Generalisation in Graph Neural Networks

### TL;DR

None

### Abstract

In recent years, several results in the supervised learning setting suggested that classical statistical learning-theoretic measures, such as VC dimension, do not adequately explain the performance of deep learning models which prompted a slew of work in the infinite-width and iteration regimes. However, there is little theoretical explanation for the success of neural networks beyond the supervised setting. In this paper we argue that, under some distributional assumptions, classical learning-theoretic measures can sufficiently explain generalization for graph neural networks in the transductive setting. In particular, we provide a rigorous analysis of the performance of neural networks in the context of transductive inference, specifically by analysing the generalisation properties of graph convolutional networks for the problem of node classification. While VC-dimension does result in trivial generalisation error bounds in this setting as well, we show that transductive Rademacher complexity can explain the generalisation properties of graph convolutional networks for stochastic block models. We further use the generalisation error bounds based on transductive Rademacher complexity to demonstrate the role of graph convolutions and network architectures in achieving smaller generalisation error and provide insights into when the graph structure can help in learning. The findings of this paper could re-new the interest in studying generalisation in neural networks in terms of learning-theoretic measures, albeit in specific problems.

## Skeleton-Graph: Long-Term 3D Motion Prediction From 2D Observations Using Deep Spatio-Temporal Graph CNNs

### TL;DR

Divergence free long-term 3D motion estimation from 2D observations using graph CNNs and tailored loss functions.

### Abstract

Estimating 3D human motion is of a high interest. This estimation is often done from 2D observed skeleton poses that are easier to capture. The 3D human pose is vital for several applications which require a precise description of the 3D skeleton including augmented reality and virtual reality. Recently, a new problem was introduced to predict the 3D human poses sequences from an observed 2D sequences. We propose Skeleton-Graph, a deep spatio-temporal graph CNN model that predicts the 3D skeleton poses in a single pass from 2D observations. Unlike prior works, Skeleton-Graph focuses on modeling the interaction between the skeleton joints exploiting their spatial configuration by learning a proper graph adjacency kernel function. By the design, Skeleton-Graph estimates the future 3D poses without divergence on the long-term unlike prior works. We also introduce a novel training objective called skeleton consistency loss. The skeleton consistency loss forces the model to predict realistic 3D skeleton poses by introducing constrains on the angles and lengths between the 3D joints. Our results show an FDE improvement of at least 27% and an ADE of 4% on both the GTA-IM and PROX datasets respectively in comparison with prior works. Also, we are 88% and 93% less divergence on the long-term motion prediction in comparison with prior works on both GTA-IM and PROX datasets.

## Topology-Imbalance Learning for Semi-Supervised Node Classification

### TL;DR

This paper propose a graph-specific imbalance issue: topology imbalance and the relative solution.

### Abstract

The class imbalance problem, as an important issue in learning node representations, has drawn increasing attention from the community. Although the imbalance considered by existing studies roots from the unequal quantity of labeled examples in different classes (quantity-imbalance), we argue that graph data expose a unique source of imbalance from the asymmetric topological properties of the labeled nodes, i.e., labeled nodes are not equal in terms of their structural role in the graph (topology imbalance). We propose to unify the two pervasive yet challenging imbalance problems by considering node influence distribution with the Label Propagation algorithm: labeled nodes exert influence of their labeled classes through the graph structure and the imbalance essentially distorts the spread of the influence, resulting in deviation of the model classification boundaries from the true class boundaries. In light of this analysis, we further propose to locate the topological position of the labeled nodes based on influence conflicts from different classes across the graph and devise a model-agnostic method ReNode to re-weight the influence of labeled nodes to address the topology-imbalance problem. Systematic experiments demonstrate the proposed method effectively promotes the performance of various graph neural networks (GNNs) under both the topology- and quantity-imbalance scenarios. Further analysis unveils varied sensitivity of different GNNs to topology imbalance, which may serve as a new perspective in evaluating GNN architectures.


## Robust Graph Learning Under Wasserstein Uncertainty

### TL;DR

None

### Abstract

A robust framework for graph learning task under data uncertainty is proposed in this paper. The uncertainty set of data distribution is characteized via Wasserstein distance without any prior assumption. Under such uncertainty set, distributionally robust optimization (DRO) is then exploited to learn a graph reliabily. This robust graph learning problem can be reformulated to a convex programing, and robustness is obtained by a regularizer whose weight equals to radius of the uncertainty set, which can be interpreted as the level of robustness. Furthermore, a projected gradient descendent (PGD) method with a linear operator is delivered to solve the convex optimization problem. Numerical experiments are carried out to verify the efficiency of our proposed framework.

## A Unified Graph Representation Learning Framework for Autonomous Programming of Heterogeneous Computing Systems

### TL;DR

We propose a unified end-to-end programmable graph representation learning (PGL) framework capable of mining the complexity of high-level programs and predict which code segments run best on a specific core on heterogeneous hardware platforms.

### Abstract

To enable heterogeneous computing systems with autonomous programming and optimization capabilities, we propose a unified end-to-end programmable graph representation learning (PGL) framework capable of mining the complexity of high-level programs down to the universal intermediate representation, extract the specific computational patterns and predict which code segments run best on a specific core on heterogeneous hardware platforms. By efficiently representing high-level programs as weighted graphs of universal intermediate representation (IR) instructions working with virtual registers of a universal memory model, the proposed framework can efficiently analyze the structural information flow of software programs and determine their parallelization. Furthermore, the proposed framework extracts multi-fractal node features, utilizes graph autoencoders to learn how to partition the graph into computational kernels (tasks), and exploits graph neural networks (GNN) to predict the correct assignment to a processor type. In the evaluation, we validate the PGL framework and demonstrate a maximum speedup of 6.22x when compared to the thread-based execution and 1.91x higher compared to the state-of-the-art technique.

## AutoGEL: An Automated Graph Neural Network with Explicit Link Information

### TL;DR

None

### Abstract

Recently, Graph Neural Networks (GNNs) have gained popularity in a variety of real-world scenarios. Despite the great success, the architecture design of GNNs heavily relies on manual labor. Thus, automated graph neural network (AutoGNN) has attracted interest and attention from the research community, which makes significant performance improvements in recent years. However, existing AutoGNN works mainly adopt an implicit way to model and leverage the link information in the graphs, which is not well regularized to the link prediction task on graphs, and limits the performance of AutoGNN for other graph tasks. In this paper, we present a novel AutoGNN work that explicitly models the link information, abbreviated to AutoGEL. In such a way, AutoGEL can handle the link prediction task and improve the performance of AutoGNNs on the node classification and graph classification task. Moreover, AutoGEL proposes a novel search space containing various design dimensions at both intra-layer and inter-layer designs and adopts a more robust differentiable search algorithm to further improve efficiency and effectiveness. Experimental results on benchmark data sets demonstrate the superiority of AutoGEL on several tasks.

## Online Adversarial Distillation for Graph Neural Networks

### TL;DR

We propose an online adversarial knowledge distillation framework for graph neural networks.

### Abstract

Knowledge distillation has been a popular technique to improve the model generalization ability on convolutional neural networks in the last several years. However, its effect on graph neural networks is less than satisfactory since the graph topology and node attributes tend to change in a dynamic way and a fixed teacher model is thus insufficient in guiding student training. In this paper, we tackle this challenge by simultaneously training a group of graph neural networks in an online distillation fashion, where the group knowledge plays a similar role as the pretrained teacher to improve the student performance. More specifically, two types of knowledge are transferred among the students to enhance each other: local knowledge reflecting information in the graph topology and node attributes, and global knowledge reflecting the prediction over classes. We transfer the global knowledge with KL-divergence as the vanilla knowledge distillation does, while exploiting the complicated structure of the local knowledge with adversarial cyclic learning. Extensive experiments verified the effectiveness of our proposed online adversarial distillation approach.

## Graph Intervention Networks for Causal Effect Estimation

### TL;DR

This paper addresses causal effect estimation when interventions are graphs by generalizing the Robinson decomposition and presenting a meta-learner algorithm.

### Abstract

We address the estimation of conditional average treatment effects (CATEs) when treatments are graph-structured (e.g., molecular graphs of drugs). Given a weak condition on the effect, we propose a plug-in estimator that decomposes CATE estimation into separate, simpler optimization problems. Our estimator (a) isolates the causal estimands (reducing regularization bias), and (b) allows one to plug in arbitrary models for learning. In experiments with small-world and molecular graphs, we show that our approach outperforms prior approaches and is robust to varying selection biases. 

## Evaluating Modules in Graph Contrastive Learning

### TL;DR

None

### Abstract

The recent emergence of contrastive learning approaches facilitates the research on graph representation learning (GRL), introducing graph contrastive learning (GCL) into the literature. These methods contrast semantically similar and dissimilar sample pairs to encode the semantics into node or graph embeddings. However, most existing works only performed model-level evaluation, and did not explore the combination space of modules for more comprehensive and systematic studies. For effective module-level evaluation, we propose a framework that decomposes GCL models into four modules: (1) a sampler to generate anchor, positive and negative data samples (nodes or graphs); (2) an encoder and a readout function to get sample embeddings; (3) a discriminator to score each sample pair (anchor-positive and anchor-negative); and (4) an estimator to define the loss function. Based on this framework, we conduct controlled experiments over a wide range of architectural designs and hyperparameter settings on node and graph classification tasks. 
Specifically, we manage to quantify the impact of a single module, investigate the interaction between modules, and compare the overall performance with current model architectures. Our key findings include a set of module-level guidelines for GCL, e.g., simple samplers from LINE and DeepWalk are strong and robust; an MLP encoder associated with Sum readout could achieve competitive performance on graph classification. Finally, we release our implementations and results as OpenGCL, a modularized toolkit that allows convenient reproduction, standard model and module evaluation, and easy extension.

## Bag of Tricks for Training Deeper Graph Neural Networks: A Comprehensive Benchmark Study

### TL;DR

We present the first fair and reproducible benchmark dedicated to assessing the "tricks" of training deep GNNs.

### Abstract

Training deep graph neural networks (GNNs) is notoriously hard. Besides the standard plights in training deep architectures such as vanishing gradients and overfitting, the training of deep GNNs also uniquely suffers from over-smoothing, information squashing, and so on, which limits their potential power on large-scale graphs. Although numerous efforts are proposed to address these limitations, such as various forms of skip connections, graph normalization, and random dropping, it is difficult to disentangle the advantages brought by a deep GNN architecture from those "tricks" necessary to train such an architecture. Moreover, the lack of a standardized benchmark with fair and consistent experimental settings poses an almost insurmountable obstacle to gauging the effectiveness of new mechanisms. In view of those, we present the first fair and reproducible benchmark dedicated to assessing the "tricks" of training deep GNNs. We categorize existing approaches, investigate their hyperparameter sensitivity, and unify the basic configuration. Comprehensive evaluations are then conducted on tens of representative graph datasets including the recent large-scale Open Graph Benchmark (OGB), with diverse deep GNN backbones. Based on synergistic studies, we discover the combo of superior training tricks, that lead us to attain the new state-of-the-art results for deep GCNs, across multiple representative graph datasets. We demonstrate that an organic combo of initial connection, identity mapping, group and batch normalization has the most ideal performance on large datasets. Experiments also reveal a number of “surprises" when combining or scaling up some of the tricks. 

## DropGNN: Random Dropouts Increase the Expressiveness of Graph Neural Networks

### TL;DR

We devise a new GNN variant (DropGNN) with larger expressive power in both theory and practice

### Abstract

This paper studies Dropout Graph Neural Networks (DropGNNs), a new approach that aims to overcome the limitations of standard GNN frameworks. In DropGNNs, we execute multiple runs of a GNN on the input graph, with some of the nodes randomly and independently dropped in each of these runs. Then, we combine the results of these runs to obtain the final result. We prove that DropGNNs can distinguish various graph neighborhoods that cannot be separated by message passing GNNs. We derive theoretical bounds for the number of runs required to ensure a reliable distribution of dropouts, and we prove several properties regarding the expressive capabilities and limits of DropGNNs. We experimentally validate our theoretical findings on expressiveness. Furthermore, we show that DropGNNs perform competitively on established GNN benchmarks.

## Causal Bandits with Unknown Graph Structure

### TL;DR

None

### Abstract

In causal bandit problems the action set consists of interventions on variables of a causal graph. Several researchers have recently studied such bandit problems and pointed out their practical applications. However, all existing works rely on a restrictive and impractical assumption that the learner is given full knowledge of the causal graph structure upfront. In this paper, we develop novel causal bandit algorithms without knowing the causal graph. Our algorithms work well for causal trees, causal forests and a general class of causal graphs. The regret guarantees of our algorithms greatly improve upon those of  standard multi-armed bandit (MAB) algorithms under mild conditions. Lastly, we prove our mild conditions are necessary: without them one cannot do better than standard MAB bandit algorithms.

## Overlapping Graph Clustering with Quality Guarantees

### TL;DR

Novel mathematical formulations and practical algorithms for overlapping graph clustering

### Abstract

Detecting communities in real-world networks and clustering similarity graphs into meaningful subsets of objects are  major data mining tasks with a wide range of applications in graph mining, collaborative filtering, and bioinformatics. In many of such applications, overwhelming empirical evidence suggests that communities and clusters are naturally overlapping, causing  disjoint community-detection methods to lose valuable information about the dataset structure. 
In this work, we introduce a framework for overlapping community detection based on two novel clustering objectives, the overlapping ratio-cut} and the related hybrid ratio-cut, which naturally extend the well-studied notion of conductance to overlapping clusters.
Crucially, we show that our formulation admits simple worst-case approximation algorithms that scale to large-scale real-world networks. Our main algorithmic contribution is a novel cut-improvement primitive that performs a small number of $s$-$t$ maximum flow computations over the instance graph to detect sparse overlapping partitions near an input partition.
We test the performance and efficiency of our proposed algorithm on a wide variety of real-world datasets against various other competitive baselines.

## Motif Prediction with Graph Neural Networks

### TL;DR

None

### Abstract

Link prediction is one of the central problems in graph mining. However, recent
studies highlight the importance of higher-order network analysis, where complex
structures called motifs are the first-class citizens. We first show that existing link
prediction schemes fail to effectively predict motifs. To alleviate this, we establish
a general motif prediction problem and we propose several heuristics that assess the
chances for a specified motif to appear. To make the scores realistic, our heuristics
consider - among others - correlations between links, i.e., the potential impact
of some arriving links on the appearance of other links in a given motif. Finally,
for highest accuracy, we develop a graph neural network (GNN) architecture for
motif prediction. Our architecture offers vertex features and sampling schemes
that capture the rich structural properties of motifs. While our heuristics are fast
and do not need any training, GNNs ensure highest accuracy of predicting motifs,
both for dense (e.g., k-cliques) and for sparse ones (e.g., k-stars). We consistently
outperform the best available competitor by more than 10% on average and up
to 32% in area under the curve. Importantly, the advantages of our approach over
schemes based on uncorrelated link prediction increase with the increasing motif
size and complexity. We also successfully apply our architecture for predicting
more arbitrary clusters and communities, illustrating its potential for graph mining
beyond motif analysis.

## Overlapping Spaces for Compact Graph Representations

### TL;DR

An approach to combine metric distances and similarity measures to achieve better vector representations of structured data.

### Abstract

Various non-trivial spaces are becoming popular for embedding structured data such as graphs, texts, or images. Following spherical and hyperbolic spaces, more general product spaces have been proposed. However, searching for the best configuration of product space is a resource-intensive procedure, which reduces the practical applicability of the idea. We generalize the concept of a product space and introduce an overlapping space that do not have the configuration search problem. The main idea is to allow subsets of coordinates to be shared between spaces of different types (Euclidean, hyperbolic, spherical). As a result, parameter optimization automatically learns the optimal configuration. Additionally, overlapping spaces allow for more compact representations since their geometry is more complex. Our experiments confirm that overlapping spaces outperform the competitors in graph embedding tasks. Here, we consider both distortion setup, where the aim is to preserve distances, and ranking setup, where the relative order should be preserved. The proposed method effectively solves the problem and outperforms the competitors in both settings. We also perform an empirical analysis in a realistic information retrieval task, where we compare all spaces by incorporating them into DSSM. In this case, the proposed overlapping space consistently achieves nearly optimal results without any configuration tuning. This allows for reducing training time, which can be significant in large-scale applications.

## Graph Neural Networks with Adaptive Residual

### TL;DR

A novel graph neural network model with strong resilience to abnormal node featutures

### Abstract

Graph neural networks (GNNs) have shown the power in graph representation learning for numerous tasks. In this work, we discover an interesting phenomenon that although residual connections in the message passing of GNNs help boost the performance, they immensely amplify GNNs' vulnerability against abnormal node features. This is undesirable because in real-world applications, node features in graphs could often be abnormal such as being naturally noisy or adversarially manipulated. We analyze possible reasons to understand this phenomenon and aim to design GNNs with stronger resilience to abnormal features. Our understandings motivate us to propose and derive a simple, principled, and interpretable message passing scheme, leading to a novel GNN with \underline{A}dapt\underline{i}ve \underline{r}esidual, AirGNN. Extensive experiments under various abnormal feature scenarios demonstrate the effectiveness of the proposed algorithm. 

## Rethinking Graph Transformers with Spectral Attention

### TL;DR

The first fully-connected Transformer model to perform well on graph-structured data.

### Abstract

In recent years, the Transformer architecture has proven to be very successful in sequence processing, but its application to other data structures, such as graphs, has remained limited due to the difficulty of properly defining positions. 
Here, we present the \textit{Spectral Attention Network} (SAN), which uses a learned positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position of each node in a given graph.
This LPE is then added to the node features of the graph and passed to a fully-connected Transformer.
By leveraging the full spectrum of the Laplacian, our model is theoretically powerful in distinguishing graphs, and can better detect similar sub-structures from their resonance.
Further, by fully connecting the graph, the Transformer does not suffer from over-squashing, an information bottleneck of most GNNs, and enables better modeling of physical phenomenons such as heat transfer and electric interaction.
When tested empirically on a set of 4 standard datasets, our model performs on par or better than state-of-the-art GNNs, and outperforms any attention-based model by a wide margin, becoming the first fully-connected architecture to perform well on graph benchmarks.

## Identifiability of AMP chain graph models

### TL;DR

None

### Abstract

We address the problem of finding identifiability conditions for Andersson-Madigan-Perlman (AMP) chain graph models, which are a common generalization of linear structural equation models and Gaussian graphical models. AMP models are described by DAGs on chain components which themselves are undirected graphs. 

For a known chain component decomposition, we show that the DAG on the chain components is identifiable if the determinants of the residual covariance matrices of the chain components are monotone non-decreasing in topological order.  This condition extends the equal variance identifiability criterion for Bayes nets, and it can be generalized from determinants to any super-additive function on positive semidefinite matrices. When the undirected structure is unknown, we describe conditions that allow recovery of the full structure using a polynomial time algorithm. We also conduct experiments comparing our algorithm's performance against existing baselines.

## Automorphic Equivalence-aware Graph Neural Network

### TL;DR

We propose a novel GNN model that levearges the concept of automorphic equivlance to provably improve its expressiveness.

### Abstract

Distinguishing the structural equivalence of nodes in a graph plays an essential role in many scientific domains, e.g., computational biologist and social network analysis. However, existing graph neural networks (GNNs) fail to capture such an important property. To make GNN aware of structural equivalence, we first introduce a localized variant of the classic concept --- automorphic equivalence~(AE). Then, we design a novel variant of GNN, i.e., GRAPE, that uses learnable AE-aware aggregators to explicitly differentiate the structural equivalences of each node's neighbors with the aids of various subgraph templates. To alleviate the barrier of subgraph template design, we also propose to automatically search the design space with an efficient genetic algorithm. Moreover, we theoretically prove that GRAPE is expressive in terms of generating distinct representations for nodes with different AE features, which fills in a fundamental gap of existing GNN variants. Finally, we empirically validate our model on eight real-world graph data, including social network, e-commerce co-purchase network, and citation network, and show that it consistently outperforms strong baselines.

## TopicNet: Semantic Graph-Guided Topic Discovery

### TL;DR

 We propose a novel knowledge-based hierarchical topic model,  which can inject prior structural knowledge as inductive bias to influence the topic discovery.

### Abstract

Existing deep hierarchical topic models are able to extract semantically meaningful topics from a text corpus  in an unsupervised manner and automatically organize them into a topic hierarchy.  However, it is unclear how to incorporate prior belief such as knowledge graph to guide the learning of the topic hierarchy. To address this issue, we introduce TopicNet as a deep hierarchical topic model that can inject prior structural knowledge as inductive bias to influence the learning. TopicNet represents each topic as a Gaussian-distributed embedding vector, projects the topics of all layers into a shared embedding space, and explores both the symmetric and asymmetric similarities between Gaussian embedding vectors to incorporate prior semantic hierarchies. With a variational auto-encoding inference network,  the model parameters are optimized by minimizing the evidence lower bound and supervised loss via stochastic gradient descent. Experiments on widely used benchmark show that TopicNet outperforms related deep topic models on discovering deeper interpretable topics and mining better document representations. 

## Implicit SVD for Graph Representation Learning

### TL;DR

Framework for representing matrices symbolically and efficiently computing their SVD, training graph networks orders-of-magnitude than SOTA, yet shows competitive empirical test performance

### Abstract

Recent improvements in the performance of state-of-the-art (SOTA) methods for Graph Representational Learning (GRL) have come at the cost of significant computational resource requirements. In this paper, we make GRL more computationally tractable for those with modest hardware. We design a framework that computes Singular Value Decomposition (SVD) of *implicitly* defined matrices. We apply our framework to several GRL tasks. For each task, We derive first-order approximation of a SOTA model, where we design (expensive-to-store) matrix $\mathbf{M}$ and train the model, in closed-form, via SVD of $\mathbf{M}$, without calculating entries of $\mathbf{M}$. By converging to a unique point in one step, and without calculating gradients, our models show competitive empirical test performance, over various graphs such as article citation and biological interaction networks. Moreover,  SVD can initialize a deeper model, which can then be fine-tuned within only a few epochs.  Overall, our algorithm trains hundreds of times faster than state-of-the-art methods, while competing on test empirical performance. We open-source our implementation.

## Learning Graph Models for Retrosynthesis Prediction

### TL;DR

None

### Abstract

Retrosynthesis prediction is a fundamental problem in organic synthesis, where the task is to identify precursor molecules that can be used to synthesize a target molecule. A key consideration in building neural models for this task is aligning model design with strategies adopted by chemists. Building on this viewpoint, this paper introduces a graph-based approach that capitalizes on the idea that the graph topology of precursor molecules is largely unaltered during a chemical reaction. The model first predicts the set of graph edits transforming the target into incomplete molecules called synthons. Next, the model learns to expand synthons into complete molecules by attaching relevant leaving groups. This decomposition simplifies the architecture, making its predictions more interpretable, and also amenable to manual correction. Our model achieves a top-1 accuracy of 53.7%, outperforming previous template-free and semi-template-based methods.

## Difference Residual in Graph Neural Networks

### TL;DR

None

### Abstract

Following feed-forward structure of classic deep neural networks (NNs), Graph Neural Networks (GNNs) often possess serious over-smoothing issue by stacking multiple layers. To overcome this issue, residual connections, which are designed for alleviating vanishing gradient problem in NNs, are adopted in GNNs to incorporate local node information. However, these simple residual connections only perform well on networks with homophily, but are ineffective on networks with heterophily, since the roles of both convolutional operations and residual connections in GNNs are significantly different from those in classic NNs. By considering the specific smoothing characteristic of graph convolutional operation, deep layers in GNNs are excepted to focus on the data, which can't be properly handled in shallow layers. To this end, a novel Difference Residual Connections (DRC), which feed the difference of the output and input of previous layer as the input of next layer, is proposed.  Essentially, Difference Residual Connections is equivalent to  inserting layers with opposite effect (e.g., sharpening) into the network to prevent the excessive effect (e.g., over-smoothing issue) induced by too many layers with similar role (e.g., smoothing) in GNNs. 
From the perspective of optimization, DRC is the gradient descent method to minimize an objective function with both smoothing and sharpening terms. The analytic solution to this objective function is determined by both graph topology and node attributes, which theoretically proves that DRC can prevent over-smoothing issue. Extensive experiments demonstrate the superiority of DRC on real networks with homophily and heterophily.

## Learning Guarantees for Graph Convolutional Networks

### TL;DR

We prove the Graph Convolutional Networks can learn Stochastic Block Models in a semi-supervised setting.

### Abstract

An abundance of neural network models and algorithms for diverse tasks on graphs have been developed in the past five years. However, very few provable guarantees have been available for the performance of graph neural network models. This state of affairs is in contrast with the steady progress on the theoretical underpinnings of traditional dense and convolutional neural networks. In this paper we present the first provable guarantees for one of the best-studied families of graph neural network models, Graph Convolutional Networks (GCNs), for semi-supervised community detection tasks. We show that with high probability over the initialization and training data, a GCN will efficiently learn to detect communities on graphs drawn from a stochastic block model. Our proof relies on a fine-grained analysis of the training dynamics in order to overcome the complexity of a non-convex optimization landscape with many poorly-performing local minima.

## Graph Convolution RPCA with Adaptive Neighbors

### TL;DR

None

### Abstract

Principal component analysis (PCA) enjoys a large popularity in dimensionality reduction. For decades, a series of PCA methods are proposed to enhance the robustness of PCA. Nevertheless, the existing robust PCA methods usually obtain the low-rank representations by linear subspace projection, which is not capable of learning complex features. Thus, it is significant to improve the representation capacity of the PCA method. To address this problem, we elaborate a novel graph convolution robust PCA method namely GRPCA, which constructs a sparse graph according to the local connectivity structure of samples, where the manifold structure of data can be captured and represented. Moreover, the adaptive loss and the Schatten $p$-norm are incorporated in the loss function of  proposed model, which strongly enhances the robustness of the PCA model. Eventually, extensive experiments conducted on several real-world datasets illustrated the effectiveness and superiority of our model.

## Interferometric Graph Transform for Community Labeling

### TL;DR

We numerically and theoretically show that the Interferometric Graph Transform is an idea representation for community labeling

### Abstract

We present a new approach for learning unsupervised node representations in community graphs. We significantly extend the Interferometric Graph Transform (IGT) to community labeling: this non-linear operator iteratively extracts features that take advantage of the graph topology through demodulation operations. An unsupervised feature extraction step cascades modulus non-linearity with linear operators that aim at building relevant invariants for community labeling. Via a simplified model, we show that the IGT concentrates around the Expected-IGT: those two representations are related through some ergodicity properties.  Experiments on community labeling tasks show that this unsupervised representation achieves performances at the level of the state of the art on the standard and challenging datasets Cora, Citeseer, Pubmed and WikiCS.

## How Attentive are Graph Attention Networks?

### TL;DR

We identify that Graph Attention Networks (GAT) compute a very weak form of attention. We show its empirical implications and propose a fix.

### Abstract

Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query.
However, in this paper we show that GATs can only compute a restricted kind of attention where the ranking of attended nodes is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention.
Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. 
To remove this limitation, we introduce a simple fix by modifying the order of operations
and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 11 OGB and other benchmarks while we match their parametric costs. 

## Interactive Graph Reasoning for Semantic Segmentation

### TL;DR

In this paper, we propose an interactive graph reasoning module to capture long-range dependencies and model global context for semantic segmentation.  

### Abstract

Recently significant progresses have been made in semantic segmentation by capturing long-range dependencies with dilated convolution, pyramid pooling and/or self-attention mechanism. Rather than adopting a single block on the top of convolutional network for global context modelling, we propose a new interactive graph reasoning (IGR) module, to perform global reasoning by projecting multiple feature maps from the coordinate space to various graphs in the latent space. Then the graphs are fused by graph mapping operations and distributed back to the coordinate space. The IGR module can be easily inserted into an existing network to adaptively model global context and efficiently propagate information. Extensive experiments show that such IGR modules can significantly boost the performance of the semantic segmentation frameworks and outperform other context modelling methods. State-of-the-art performances have been achieved on the Cityscapes, PASCAL VOC 2012 and PASCAL Context benchmark datasets.

## Auxiliary learning induced graph convolutional networks

### TL;DR

None

### Abstract

In this article, we propose a novel auxiliary learning induced graph convolutional network in a multi-task fashion. Specifically, both the link prediction and pseudo label generation are used as two auxiliary tasks to complement the primary task of node classification. Those two auxiliary tasks are jointly trained with the primary node classification task via the graph meta-learning strategy. The experimental results demonstrate that the proposed method consistently and significantly outperforms the existing methods and achieves state-of-the-art node classification results on the benchmark citation network datasets. 

## Second-Order Continuous Graph Neural Networks

### TL;DR

We propose a second-order continuous graph neural network.

### Abstract

Graph neural networks (GNN) are powerful tools in learning graph embeddings. Increasing the depth of GNNs is believed to be the pivot to boost their representation abilities. Inspired by the recent success of a family of models, neural ordinary differential equations (ODEs), there is some research interest in continuous graph neural networks. Existing continuous GNN frameworks learn the first-order dynamics. However, there are some remaining problems \textit{w.r.t.} the prevailing first-order frameworks. Firstly, augmenting node features is an essential however heuristic step for the numerical stability of current frameworks; secondly, first-order methods characterize a diffusion process, in which the over-smoothing effect \textit{w.r.t.} node representations are intrinsic; and thirdly, there are some difficulties to integrate the topology of graphs into the ODEs. In this paper, we propose a framework employing second-order graph neural networks, which usually learns a less stiff transformation compared to the first-order counterpart. Our method can also be viewed as a coupled first-order model, which is easy to implement and has better interpretability. We construct an analog between continuous GNNs and some famous partial differential equations and discuss some properties of the first and second-order models. Extensive experiments demonstrate the effectiveness of our proposed method, and the results outperform related baselines.

## Robustification of Online Graph Exploration Methods

### TL;DR

None

### Abstract

Exploring unknown environments is a fundamental task in many domains, e.g., robot navigation, network security, and internet search. We initiate the study of a learning-augmented variant of the classical, notoriously hard online graph exploration problem by adding access to machine-learned predictions. We propose an algorithm that naturally integrates predictions into the well-known Nearest Neighbor (NN) algorithm and significantly outperforms any known online algorithm if the prediction is of high accuracy while maintaining good guarantees when the prediction is of poor quality. We provide theoretical worst-case bounds that gracefully degrade with the prediction error, and we complement them by computational experiments that confirm our results. Further, we extend our concept to a general framework to robustify algorithms. By interpolating carefully between a given algorithm and NN, we prove new performance bounds that leverage the individual good performance on particular inputs while establishing robustness to arbitrary inputs.

## A Graph-Based Dataset Similarity Metric

### TL;DR

None

### Abstract

A number of metrics are available for computing similarity and distance between a pair of samples from same or different datasets. However, finding a notion of how similar two datasets are in distribution remains an open question. This paper presents a novel framework to evaluate the similarity between datasets based on a graph structure and statistical hypothesis testing. We also provide theoretical guarantees to explain the behavior of our proposed similarity metric. The proposed framework provides great flexibility for modeling similarity between datasets as it is agnostic to data type, model and task. Application to a specific data type, model, or task only requires an appropriate sample-level similarity metric provided by domain experts. Through various experiments, we show that our framework is sensitive to perturbations in data, and is able to tell datasets apart based on their semantic similarity. Our results also suggest a strong correlation between our metric and domain adaptation hardness across different datasets. Further improvements to the framework are also discussed and left as future directions.

## Chart-Based Rotation Synchronization

### TL;DR

None

### Abstract

This paper introduces a novel rotation synchronization approach that takes noisy rotations computed between pairs of objects specified by a sparse graph as input and outputs improved rotations between all objects. Most existing approaches have focused on solving optimization problems on the entire input graph. The success of these techniques typically builds on the assumption that the data matrix that stores pairwise rotations in blocks has a salient spectral gap. This paper identifies that such a spectral gap may not always exist when considering the entire input graph. In contrast, we propose to perform rotation synchronization on suitable sub-graphs. Specifically, we introduce a scoring function for sub-graphs by analyzing exact and robust recovery conditions of a Laplacian rotation synchronization approach. We then show two efficient methods for optimizing sub-graphs. The first approach aims for outlier removal by computing a collection of sub-graphs covering edges of the input graph. The second approach computes sub-graphs that cover all object pairs to propagate relative rotations. Experimental results show that our method is superior to state-of-the-art rotation synchronization approaches across diverse benchmark datasets.

## On the Performance of the MLE in the Bradley-Terry Model under General Graph Topologies

### TL;DR

We investigate the performance of the regularized MLE of the Bradley-Terry model parameters for a general topology of the pairwise comparison graph and focus on the l-infinity estimation loss.

### Abstract

The Bradley-Terry (BT) model is a popular parametric approach to estimating global rankings from pairwise comparison data. In this paper we investigate the properties and performance of the regularized maximum likelihood estimator (MLE) of the BT model parameters for a general topology of the pairwise comparison graph and focus on the $\ell_{\infty}$ estimation loss, which is ideally suited to global and partial ranking tasks. We derive an upper bound for the risk of the regularized MLE for the BT model parameters under fairly general conditions. Our upper bound captures the explicit dependence of the $\ell_{\infty}$ estimation loss on the algebraic connectivity of the comparison graph, the maximal performance gap between items, and the sample complexity. We then illustrate how our bound can be used to quantify the hardness of the global ranking under a broad class of pairwise comparison graph topologies. Given that our bound is derived under relatively weak assumptions, we discuss in detail how it relates to known sharp bounds obtained under stronger assumptions and specific graph topologies, and demonstrate that our results in some cases recover existing bounds, and perform well in general.

## Robust Counterfactual Explanations on Graph Neural Networks

### TL;DR

We propose a novel method to produce robust counterfactual explanations for the predictions of Graph Neural Networks.

### Abstract

Massive deployment of Graph Neural Networks (GNNs) in high-stake applications generates a strong demand for explanations that are robust to noise and align well with human intuition. Most existing methods generate explanations by identifying a subgraph of an input graph that has a strong correlation with the prediction. These explanations are not robust to noise because independently optimizing the correlation for a single input can easily overfit noise. Moreover, they do not align well with human intuition because removing an identified subgraph from an input graph does not necessarily change the prediction result. In this paper, we propose a novel method to generate robust counterfactual explanations on GNNs by explicitly modelling the common decision logic of GNNs on similar input graphs. Our explanations are naturally robust to noise because they are produced from the common decision boundaries of a GNN that govern the predictions of many similar input graphs. The explanations also align well with human intuition because removing the set of edges identified by an explanation from the input graph changes the prediction significantly. Exhaustive experiments on many public datasets demonstrate the superior performance of our method. 

## Subgraph Federated Learning with Missing Neighbor Generation

### TL;DR

We study a novel yet realistic setting of node classification in a distributed subgraph system, and propose novel federated learning frameworks to address the unique challenges in this setting.

### Abstract

Graphs have been widely used in data mining and machine learning due to their unique representation of real-world objects and their interactions. As graphs are getting bigger and bigger nowadays, it is common to see their subgraphs separately collected and stored in multiple local systems. Therefore, it is natural to consider the subgraph federated learning setting, where those local systems, each holding a small subgraph that may be biased from the distribution of the whole graph, aim to collaboratively train a powerful and generalizable graph mining model without directly sharing their graph data. In this work, towards the novel yet realistic setting of subgraph federated learning, we propose two major techniques: (1) FedSage, which trains a GraphSage model based on FedAvg to integrate node features, link structures, and task labels on multiple local subgraphs; (2) FedSage+, which trains a missing neighbor generator along FedSage to deal with missing links across local subgraphs. Empirical results on four real-world graph datasets with synthesized subgraph federated learning settings demonstrate the effectiveness and efficiency of our proposed techniques. At the same time, consistent theoretical implications are made towards their generalization ability on the global graphs.

## Digraph Contrastive Learning

### TL;DR

In this paper, we present the first contrastive learning framework for learning digraph representation.

### Abstract

Graph Contrastive Learning (GCL) has emerged to learn generalizable representations from contrastive views. However, it is still in its infancy with two concerns: 1) changing the graph structure through data augmentation to generate contrastive views may mislead the message passing scheme, as such graph changing action deprives the intrinsic graph structural information, especially the directional structure in digraphs; 2) since GCL usually generates a limited number of contrastive views, it does not take full advantage of the contrastive information provided by data augmentation, resulting in incomplete structure information for models learning. In this paper, we design a digraph data augmentation method called Laplacian perturbation and theoretically analyze how it provides contrastive information without changing the digraph structure. Moreover, we present the multi-view digraph contrastive learning framework, which learns from all possible contrastive views generated by Laplacian perturbation. Then we train it using multi-task curriculum learning to progressively learn from multiple easy-to-hard contrastive views. We empirically show that our model can retain more structural features of digraphs than other GCL models because of its ability to provide complete contrastive information. Experiments on various benchmarks reveal our dominance over the state-of-the-art approaches.

## Topic Modeling Revisited: A Document Graph-based Neural Network Perspective

### TL;DR

None

### Abstract

Traditional topic modeling usually has the bag-of-words assumption, where each word is required to be conditionally independent in the same document. As a result, the semantic dependency among words is ignored in the process of the generative story and the topic formulation. However, the semantic dependency among words, if properly measured, can be exploited for improving the semantic comprehension and model interpretability. To this end, in this paper, we transform each document into a directed graph with word dependency as edges between word nodes and develop a Graph Neural Topic Modeling (GNTM) method. Specifically, in GNTM, a well-defined probabilistic generative story is designed to model both the graph structure and word sets with multinomial distributions on the vocabulary and word dependency edge set as the topics. Meanwhile, an Autoencoder Variational Inference (AVI) approach is proposed to learn our model with graph neural networks to encode the document graphs. Moreover, we theoretically show that Latent Dirichlet Allocation (LDA) can be derived from GNTM as a special case with similar objective functions. Finally, extensive experiments on four benchmark datasets have clearly demonstrated the effectiveness and interpretability of GNTM compared with state-of-the-art baselines.

## VSGM - Enhance robot task understanding ability through visual semantic graph

### TL;DR

None

### Abstract

In recent years, developing AI for robotics has raised much attention. The interaction of vision and language of robots is particularly difficult. We consider that giving robots an understanding of visual semantics and language semantics will improve inference ability. In this paper, we propose a novel method-VSGM (Visual Semantic Graph Memory), which uses the semantic graph to obtain better visual image features, improve the robot's visual understanding ability. By providing prior knowledge of the robot and detecting the objects in the image, it predicts the correlation between the attributes of the object and the objects and converts them into a graph-based representation; and mapping the object in the image to be a top-down egocentric map. Finally, the important object features of the current task are extracted by Graph Neural Networks. The method proposed in this paper is verified in the ALFRED (Action Learning From Realistic Environments and Directives) dataset. In this dataset, the robot needs to perform daily indoor household tasks following the required language instructions. After the model is added to the VSGM, the task success rate can be improved by 6~10 %.

## Graph Convolution Meets Causality:  An Intervention Approach to Node-Level Prediction

### TL;DR

We harness causal intervention approach to enhance GNN model for node-level prediction tasks and improve its generalization

### Abstract

While the representation power of graph neural networks (GNN) has been extensively studied, its generalization capability receives little attention. Like neural networks that incline to capture spurious correlations from features to labels, GNN is also at risk of relying on meaningless shortcut in training distribution and neglecting true causal relations behind data. In this paper, we introduce Graph Intervention Neural Networks (GINN) aiming to learn from P(y_v|do(G_v)) that intervenes the ego-networks of centered nodes and removes the confounding bias of context prior in order to tackle the causality between graph features and labels. To put the idea into practice, we propose variational context adjustment which amortizes the inference for graph contexts over different centered nodes, and further design an ego-centered convolution filter conditioned on ego-network features. As a by-product, we show that GINN possesses superior expressiveness for node-level prediction tasks. Experiments on multiple node classification benchmarks demonstrate that GINN with vanilla GCN as backbone 1) can achieve up to 41% improvement of accuracy compared to the counterpart without proposed components on real-world datasets with artificial confounding bias, and 2) significantly exceed recently proposed models on disassortative graphs.

## A Tensorized Spectral Attention Mechanism for Efficient Natural Language Processing

### TL;DR

None

### Abstract

The attention mechanism is at the core of state-of-the-art Natural Language Processing (NLP) models, owing to its ability to focus on the most contextually relevant part of a sequence. However, current attention models rely on "flat-view" matrix methods to process sequence of tokens embedded in vector spaces, resulting in exceedingly high parameter complexity for practical applications. To this end, we introduce a novel Tensorized Spectral Attention (TSA) mechanism, which leverages on the Graph Tensor Network (GTN) framework to efficiently process tensorized token embeddings via attention based spectral graph filters. By virtue of multi-linear algebra, such tensorized token embeddings are shown to effectively bypass the Curse of Dimensionality, reducing the parameter complexity of the attention mechanism from exponential to linear in the weight matrix dimensions. Furthermore, the graph formulation of the attention domain enables the processing of tensorized embeddings through spectral graph convolution filters, which further increases its expressive power. The benefits of the TSA are demonstrated through five benchmark NLP experiments, where the proposed mechanism is shown to achieve better or comparable results against traditional attention models, while incurring drastically lower parameter complexity.

## Semi-supervised Learning via Gaussian Process: InfinitelyWide Graph Convolutional Networks

### TL;DR

None

### Abstract

Graph convolutional neural networks (GCNs) has demonstrated promising success on graph-based semisupervised classification recently. Meanwhile, the deep neural network is shown to have an equivalence to Gaussian processes (GPs) by giving the hidden layers be infinitely wide. To leverage both the advantages of powerful representation from GCNs and Bayesian inferences via GPs, we investigate the property of infinitely wide layer on GCNs and propose a GP regression model via GCNs (GPGC) for semi-supervised learning on graph-structured data. In the process, we formulate the kernel matrix computation of GPGC in an iterative analytical form. Finally, we derive a conditional distribution for the labels of unobserved nodes based on the graph structure, labels for the observed nodes, and the feature matrix of all the nodes. We conduct extensive experiments to evaluate that GPGC can outperform vanilla GP and GCNs models by a clear margin. We also show that with the increasing of hidden layers, the performance of GCNs improves accordingly, which in turn verifies the effectiveness and necessity of deriving infinitely wide GCNs. Last but not least, GPGC provides an alternative explainability to the results from GCNs, which is often being blamed to be a black box. 

## Beyond the Homophily and Homogeneity Assumption: Relation-based Frequency Adaptive Graph Neural Networks

### TL;DR

In this paper, we propose a novel Relation-based Frequency Adaptive Graph Neural Network (RFA-GNN) to handle both homophily/heterophily and homogeneity/heterogeneity settings in a unified framework in an end-to-end manner.

### Abstract

Graph neural networks (GNNs) have been proven to be effective for various graph-related tasks. However, most existing GNNs are based on the assumption of homophily and cannot be directly generalized to heterophily settings, where connected nodes may have different features and class labels. Moreover, real-world graphs often arise from highly entangled latent factors, but existing GNNs tend to ignore this and simply denote the heterogeneous relations between nodes as binary-value homogeneous edges. In this paper, we propose a novel Relation-based Frequency Adaptive Graph Neural Network (RFA-GNN) to handle both heterophily and heterogeneity in a unified framework. RFA-GNN first decomposes an input graph into multiple relation graphs, each representing a latent relation between nodes. More importantly, we provide detailed theoretical analysis from the perspective of spectral signal processing, based on which we propose a relation-based frequency adaptive mechanism that adaptively pick signals of different frequencies in each corresponding relation space during the message passing process. Extensive experiments on synthetic and real-world datasets have shown that RFA-GNN yields truly encouraging results for both heterophily and heterogeneity settings.

## Edge Rewiring Goes Neural: Boosting Network Resilience via Policy Gradient

### TL;DR

We present an end-to-end neural network-based solver for boosting the resilience of a graph network which protects the entire system from natural disasters or targeted attacks.

### Abstract

Improving the resilience of an infrastructure system protects the entire system from natural disasters or targeted attacks with applications in various areas such as transportation networks and power supply networks. By modeling the system as a graph, the network resilience is optimized via rewiring edges of the graph. Traditional edge rewiring methods for maximizing network resilience suffer from two limitations: transduction and inefficiency. To overcome this, we develop ResiNet, an end-to-end framework for discovering resilient network topologies against malicious attacks. First, ResiNet inherits the inductive ability of graph neural networks (GNNs) and thus adapts to arbitrary graph size and arbitrary permutation of node ordering by leveraging an autoregressive parametric latent action space. Second, instead of repeatedly considering one-step resilience gain as is in traditional approaches, we cast the problem of maximizing network resilience as a sequential decision-making process and solve it via policy gradient. Armed with these characterizations, ResiNet is the first to generalize to unseen graphs efficiently without searching again from scratch. Experiments on both synthetic and real datasets demonstrate that ResiNet outperforms state-of-the-art methods and learns a resilience-aware policy that can guide the optimization process.

## Path-specific Causal Fair Prediction via Auxiliary Graph Structure Learning

### TL;DR

None

### Abstract

Algorithm fairness has become a trending topic, and it has a great impact on social welfare. Among different fairness definitions, path-specific causal fairness is a widely adopted one with great potentials, as it distinguishes the fair and unfair effects that the sensitive attributes exert on algorithm predictions. Existing methods based on path-specific causal fairness either require graph structure as the prior knowledge or have high complexity in the calculation of path-specific effect. To tackle these challenges, we propose a novel casual graph based fair prediction framework, which integrates graph structure learning into fair prediction to ensure that unfair pathways are excluded in the causal graph. Furthermore, we generalize the proposed framework to the scenarios where sensitive attributes can be non-root nodes and affected by other variables, which is commonly observed in real-world applications but hardly addressed by existing works. We provide theoretical analysis on the generalization bound for the proposed fair prediction method, and conduct a series of experiments on real-world datasets to demonstrate that the proposed framework can provide better prediction performance and algorithm fairness trade-off. 

## MT-KGQA : multi-task end-to-end model for complex knowledge graph question answering

### TL;DR

None

### Abstract

$In this paper, we propose an end-to-end model applied to complex knowledge graph question answering tasks. In real scenarios, the complex knowledge graph question answering task is composed of many NLP subtasks, including text classification, semantic similarity judgment, table question answering, and so on. Traditional graph question answering schemes generally perform these types of tasks step by step and independently. A step-by-step system will cause errors to accumulate, and it will also slow down the inference speed of the overall task. Our end-to-end model is a semantic understanding model based on the self-encoding BERT model. Through a large number of experiments, we designed a model structure that can accurately complete each sub-task, ensuring that each sub-task does not interfere with each other, and combined with a mixed training method of multiple types of data. Through these innovations, we integrated the knowledge graph question answering task, which contains multiple sub-tasks, into a model to complete, greatly speeding up the inference speed in application. In addition, the model can cover most natural language understanding tasks. In application scenarios, different tasks can be combined to meet different needs.$

## Causal Bandits for General Graphs with Atomic Interventions

### TL;DR

None

### Abstract

We study the problem of determining the best intervention in a Causal Bayesian Network (CBN) specified only by its causal graph. We model this as a stochastic multi-armed bandit (MAB) problem with side-information, where the interventions correspond to the arms of the bandit instance. First, we propose a simple regret minimization algorithm that takes as input a semi-Markovian causal graph with atomic interventions and possibly unobservable variables, and achieves $\tilde{O}(\sqrt{M/T})$ expected simple regret, where $M$ is dependent on the input CBN and could be very small compared to the number of arms}. We also show that this is almost optimal for CBNs described by causal graphs having an $n$-ary tree structure. Our simple regret minimization results, both upper and lower bound, subsume previous results in the literature, which assumed additional structural restrictions on the input causal graph. In particular, our results indicate that the simple regret guarantee of our proposed algorithm can only be improved by considering more nuanced structural restrictions on the causal graph. Next, we propose a cumulative regret minimization algorithm that takes as input a general causal graph with all observable nodes and atomic interventions and performs better than the optimal MAB algorithm that does not take causal side-information into account. We also experimentally compare both our algorithms with the best known algorithms in the literature. To the best of our knowledge, this work gives the first simple and cumulative regret minimization algorithms for CBNs with general causal graphs under atomic interventions and having unobserved confounders.  

## Multifractal Graph Neural Networks for Code Representation Learning

### TL;DR

None

### Abstract

Learning representations for code is critical to a wide range of applications from compiler optimization and program classification to system design and optimization. Moreover, graph-based learning representations of software programs enable new optimizations for programming languages (e.g., C++, Python) that are unavailable for natural language. Despite the promising results of graph neural networks (GNNs) for learning software representations, the existing GNN-based approaches exhibit limitations in capturing the long-range topological dependencies, which in fact is particularly important in code representations, especially for tasks such as reachability analysis that depend on long-range information. To address these issues, we propose a Multifractal GNN framework for low level virtual machine (LLVM) intermediate representation (IR)  graphs capable of capturing the long-range structural complexity of code graphs and exploiting the estimated multi-fractal geometric features in the GNNs for code representation learning. This approach not only captures the long-range dependencies among nodes but also preserves them during the learning process. Experiment results demonstrate the effectiveness and superior performance of the proposed approach in code representation learning for both the node-level reachability analysis task and the graph-level classification task for heterogeneous device mapping. In addition, our case study on the models' performance over nodes with different numbers of hops away from the root demonstrate the improvement by the proposed framework in predicting nodes with long-range dependency, and the analysis on the graph complexity v.s. the best number of graph layers for GNNs also provides guidance and insights for future studies and applications in this topic.

## Dual-view Molecule Pre-training

### TL;DR

None

### Abstract

Inspired by its success in natural language processing and computer vision, pre-training has attracted substantial attention in cheminformatics and bioinformatics, especially for molecule based tasks. A molecule can be represented by either a graph (where atoms are connected by bonds) or a SMILES sequence (where depth-first-search is applied to the molecular graph with specific rules). Existing works on molecule pre-training use either graph representations only or SMILES representations only. In this work, we propose to leverage both the representations and design a new pre-training algorithm, dual-view molecule pre-training (briefly, DMP), that can effectively combine the strengths of both types of molecule representations. The model of DMP consists of two branches: a Transformer branch that takes the SMILES sequence of a molecule as input, and a GNN branch that takes a molecular graph as input. The training of DMP contains three tasks: (1) predicting masked tokens in a SMILES sequence by the Transformer branch, (2) predicting masked atoms in a molecular graph by the GNN branch, and (3) maximizing the consistency between the two high-level representations output by the Transformer and GNN branches separately. After pre-training, we can use either the Transformer branch (this one is recommended according to empirical results), the GNN branch, or both for downstream tasks. DMP is tested on nine molecular property prediction tasks and achieves state-of-the-art performances on seven of them. Furthermore, we test DMP on three retrosynthesis tasks and achieve state-of-the-result on the USPTO-full dataset.

## Heterogeneous Graph Contrastive Learning via Knowledge-driven and Learning-based Sampling

### TL;DR

None

### Abstract

The recent success of contrastive learning (CL) has already been extended from visual data to graph-structured data. However, many real-world graphs consist of diverse node or edge types, and it remains unaddressed how to exploit such heterogeneous structured information in CL. To this end, this paper presents the first CL framework specifically designed for heterogeneous graphs, named $\textit{heterogeneous graph contrastive learning}$ ($\textbf{HGCL}$). At the core of HGCL are two unique approaches to generate positive samples: (i) a $\textbf{knowledge-driven}$ approach that leverages meta-path, an important form of prior knowledge available for certain heterogeneous graphs. We generate different graph views by perturbing nodes or edges sampled from the provided meta-paths. By enforcing their consistency, we focus on learning on the structurally important and semantically essential correlations along the meta-paths; and (ii) a $\textbf{data-driven}$ approach that utilizes the attention mechanism to learn the sampling distribution for identifying positive pairs, and to seamlessly update the learned distribution via the Gumbel-Softmax trick. We conduct experiments on three real-world heterogeneous graph datasets for the tasks of node classification, node clustering, and link prediction, in the settings of semi-supervised and unsupervised learning. Extensive results show that while existing graph CL approaches that are blind to data heterogeneity yield sub-optimal performance, the proposed HGCL, with either positive sampling approach,  consistently yields state-of-the-art prediction results, e.g.,  $\textbf{93.06}\%$ on DBLP node classification. When available, the two sampling approaches can be combined for further boosts. We promise to release all codes upon acceptance.

## Relational VAE: A Continuous Latent Variable Model for Graph Structured Data

### TL;DR

A Variational Bayes approach to probabilistic modeling for directed attributed graph data.

### Abstract

Graph Networks (GNs) enable the fusion of prior knowledge and relational reasoning with flexible function approximations. In this work, a general GN-based model is proposed which takes full advantage of the relational modeling capabilities of GNs and extends these to probabilistic modeling with Variational Bayes (VB). To that end, we combine complementary pre-existing approaches on VB for graph data and propose an approach that relies on graph-structured latent and conditioning variables. It is demonstrated that Neural Processes can also be viewed through the lens of the proposed model. We show applications on the problem of structured probability density modeling for simulated and real wind farm monitoring data, as well as on the meta-learning of simulated Gaussian Process data. We release the source code, along with the simulated datasets.

## Differentiable Scaffolding Tree for Molecule Optimization

### TL;DR

differentiable scaffolding tree for molecule optimization. 

### Abstract

The structural design of functional molecules, also called molecular optimization, is an essential chemical science and engineering task with important applications, such as drug discovery.
Deep generative models and combinatorial optimization methods achieve initial success but still struggle directly modeling discrete chemical structures, relying on enumeration heavily, thus are inefficient in terms of oracle calls. 
This paper proposes differentiable scaffolding tree (DST) that utilizes the learned oracle network to make the chemical structure locally differentiable. 
This technique enables a direct gradient-based optimization on a chemical graph structure by back-propagating the derivatives from the target properties through a graph neural network. 
Our empirical studies show the direct gradient-based molecular optimizations are both effective and efficient. Furthermore, the learned graph parameters can also provide explanation that helps the domain experts understand the model output.

## Hierarchical Graph Pattern Understanding for Zero-Shot Video Object Segmentation

### TL;DR

This work introduces a new hierarchical graph pattern understanding framework to mine high-order associations of motion-appearance features for target objects, achieving state-of-the-art performance zero-shot video object segmentation.

### Abstract

This paper proposes a new hierarchical graph neural network (GNN) architecture, dubbed hierarchical graph pattern understanding (HGPU), for zero-shot video object segmentation (ZS-VOS). Inspired by the strong ability of GNNs in capturing structural relations, HGPU innovatively leverages motion cues (i.e., optical flow) to enhance the high-order representations from the neighbors of target frames. Specifically, a hierarchical graph pattern encoder with message aggregation is introduced to acquire different levels of motion and appearance features in a sequential manner. Furthermore, a decoder is designed for hierarchically parsing and understanding the transformed multi-modal contexts to achieve more accurate and robust results. HGPU achieves state-of-the-art performance on four publicly available benchmarks (DAVIS-16, YouTube-Objects, Long-Videos and DAVIS-17).~\footnote{The source codes are available at \url{https://github.com/ZSVOS/HGPU}.}

## Dynamic Labeling for Unlabeled Graph Neural Networks

### TL;DR

None

### Abstract

Existing graph neural networks (GNNs) largely rely on node embeddings, which represent a node as a vector by its identity, type, or content. However, graphs with unlabeled nodes widely exist in real-world applications (e.g., anonymized social networks). Previous GNNs either assign random labels to nodes (which introduces artefacts to the GNN) or assign one embedding to all nodes (which fails to distinguish one node from another). In this paper, we analyze the limitation of existing approaches in two types of classification tasks, graph classification and node classification. Inspired by our analysis, we propose two techniques, Dynamic Labeling and Preferential Dynamic Labeling, that satisfy desired properties statistically or asymptotically for each type of the task. Experimental results show that we achieve high performance in various graph-related tasks.

## Skeleton-based Gait Recognition using Game Engine and Graph Convolution Network

### TL;DR

None

### Abstract

This paper introduces a new method for skeleton-based gait recognition using game engine and graph convolution networks. The game engine is used as a top-down method and the graph convolution networks is used as a bottom-up method. The skeleton data is fed into the game engine to recover the gait paramters and generate a gait simulation for recognition by comparing with the source data. On the other hand, a graph convolution networks with adaptive weights and residual connection is used for gait recognition in another stream. Finally, the results from both methods are combined to make the final decision. Our approach is validated on UPCV Gait K1, UPCV Gait K2 and KinectGait dataset and achieves superior performance to SOTA methods for skeleton-based gait recognition.

## Efficient Neural Causal Discovery without Acyclicity Constraints

### TL;DR

We present ENCO, an efficient structure learning method that leverages observational and interventional data and scales to graphs with a thousand variables.

### Abstract

Learning the structure of a causal graphical model using both observational and interventional data is a fundamental problem in many scientific fields. A promising direction is continuous optimization for score-based methods, which efficiently learn the causal graph in a data-driven manner. However, to date, those methods require constrained optimization to enforce acyclicity or lack convergence guarantees. In this paper, we present ENCO, an efficient structure learning method leveraging observational and interventional data. ENCO formulates the graph search as an optimization of independent edge likelihoods with the edge orientation being modeled as a separate parameter. Consequently, we can provide convergence guarantees of ENCO under mild conditions without constraining the score function with respect to acyclicity. In experiments, we show that ENCO can efficiently recover graphs with hundreds of nodes, an order of magnitude larger than what was previously possible, while handling deterministic variables and latent confounders.

## Neural Subgraph Matching

### TL;DR

None

### Abstract

Subgraph matching is the problem of determining the presence of a given query graph in a large target graph. 
Despite being an NP-complete problem, the subgraph matching problem is crucial in domains ranging from network science and database systems to biochemistry and cognitive science. 
However, existing techniques based on combinatorial matching and integer programming cannot handle matching problems with both large target and query graphs.
Here we propose NeuroMatch, an accurate, efficient, and robust neural approach to subgraph matching. NeuroMatch decomposes query and target graphs into small subgraphs and embeds them using graph neural networks. Trained to capture geometric constraints corresponding to subgraph relations, NeuroMatch then efficiently performs subgraph matching directly in the embedding space. Experiments demonstrate that NeuroMatch is 100x faster than existing combinatorial approaches and 10% better in AUROC than existing approximate subgraph matching heuristic methods.

## Discovering Dynamic Salient Regions for Spatio-Temporal Graph Neural Networks

### TL;DR

None

### Abstract

Graph Neural Networks are perfectly suited to capture latent interactions between various entities in the spatio-temporal domain (e.g. videos). However, when an explicit structure is not available, it is not obvious what atomic elements should be represented as nodes. Current works generally use pre-trained object detectors or fixed, predefined regions to extract graph nodes. Improving upon this, our proposed model learns nodes that dynamically attach to well-delimited salient regions, which are relevant for a higher-level task, without using any object-level supervision. Constructing these localized, adaptive nodes gives our model inductive bias towards object-centric representations and we show that it discovers regions that are well correlated with objects in the video. In extensive ablation studies and experiments on two challenging datasets, we show superior performance to previous graph neural networks models for video classification.

## Post-processing for Individual Fairness

### TL;DR

Post-processing algorithms for achieving individual fairness

### Abstract

Post-processing in algorithmic fairness is a versatile approach for correcting bias in ML systems that are already used in production. The main appeal of post-processing is that it avoids expensive retraining. In this work, we propose a suite of general post-processing algorithms for individual fairness (IF). We consider a setting where the learner only has access to the predictions of the original model and a similarity graph between individuals guiding the desired fairness constraints. We cast the IF post-processing problem as a graph smoothing problem corresponding to graph Laplacian regularization that preserves the desired ``treat similar individuals similarly'' interpretation. Our theoretical results demonstrate the connection of the new objective function to a local relaxation of the original individual fairness. Empirically, our post-processing algorithms correct individual biases in large scale NLP models, e.g., BERT, while preserving accuracy.

## Decentralized Multi-Armed Bandit Can Outperform Classic Upper Confidence Bound

### TL;DR

None

### Abstract

This paper studies a decentralized multi-armed bandit problem in a multi-agent network. The problem is simultaneously solved by $N$ agents assuming they face a common set of $M$ arms and share the same mean of each arm’s reward. Each agent can receive information only from its neighbors, where the neighbor relations among the agents are described by a directed graph whose vertices represent agents and whose directed edges depict neighbor relations. A fully decentralized multi-armed bandit algorithm is proposed for each agent, which twists the classic consensus algorithm and upper confidence bound (UCB) algorithm. It is shown that the algorithm guarantees each agent to achieve a better logarithmic asymptotic regret than the classic UCB provided the neighbor graph is strongly connected. The regret can be further improved if the neighbor graph is undirected and connected. 

## Steerable Equivariant Message Passing on Molecular Graphs

### TL;DR

Steerable features allow graph neural networks to endow nodes, edges and messages with directional information, which is highly beneficial for predicting molecular properties.

### Abstract

Graph neural networks have emerged as a scalable and efficient method for predicting molecular properties. However, generalisation on molecular graphs is difficult due to variations in structure and composition. One solution to foster generalisation is to exploit and incorporate symmetries into neural networks. In this work, we introduce Steerable E(3) Equivariant Graph Neural Networks (SEGNNs) which are equivariant to rotation, translation, reflection and permutation. SEGNNs generalise equivariant graph neural networks, such that information at nodes and edges is not restricted to be invariant (scalar), but can also be covariant (vector, tensor). SEGNNs operate effectively on local graphs with small cutoff radius, reducing the number of messages sent in every layer and consequently improve scalability. We demonstrate the effectiveness of SEGNNs by obtaining state of the art results on the QM9 and the recently proposed Open Catalyst Project OC20 dataset.

## Tree in Tree: from Decision Trees to Decision Graphs

### TL;DR

This paper introduces TnT decision graph as a more accurate and efficient alternative to decision trees.

### Abstract

Decision trees have been widely used as classifiers in many machine learning applications thanks to their lightweight and interpretable decision process. This paper introduces Tree in Tree decision graph (TnT), a framework that extends the conventional decision tree to a more generic and powerful directed acyclic graph. TnT constructs decision graphs by recursively growing decision trees inside the internal or leaf nodes instead of greedy training. The time complexity of TnT is linear to the number of nodes in the graph, therefore it can construct decision graphs on large datasets. Compared to decision trees, we show that TnT achieves better classification performance with reduced model size, both as a stand-alone classifier and as a base-estimator in bagging/AdaBoost ensembles. Our proposed model is a novel, more efficient and accurate alternative to the widely-used decision trees.

## Self-Supervised Graphs for Audio Representation Learning with Limited Labeled Data

### TL;DR

None

### Abstract

Large scale databases with high-quality manual annotations are scarce in audio domain. We explore a self-supervised graph approach to learning audio representations from limited labeled data. Considering each audio sample as a graph node, we propose a subgraph-based learning framework with new pretext tasks that can learn effective audio representations despite limited data being highly limited. During training, subgraphs are constructed to exploit the relationship between the labeled and unlabeled audio samples, while during inference we use random edges to alleviate the overhead of graph construction. We evaluate our model on three benchmark audio databases for two tasks: acoustic event detection and speech sentiment analysis. Our semi-supervised model performs better or on par with fully supervised models and outperforms several competitive existing models. Our model is compact (240k parameters), and can produce generalized audio representations robust to noise.

## Dissecting the Diffusion Process in Linear Graph Convolutional Networks

### TL;DR

We addreess the fundemental limitations of linear GCNs from a continuous perspective and make linear GCNs comparable to state-of-the-art nonlinear GCNs.

### Abstract

Graph Convolutional Networks (GCNs) have attracted more and more attentions in recent years. A typical GCN layer consists of a linear feature propagation step and a nonlinear transformation step. Recent works show that a linear GCN can achieve comparable performance to the original non-linear GCN while being much more computationally efficient. In this paper, we dissect the feature propagation steps of linear GCNs from a perspective of continuous graph diffusion, and analyze why linear GCNs fail to benefit from more propagation steps. Following that, we propose Decoupled Graph Convolution (DGC) that decouples the terminal time and the feature propagation steps, making it more flexible and capable of exploiting a very large number of feature propagation steps. Experiments demonstrate that our proposed DGC improves linear GCNs by a large margin and makes them competitive with many modern variants of non-linear GCNs.

## Is Heterophily A Real Nightmare For Graph Neural Networks Performing Node Classification?

### TL;DR

A GNN frameworks to address heterophily challenges for node classification tasks.

### Abstract

Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using the graph structures based on the relational inductive bias (homophily assumption). Though GNNs are believed to outperform NNs in real-world tasks, performance advantages of GNNs over graph-agnostic NNs seem not generally satisfactory. Heterophily has been considered as a main cause and numerous works have been put forward to address it. In this paper, we first show that not all cases of heterophily are harmful for GNNs with aggregation operation. Then, we propose new metrics based on a similarity matrix which considers the influence of graph structure and input features on GNNs. The metrics demonstrate advantages over the commonly used homophily metrics by tests on synthetic graphs. From the metrics and the observations, we find some cases of harmful heterophily can be addressed by diversification operation. With this fact and knowledge of filterbanks, we propose the Adaptive Channel Mixing (ACM) framework to adaptively exploit aggregation, diversification and identity operations in each GNN layer to address harmful heterophily. We validate the ACM-augmented baselines with 11 real-world node classification tasks. They consistently achieve significant performance gain and exceed the state-of-the-art GNNs on most of the tasks without incurring significant computational burden.

## BNS-GCN: Efficient Full-Graph Training of Graph Convolutional Networks with Partition-Parallelism and Random Boundary Node Sampling

### TL;DR

None

### Abstract

Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art for graph-based learning tasks. However, it is challenging to train GCNs at scale, hindering both their applications to real-world large graphs and exploration of deeper GCN architectures. While it might be natural to consider graph partition and distributed training for tackling this challenge, this direction has been slightly touched upon previously due to the ineffectiveness of existing designs. In this work, we first analyze why distributed GCN training is ineffective and identify the underlying cause to be the excessive number of boundary nodes of each partitioned subgraph, which easily explodes the memory and communication costs for GCN training. Furthermore, we propose a simple yet effective method dubbed BNS-GCN that adopts random Boundary-Node-Sampling to enable efficient and scalable distributed GCN training. Experiments and ablation studies consistently validate the effectiveness of BNS-GCN, e.g., boosting the throughput by up to 16.2× and reducing the memory usage by up to 58%, while maintaining a full-graph accuracy. Additionally, both theoretical and empirical analysis show that BNS-GCN enjoys better generalization accuracy than existing sampling-based methods. We believe that our BNS-GCN opens up a new paradigm for enabling GCN training at scale. All codes will be released publicly upon acceptance.

## HL-Net: Heterophily Learning Network for Scene Graph Generation

### TL;DR

None

### Abstract

Scene graph generation (SGG) aims to detect objects and predict their pairwise relationships. Current SGG methods generally utilize graph neural networks (GNNs) to acquire information about the context between objects/relationships. Despite their effectiveness, current SGG methods only assume scene graph homophily while ignoring the heterophily. In this paper, we propose a novel Heterophily Learning Network (HL-Net) to comprehensively explore the homophily and heterophily of objects/relationships in scene graphs. More specifically, HL-Net devises 1) an adaptive reweighting transformer module, which adaptively integrates the information from different layers to exploit both the heterophily and homophily in objects, 2) a relationship label propagation module that efficiently explores the connections between relationships by considering the heterophily to refine the relationship representation, and 3) a heterophily-aware message-passing module to further distinguish the heterophily and homophily between objects/relationships, thus enabling more reasonable message passing in graphs. We conducted extensive experiments on two public datasets: Visual Genome (VG) and Open Images (OI). The experimental results demonstrate the superiority of our proposed HL-Net over existing state-of-the-art approaches. Specifically, HL-Net outperforms the second-best competitors by 2.1$\%$ on the VG dataset for scene graph classification and 1.2$\%$ on the IO dataset for the final score. Codes will be released on GitHub.

## Co-conductance clustering

### TL;DR

We propose a new quality measure for graph clustering, show a constant-factor approximation algorithm for the measure, and study it empirically obtaining good results according to both ground truth-based and synthetic benchmarks.

### Abstract

   As an important task in data mining, graph clustering aims to identify subsets of vertices in a graph that are significantly denser than their connection to the remaining part of the graph. Some of the most popular graph clustering objectives are normalized cut and conductance. While these clustering metrics are well-studied, most known algorithms for them need a predetermined number of clusters, lack scalability, or have weak theoretical guarantees. In this paper, we address these shortcomings by introducing a new quality measure, referred to as co-conductance, which captures the ratio between the number of internal cluster edges to the sum of vertex degrees. We aim to maximize the sum of the p-th power of co-conductance of clusters.  For the canonical case of p=1, co-conductance is equal to the number of clusters minus the normalized cut, and for this case, we present a linear-time constant-factor approximation algorithm. We also show that co-conductance has desirable theoretic properties, and develop heuristic clustering algorithm that optimizes co-conductance. Finally, we provide an empirical study of our algorithm on publicly available data-sets and demonstrate that it is effective at minimizing the normalized cut, and at the same time performs favorably against state-of-the-art community detection algorithms. 

## Modeling Heterogeneous Hierarchies with Relation-specific Hyperbolic Cones

### TL;DR

We present ConE, a knowledge graph embedding method that can capture the transitive closure properties of heterogeneous hierarchical relations. 

### Abstract

Hierarchical relations are prevalent and indispensable for organizing human knowledge captured by a knowledge graph (KG). The key property of hierarchical relations is that they induce a partial ordering over the entities, which needs to be modeled in order to allow for hierarchical reasoning. However, current KG embeddings can model only a single global hierarchy (single global partial ordering) and fail to model multiple heterogeneous hierarchies that exist in a single KG. Here we present ConE (Cone Embedding), a KG embedding model that is able to simultaneously model multiple hierarchical as well as non-hierarchical relations in a knowledge graph. ConE embeds entities into hyperbolic cones and models relations as transformations between the cones. In particular, ConE uses cone containment constraints in different subspaces of the hyperbolic embedding space to capture multiple heterogeneous hierarchies. Experiments on standard knowledge graph benchmarks show that ConE obtains state-of-the-art performance on hierarchical reasoning tasks as well as knowledge graph completion task on hierarchical graphs. In particular, our approach yields new state-of-the-art Hits@1 of 45.3% on WN18RR and 16.1% on DDB14 (0.231 MRR). As for hierarchical reasoning task, our approach outperforms previous best results by an average of 20% across three hierarchical datasets.

## HA-GCN: Efficient Graph Convolutional Networks via Hardware-Aware Pruning and Quantization

### TL;DR

We significantly boost the inference efficiency of graph convolutional networks via hardware-aware pruning and quantization.

### Abstract

Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art model for non-Euclidean graph representation learning. However, it remains notoriously challenging to inference GCNs over large graph datasets, limiting their application to large-scale real-world tasks. This is because large real-world graphs tend to follow the power-law distribution and therefore have highly irregular and sparse adjacency matrices, resulting in prohibitive inefficiencies in both data processing and movement. To tackle these challenges, we propose a Hardware-Aware GCN compression framework dubbed HA-GCN, dedicated to boosting GCNs' inference efficiency. Our proposed HA-GCN features the following two highlights: (1) a divide and conquer training strategy for largely alleviating the graph irregularity by enforcing (i.e., sparsifying and diagonalizing) chunk patterns within GCNs’ adjacency matrices, while maintaining the accuracy; and (2) a chunk-based quantization algorithm for enabling ultra-low yet hardware-friendly mixed-precision during inference. Extensive experiments and ablation studies on various GCN models and datasets consistently validate that HA-GCN can aggressively trim down the energy cost (1.77x ~ 15.2x) and latency (4.52x ~ 28.8x) of GCN inference while leading to a comparable or even better accuracy, over state-of-the-art GCN compression techniques. All codes and pretrained models will be released publicly upon acceptance.

## Re-ranking for image retrieval and transductive few-shot classification

### TL;DR

An unified framework for imager retrieval and transductive few-shot classification

### Abstract

In the problems of image retrieval and few-shot classification, the mainstream approaches focus on learning a better feature representation. However, directly tackling the distance or similarity measure between images could also be efficient. To this end, we revisit the idea of re-ranking the top-k retrieved images in the context of image retrieval (e.g., the k-reciprocal nearest neighbors) and generalize this idea to transductive few-shot learning. 
    
We propose to meta-learn the re-ranking updates such that the similarity graph converges towards the target similarity graph induced by the image labels.  Specifically, the re-ranking module takes as input an initial similarity graph between the query image and the contextual images using a pre-trained feature extractor, and predicts an improved similarity graph by leveraging the structure among the involved images. We show that our re-ranking approach can be applied to unseen images and can further boost existing approaches for both image retrieval and few-shot learning problems.  Our approach operates either independently or in conjunction with classical re-ranking approaches, yielding clear and consistent improvements on image retrieval (CUB, Cars, SOP) and transductive few-shot classification (Mini-ImageNet, tiered-ImageNet, and CIFAR-FS) benchmarks.

## PI-GNN: A Novel Perspective on Semi-Supervised Node Classification against Noisy Labels

### TL;DR

None

### Abstract

Semi-supervised node classification on graphs is a fundamental problem in graph mining that uses a small set of labeled nodes and many unlabeled nodes for training, so that its performance is quite sensitive to the quality of the node labels. However, it is expensive to maintain the label quality for real-world graph datasets, which presents huge challenges for the learning algorithm to keep a good generalization ability. In this paper, we propose a novel robust learning objective dubbed pairwise interactions (PI) for the model, such as Graph Neural Network (GNN) to combat against noisy labels. Unlike classic robust training approaches that operate on the pointwise interactions between node and class label pairs, PI explicitly forces the embeddings for node pairs that hold a positive PI label to be close to each other, which can be applied to both labeled and unlabeled nodes. We design several instantiations for the PI labels based on the graph structure as well as node class labels, and further propose a new uncertainty-aware training technique to mitigate the negative effect of the sub-optimal PI labels. Extensive experiments on different datasets and GNN architectures demonstrate the effectiveness of PI, which also brings a promising improvement over the state-of-the-art methods.

## Learning Structure from Spatio-Temporal Data: Soil Moisture Forecasting through Graph Neural Network

### TL;DR

None

### Abstract

Spatio-temporal data analysis has been an important topic due to its diverse set of real-world applications especially in the field of computational sustainability. This type of data shows significant correlation both spatially and over time. Conventional data driven approaches often fail to model the dynamic dependency jointly over space and time for spatio-temporal data. In this work, we map the prediction over spatio-temporal data to semi-supervised learning on temporal graphs. We choose soil moisture forecasting as a stylized application of modeling spatio-temporal data which also has dependency on multiple external factors such as weather and vegetation. We propose a dynamic graph neural network (GNN) which can use the dependency of related locations over a region to forecast soil moisture, even in the presence of missing values in the training. However, unlike social or information networks, graph structure is not explicitly given for spatio-temporal data. Hence, we incorporate the problem of graph structure learning in the framework of dynamic GNN. Our algorithm, referred to as DGLR, performs end-to-end learning to predict soil moisture over multiple locations in a region over time along with updating the graph structure. Our solution achieves state-of-the-art results on real-world soil moisture datasets compared to existing machine learning approaches.

## Not All Low-Pass Filters are Robust in Graph Convolutional Networks

### TL;DR

None

### Abstract

Graph Convolutional Networks (GCNs) are promising deep learning approaches in the representation learning for graph-structured data. However, despite the proliferation of such methods, it is known that they are vulnerable to carefully crafted adversarial attacks on the graph structure. In this paper, we first conduct an adversarial vulnerability analysis based on matrix perturbation theory. We prove that the low-frequency components of the symmetric normalized Laplacian, which is usually used as the convolutional filter in GCNs, could be more robust against structural perturbations when their eigenvalues fall into a robust interval. Our results indicate that not all low-frequency components are robust to adversarial attacks and provide a deeper understanding of the relationship between graph spectrum and robustness of GCNs. Motivated by the theory, we present GCN-LFR, a general robust co-training paradigm for GCN-based models, that encourages transferring the robustness of low-frequency components with an auxiliary neural network. To this end, GCN-LFR could enhance the robustness of various kinds of GCN-based models against poisoning structural attacks in a plug-and-play manner. Extensive experiments across five benchmark datasets and five GCN-based models also confirm that GCN-LFR is resistant to the adversarial attacks without compromising on performance in the benign situation.

## On Local Aggregation in Heterophilic Graphs

### TL;DR

We show that simple GNNs achieve state of the art on heterophilic datasets, and propose the neighborhood information content (NIC) graph metric as a more useful predictor of GNN accuracy than graph homophily.

### Abstract

Many recent works have studied the performance of Graph Neural Networks (GNNs) in the context of graph homophily - a label-dependent measure of connectivity. Traditional GNNs generate node embeddings by aggregating information from a node's neighbors in the graph. Recent results in node classification tasks show that this local aggregation approach performs poorly in graphs with low homophily (heterophilic graphs). Several mechanisms have been proposed to improve the accuracy of GNNs on such graphs by increasing the aggregation range of a GNN layer, either through multi-hop aggregation, or through long-range aggregation from distant nodes.In this paper, we show that properly tuned classical GNNs and multi-layer perceptrons match or exceed the accuracy of recent long-range aggregation methods on heterophilic graphs. Thus, our results highlight the need for alternative datasets to benchmark long-range GNN aggregation mechanisms. We also show that homophily is a poor measure of the information in a node's local neighborhood and propose the Neighborhood Information Content (NIC) metric, which is a novel  information-theoretic graph metric. We argue that NIC is more relevant for local aggregation methods as used by GNNs. We show that, empirically, it correlates better with GNN accuracy in node classification tasks than homophily.

## A single scalar bias term enhances  expressive power of link prediction

### TL;DR

None

### Abstract

Ubiquitous existence of graph structures has encouraged the development of graph neural networks (GNNs). In this paper, we focus on link prediction with the GNNs, acquiring edge representation from the node representation. We first theoretically show the limitation of the number of nodes that constitutes an independent set in the graph for the typical graph auto-encoders (GAEs), which indicates a severe lack of its expressive power for link prediction. Secondly, we reveal that introducing a simple single scalar bias term overcomes the limitation and significantly enhances their expressive power, which leads to better predictive performance. As a critical technique to make full use of the bias term, we also propose a learning rate scheduling technique for the single bias term. Then, we compare our proposed method, coined Shited-GAEs, with the existing state-of-the-art methods both on citation networks and on corporate relational networks, and show that our method significantly outperforms the existing methods. Moreover, we empirically show that the Shifted-GAEs successfully alleviate over-smoothing effect, poor node representation caused by multiple-time node feature aggregation, by incorporating novel anti-over-smoothing architectures such as GCNII, while the original GAEs degrade the performance significantly even with such a technique as GCNII.

## Spectral Propagation Graph Network for Few-shot Time Series Classification

### TL;DR

 A novel method named spectral propagation graph network (SPGN) to address few-shot time series classification tasks.

### Abstract

Few-shot Time Series Classification (few-shot TSC) is a challenging problem in time series analysis. It is more difficult to classify when time series of the same class are not completely consistent in spectral domain or time series of different classes are partly consistent in spectral domain. To address this problem, we propose a novel method named Spectral Propagation Graph Network (SPGN) to explicitly model and propagate the spectrum-wise relations between different time series with graph network. To the best of our knowledge, SPGN is the first to utilize spectral comparisons in different intervals and involve spectral propagation across all time series with graph networks for few-shot TSC. SPGN first uses bandpass filter to expand time series in spectral domain for calculating spectrum-wise relations between time series. Equipped with graph networks, SPGN then integrates spectral relations with label information to make spectral propagation. The further study conveys the bi-directional effect between spectral relations acquisition and spectral propagation. We conduct extensive experiments on few-shot TSC benchmarks. SPGN outperforms state-of-the-art results by a large margin in $4\% \sim 13\%$. Moreover, SPGN surpasses them by around $12\%$ and $9\%$ under cross-domain and cross-way settings respectively.

## Monolith to Microservices: Representing Application Software through Heterogeneous GNN

### TL;DR

None

### Abstract

Monolith software applications encapsulate all functional capabilities into a single deployable unit. While there is an intention to maintain clean separation of functionalities even within the monolith, they tend to get compromised with the growing demand for new functionalities, changing team members, tough timelines, non-availability of skill sets, etc. As such applications age, they become hard to understand and maintain. Therefore, microservice architectures are increasingly used as they advocate building an application through multiple smaller sized, loosely coupled functional services, wherein each service owns a single functional responsibility. This approach has made microservices architecture as the natural choice for cloud based applications. But the challenges in the automated separation of functional modules for the already written monolith code slows down their migration task.

Graphs are a natural choice to represent software applications. Various software artifacts like programs, tables and files become nodes in the graph and the different relationships they share, such as function calls, inheritance, resource(tables, files) access types (create, read, update, delete) can be represented as links in the graph. We therefore deduce this traditional application decomposition problem to a heterogeneous graph based clustering task. Our solution is the first of its kind to leverage heterogeneous graph neural network to learn representations of such diverse software entities and their relationships for the clustering task. We study the effectiveness by comparing with works from both software engineering and existing graph representation based techniques. We experiment with applications written in an object oriented language like Java and a procedural language like COBOL and show that our work is applicable across different programming paradigms.

## Beyond Bandit Feedback in Online Multiclass Classification

### TL;DR

We present new and improved results for the online multiclass classification setting.

### Abstract

We study the problem of online multiclass classification in a setting where the learner's feedback is determined by an arbitrary directed graph. While including bandit feedback as a special case, feedback graphs allow a much richer set of applications, including filtering and label efficient classification.
We introduce \textproc{Gappletron}, the first online multiclass algorithm that works with arbitrary feedback graphs. For this new algorithm,
we prove surrogate regret bounds that hold, both in expectation and with high probability, for a large class of surrogate losses. Our bounds are of order $B\sqrt{\rho KT}$, where $B$ is the diameter of the prediction space, $K$ is the number of classes, $T$ is the time horizon, and $\rho$ is the domination number (a graph-theoretic parameter affecting the amount of exploration). In the full information case, we show that \textproc{Gappletron} achieves a constant surrogate regret of order $B^2K$. We also prove a general lower bound of order $\max\big\{B^2K,\sqrt{T}\big\}$ showing that our upper bounds are not significantly improvable. Experiments on synthetic data show that for various feedback graphs our algorithm is competitive against known baselines.

## Graph attention networks focus on symmetry in microRNA

### TL;DR

We show that a relatively simple graph neural network is able to focus on segments of micro RNA that are biologically-relevant for cleavage.

### Abstract

Recent work has shown that microRNA folds into stable hairpin-like structures are much more easily processed in mammalian cells than unstable or non-hairpin-like structures. We wondered whether we could train a graph attention network to focus on topological symmetry. This study provides evidence that even simple graph attention networks can discriminate between efficiently and non-efficiently processed microRNA. They accomplish this by focusing attention on the structural bonds that make up long hairpins.

## Very Deep Graph Neural Networks Via Noise Regularisation

### TL;DR

We show significant gains from training very deep GNNs using a new noise regulariser on quantum chemistry tasks, achieving SOTA on QM9 and Open Catalyst IS2RE.

### Abstract

Graph Neural Networks (GNNs) perform learned message passing over an input graph, but conventional wisdom says performing more than handful of steps makes training difficult and does not yield improved performance. Here we show the contrary. We train a deep GNN with up to 100 message passing steps and achieve several state-of-the-art results on two challenging molecular property prediction benchmarks, Open Catalyst 2020 IS2RE and QM9. Our approach depends crucially on a novel but simple regularisation method, which we call ``Noisy Nodes'', in which input nodes are corrupted with noise, and the model must remove the noise while making its target predictions. Our results show this regularisation method allows the model to monotonically improve in performance with increased message passing steps. Our work opens new opportunities for reaping the benefits of deep neural networks in the space of graph and other structured prediction problems.

## Graph Contrastive Learning Based on Feature-Structure Consistency

### TL;DR

We propose to embed feature-structure consistency into contrastive learning, different from the current mainstream augmentation-based methods.

### Abstract

The abundant features and structure information on graphs provide a potential guarantee for learning high-quality representations without supervision. Feature attribute represents the inherent properties of nodes, while structure attribute describes their neighborhood relationship and status. These two types of attributes should be consistent in identifying a member on the graph. Nevertheless, most current graph contrastive learning methods rely on graph augmentation, overlooking the consistency between feature and structure. In our study, we propose to embed this consistency into contrastive learning framework. Under this framework, node representations are learned in an unsupervised manner by maximizing the agreement between feature representation and structure representation of the same node. Furthermore, two contrastive schemes equipped with feature-structure consistency are put forward to explore information on graphs from various levels. Extensive experiments on node classification tasks and clustering tasks demonstrate the effectiveness and superiority of our approaches.

## Extreme Learning Machine to Graph Convolutional Networks

### TL;DR

We propose ELM-GCN, a novel learning algorithm to train GCN analytically. 

### Abstract

Graph Convolutional Network (GCN) is a powerful model to deal with data arranged as a graph, a structured non-euclidian domain. It is known that GCN reaches high accuracy even when operating with just 2 layers. Another well-known result shows that Extreme Learning Machine (ELM) is an efficient analytic learning technique to train 2 layers Multi-Layer Perceptron (MLP). In this work, we extend ELM theory to operate in the context of GCN, giving rise to ELM-GCN, a novel learning mechanism to train GCN that turns out to be faster than baseline techniques while maintaining prediction capability. We also show a theoretical upper bound in the number of hidden units required to guarantee the GCN performance. To the best of our knowledge, our approach is the first to provide such theoretical guarantees while proposing a non-iterative learning algorithm to train graph convolutional networks.

## Graph Attention Networks with LSTM-based Path Reweighting

### TL;DR

We propose a new LSTM-based path reweigting GNN (PR-GAT) for node classification and node embedding, which can avoid inherent problems of over-smoothing, non-robustness and overfitting.

### Abstract

Graph Neural Networks (GNNs) have been extensively used for mining graph-structured data with impressive performance. However, traditional GNNs suffer from over-smoothing, non-robustness and over-fitting problems. To solve these weaknesses, we design a novel GNN solution, namely Graph Attention Network with LSTM-based Path Reweighting (PR-GAT). PR-GAT can automatically aggregate multi-hop information, highlight important paths and filter out noises. In addition, we utilize random path sampling in PR-GAT for data augmentation. The augmented data is used for predicting the distribution of corresponding labels. Finally, we demonstrate that PR-GAT can mitigate the issues of over-smoothing, non-robustness and overfitting. We achieve state-of-the-art accuracy on 5 out of 7 datasets and competitive accuracy for other 2 datasets. The average accuracy of 7 datasets have been improved by 0.5\% than the best SOTA from literature.

## Explaining Dynamic Graph Neural Networks via Relevance Back-propagation

### TL;DR

None

### Abstract

Graph Neural Networks (GNNs) have shown remarkable effectiveness for representation learning of graph-structured data. Specifically, dynamic GNNs have attracted increasing attention due to the prevalence of time-varying graph data in various applications. Explainability of neural network models is critical for understanding the model's underlying characteristics and brings the opportunity to unearth potential improvement. However, the explainability of dynamic GNNs still remains a largely unexplored area. Faced with this challenge, the present paper introduces a novel framework for explaining dynamic GNNs, called DGExplainer, by propagating the relevance with respect to the output of the model back to the input features. The proposed scheme can also identify important nodes and time-slots based on the obtained relevance. Experiments on real datasets are carried out to demonstrate the effectiveness and stability of the proposed framework for different tasks such as link prediction and node regression.

## PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations

### TL;DR

We propose a novel PDE-motivated family of GCN architectures that prevent over-smoothing and obtain state-of-the-art results, backed up by theoretical guarantees.

### Abstract

Graph neural networks are increasingly becoming the go-to approach in various fields such as computer vision, computational biology and chemistry, where data are naturally explained by graphs. However, unlike traditional convolutional neural networks, deep graph networks do not necessarily yield better performance than shallow graph networks. 
This behavior usually stems from the over-smoothing phenomenon. In this work, we propose a family of architectures
to control this behavior by design. Our networks are motivated by numerical methods for solving Partial Differential Equations (PDEs) on manifolds, and as such, their behavior can be explained by similar analysis. Moreover, as we demonstrate using an extensive set of experiments, our PDE-motivated networks can generalize and be effective for various types of problems from different fields. Our architectures obtain better or on par with the current state-of-the-art results for problems that are typically approached using different architectures.

## Rate-Optimal Subspace Estimation on Random Graphs

### TL;DR

None

### Abstract

We study the theory of random bipartite graph whose adjacency matrix is generated according to a connectivity matrix $M$. We consider the bipartite graph to be sparse, i.e., the entries of $M$ are upper bounded by certain sparsity parameter. We show that the performance of estimating the connectivity matrix $M$ depends on the sparsity of the graph. We focus on two measurement of performance of estimation: the error of estimating $M$ and the error of estimating the column space of $M$. In the first case, we consider the operator norm and Frobenius norm of the difference between the estimation and the true connectivity matrix. In the second case, the performance will be measured by the difference between the estimated projection matrix and the true projection matrix in operator norm and Frobenius norm. We will show that the estimators we propose achieve the minimax optimal rate.

## Finding Hamiltonian cycles with graph neural networks

### TL;DR

We train graph neural network models to output Hamiltonian cycles.

### Abstract

We present two graph neural network models that produce a Hamiltonian cycle one node at a time. Unlike the traveling salesman problem, there has been no attempt to tackle Hamiltonian cycle problem using recent advances in graph neural network architectures. Yet, finding Hamiltonian cycles is important for real life problems such as de novo genome assembly. We also propose a new evaluation method based on asymptotic properties of certain Erd\H{o}s-R\'enyi random graphs. Our first model is trained in a supervised manner on graphs with preconstructed Hamiltonian cycle and thus requires no explicit solver for the problem. The same is true for the second model which we train with policy gradient reinforcement learning. While the second model has worse out-of-the-box performance, it is more flexible and can be fine-tuned to increase performance on any particular graph.

## Query Embedding on Hyper-Relational Knowledge Graphs

### TL;DR

We investigate how to extend the multi-hop reasoning problem to hyper-relational queries on knowledge graphs and consider methods for solving it.

### Abstract

Multi-hop logical reasoning is an established problem in the field of representation learning on knowledge graphs (KGs). It subsumes both one-hop link prediction as well as other more complex types of logical queries. Existing algorithms operate only on classical, triple-based graphs, whereas modern KGs often employ a hyper-relational modeling paradigm. In this paradigm, typed edges may have several key-value pairs known as qualifiers that provide fine-grained context for facts. In queries, this context modifies the meaning of relations, and usually reduces the answer set. Hyper-relational queries are often observed in real-world KG applications, and existing approaches for approximate query answering cannot make use of qualifier pairs. In this work, we bridge this gap and extend the multi-hop reasoning problem to hyper-relational KGs allowing to tackle this new type of complex queries. Building upon recent advancements in Graph Neural Networks and query embedding techniques, we study how to embed and answer hyper-relational conjunctive queries. Besides that, we propose a method to answer such queries and demonstrate in our experiments that qualifiers improve query answering on a diverse set of query patterns.


## Learning Conjoint Attentions for Graph Neural Nets

### TL;DR

None

### Abstract

In this paper, we present Conjoint Attentions (CAs), a series of novel learning-to-attend strategies for graph neural networks (GNNs). Different from previous graph attentions computing weights for feature aggregation solely bestowing layer-wise node features propagated inside GNN, CAs are capable of adaptively leveraging heterogeneous learnable factors either inside or outside of GNNs to compute attention scores. The significant node features that are reckoned by the conjoint criteria are therefore more likely to be propagated in the GNN. Given new conjoint attention strategies, we then propose Graph Conjoint Attention Networks (CATs) that can learn representations embedded with significant latent features deemed by the conjoint attentions. Besides, we theoretically validate the discriminative capacity of CATs. CATs utilizing the proposed conjoint attention strategies have been extensively tested in prevalent benchmarking datasets and comprehensively compared with state-of-the-art baselines. The obtained outstanding performance demonstrates the effectiveness of the proposed conjoint attentions.

## Neural Collaborative Graph Machines for Table Structure Recognition

### TL;DR

None

### Abstract

Recently, a series of graph-based table structure recognition methods has achieved impressive progress with the help of deep graph model. Most of them exploit single visual cues of tabular elements or simply combine visual cues with other modalities via early fusion to reason their graph relationships. However, neither early fusion nor individually reasoning in terms of multiple modalities can be appropriate for all varieties of table structures with great diversity. Instead, different modalities are expected to collaborate with each other in different patterns for different table cases. In the community, the importance of intra-inter modality interactions for table structure reasoning is still unexplored. In this paper, we define it as heterogeneous table structure recognition (Hetero-TSR) problem. With the aim of filling this gap, we present a novel Neural Collaborative Graph Machines (NCGM) equipped with stacked collaborative blocks, which alternatively extracts intra-modality context and models inter-modality interactions in a hierarchical way. It can represent the intra-inter modality relationships of tabular elements more robustly, which significantly improves the recognition performance. We also show that the proposed NCGM can modulate collaborative pattern of different modalities conditioned on the context of intra-modality cues, which is vital for diversified table cases. Experimental results on benchmarks demonstrate our proposed NCGM achieves state-of-the-art performance and beats other contemporary methods by a large margin especially under challenging scenarios.

## Combinatorial optimization as a denoising problem with equivariant graph neural networks

### TL;DR

None

### Abstract

Combinatorial optimization is a well-established area in computer science that provides problem-specific algorithms with a focus on worst case. In practice, problem instances are often closely related due to the common nature of their underlying data distribution and this work explores machine learning approaches which may exploit their common patterns. More specifically, we show that a single deep learning architecture can provide surprisingly good performance in a multitude of problem instances. To do so, we first reframe each combinatorial optimization problem as the search for hidden structure in a noisy input graph. To solve the lack of training data, we propose a planting procedure to add a solution inside the noisy input graph at training. We rely on techniques from probabilistic combinatorics to make these training examples the right amount of difficulty for efficient training and generalization. We then propose a new equivariant graph neural network architecture computing tensors of order two (i.e. embeddings for all pairs of nodes) that is well suited to tackle the main challenges of combinatorial optimization problems. We prove new representation theorems for invariant and equivariant functions showing the expressive power of the graph neural layers, and demonstrate the efficacy of our approach by applying it to the maximum clique problem, the travelling salesman problem and the minimum bisection problem without modifying our architecture.

## HOP: A Geometric Constraint for Hyperbolic Graph Embedding

### TL;DR

A geometric constraint helps the network to push nodes approaching the boundary and align the root with the origin of the hyperbolic space, thereby obtaining a well hierarchical and high-quality embedding.

### Abstract

Hyperbolic geometry is emerging as a promising alternative for graph representation learning of data with scale-free or hierarchical layouts. Most of the existing literature concentrates upon how to extend the graph neural network operations into hyperbolic space, whereas many interesting and exciting geometry properties of this unique metric space have not been explicitly explored or well-considered. One of the remarkable properties of hyperbolic geometry is that its capacity increases exponentially with the radius, which provides sufficient space for the embeddings. It is not difficult to understand that when the nodes are optimized to be away from the origin of hyperbolic space, or equivalently, to approach the boundary, the latent hierarchy layout could be largely preserved. To embrace those inspiring and exciting properties, we propose to explicitly include a global and origin-aware distance constraint, i.e., Hyperbolic Origin Penalty (HOP), into the hyperbolic graph models to improve the embedding quality. HOP is designed to encourage the node embeddings to approach the boundary so that the embedding can be more distinguishable, where the main success lies in the centering shifting operation. Extensive experiments on five graph datasets show the effectiveness of our proposal and the improvements up to 12.2\%. Notably, the proposed HOP is easy to implement without introducing additional parameters or changing the original network architecture, which is broadly applicable to hyperbolic models. 

## Towards Deepening Graph Neural Networks: A GNTK-based Optimization Perspective

### TL;DR

We characterize the optimization property of deep graph neural networks (GNNs) through GNTK and propose a graph-adaptive method to deepen the GNNs.

### Abstract

Graph convolutional networks (GCNs) and their variants have achieved great success in dealing with graph-structured data. However, it is well known that deep GCNs suffer from the over-smoothing problem, where node representations tend to be indistinguishable as more layers are stacked up. The theoretical research to date on deep GCNs has focused primarily on expressive power rather than trainability, an optimization perspective. Compared to expressivity, trainability attempts to address a more fundamental question: given a sufficiently expressive space of models, can we successfully find a good solution (model) by gradient descent-based optimizer? This work fills this gap by exploiting the Graph Neural Tangent Kernel (GNTK), which governs the optimization trajectory under gradient descent for wide GCNs. We formulate the asymptotic behaviors of GNTK in the large depth, which enables us to reveal the dropping trainability of wide and deep GCNs at an exponential rate in the optimization process. Additionally, we extend our theoretical framework to analyze residual connection-resemble techniques, which are found to be only able to mildly mitigate the exponential decay of trainability. To overcome the exponential decay problem more fundamentally, we propose Critical DropEdge, a connectivity-aware and graph-adaptive sampling method, inspired by the critical percolation theory. Experimental evaluation consistently confirms using our proposed method can achieve better results compared to original deep GNNs with both infinite-width and finite-width.

## RS3C: Robust Structure-aware Semi-Supervised Classification for Complicated Noise

### TL;DR

None

### Abstract

Semi-supervised learning with noisy labels is practically
challenging given the additional uncertainty of few labeled data.
The problem is more complicated when outliers and label noise exist
simultaneously (e.g. wrong labeling and sample outliers in medical
image analysis). In this paper, we present a novel framework called
robust structure-aware semi-supervised classification (RS3C) which
is robust to both outliers and noisy labels. Particularly, RS3C
applies joint dimensionality reduction and network sparse
regularization simultaneously on the graph Laplacian matrix in an
iterative fashion to preserve the intrinsic graph structure and
ensure robustness to the compound noise. First, in order to relieve
the influence from noisy features and outliers, a novel
semi-supervised robust dimensionality reduction is applied to the
graph Laplacian matrix relying on robust estimators to assign small
or even zero weights to outliers. Meanwhile, to tackle noisy labels,
the denoised graph similarity information is encoded into the
network regularization. Moreover, a two-step alternative
optimization is conducted to address the problem with guaranteed convergence. 
The proposed method is scalable and can be coupled with feature extraction from a
deep neural network for enhanced robustness and accuracy. Extensive
experimental results demonstrate the promising performance of this
framework when applied to multiple benchmark datasets with respect
to state-of-the-art approaches for image classification.

## Reversible Subgraph Embedding

### TL;DR

We are the first to develop a novel reversible subgraph embedding supporting  subgraph reconstruction and graph reconstruction.

### Abstract

In striking contrast to the widely studied graph embedding, subgraph embedding has received far less attention despite its importance in a variety of subgraph-level tasks. Moreover, the existing subgraph embeddings are not suitable for tasks involving subgraph processing and recovery, since we have to materialize subgraph embeddings for all possible candidate subgraphs that are exponential to the size of the input graph. Therefore, it is desired to devise a subgraph embedding based on which the subgraph can be reconstructed. To the end, we develop a novel reversible subgraph embedding in this paper. By importing the compressed sensing theory into learning node embeddings, we design a reversible read-out operation such that the aggregation vector can be recovered according to the subgraph embedding, where the aggregation vector acts as a bridge between the adjacency matrix and subgraph embedding of the subgraph. To reconstruct the structure of the subgraph from the decoded aggregation vector, we present a bijective rule by applying a simple transformation between binary number and decimal number with a scale operation. We conduct extensive experiments over real graphs to evaluate the proposed subgraph embedding. Experimental results demonstrate that our proposed method greatly and consistently outperforms the baselines in tasks including subgraph reconstruction and graph reconstruction. Furthermore, it exhibits promising performance in node classification and graph classification.

## Persistence Ensembled Graph Neural Networks: Protein Structure Representation Learning with Graph Neural Networks and Topological Data Analysis

### TL;DR

We introduce an approach combining topological data analysis and graph neural networks to learn from protein structure; our approach improves performance over existing architectures.

### Abstract

Deep learning on three dimensional protein structure is an emerging field in structural biology. Protein structure directly influences protein function, and so understanding protein structure--function relationships is an important challenge. Structure also plays a key role in protein design; for example, it can be useful in assessing the viability of novel candidate proteins. Recent work has shown that graph neural networks show promise in structure-based protein prediction tasks, including functional annotation and model quality assessment. In this work, we introduce an approach that combines graph neural networks and deep topological networks (based on persistent homology). Our Persistence Ensembled Graph Neural Networks are an end-to-end trainable framework for protein structure representation learning that learns directly from the 3D atomic coordinates of a protein. We demonstrate our approach on two important problems in protein structure learning: functional annotation predictions across gene ontology classes and protein structure ranking. Our approach improves prediction performance across all the tasks in comparison to graph neural network architectures. In the case of functional annotation prediction, despite only training on tens of thousands of structures, our model also has higher predictive accuracy over a near state-of-the-art protein sequence-aware method that had been pretrained on over 2 billion sequences. This demonstrates the importance on structure in protein functional annotation pipelines.

## Convergent Boosted Smoothing for Modeling GraphData with Tabular Node Features

### TL;DR

We develop a convergent method for combining boosting and graph propagation layers. 

### Abstract

Many practical modeling tasks require making predictions using tabular data composed of heterogeneous feature types (e.g., text-based, categorical, continuous, etc.).  In this setting boosted decision trees and related ensembling techniques generally dominate real-world applications involving iid training/test sets.  However, when there are relations between samples and the iid assumption is no longer reasonable, it remains unclear how to incorporate these dependencies within existing boosting pipelines.  To this end, we propose a generalized framework for combining boosted trees and more general model ensembling techniques, with graph propagation layers that share  node/sample information across edges connecting related samples.  And unlike previous efforts to integrate graph-based models with boosting, our approach is anchored to a principled meta loss function such that provable convergence can be guaranteed under relatively mild assumptions. Across a variety of benchmarks involving non-iid graph data with tabular node features, our framework achieves comparable or superior performance.

## Verifying Feasible Node and Structural Statistics of Graph Probabilistic Generative Models

### TL;DR

None

### Abstract

Probabilistic generative models of power-law graphs are important tools that enable both representation and sampling of the heavy tails and power-law degree distributions of real world networks. Due to the usefulness of node-attributes to represent graph structures and the properties of the nodes, applied researchers have created many models of graphs that incorporate node attributes. However, given random attributed graph(s) it is not clear what are the general conditions that establish goodness of fit given the estimated parameters. In this paper, we define these conditions in terms of the mean square contingency coefficient for both constant and random networks. The conditions assess whether the structure of the attributed-graph has been learned by ensuring that the discrepancy of the mean square contingency coefficient (constant, or random) is minimal with high probability. We apply these criteria to verify the representation capability of a probabilistic generative model for various popular types of models.

## Compositional Reinforcement Learning from Logical Specifications

### TL;DR

None

### Abstract

We study the problem of learning control policies for complex tasks given by logical specifications. Recent approaches automatically generate a reward function from a given specification and use a suitable reinforcement learning algorithm to learn a policy that maximizes the expected reward. These approaches, however, scale poorly to complex tasks that require high-level planning. In this work, we develop a compositional learning approach, called DIRL, that interleaves high-level planning and reinforcement learning. First, DIRL encodes the specification as an abstract graph; intuitively, vertices and edges of the graph correspond to regions of the state space and simpler sub-tasks, respectively. Our approach then incorporates reinforcement learning to learn neural network policies for each edge (sub-task) within a Dijkstra-style planning algorithm to compute a high-level plan in the graph. An evaluation of the proposed approach on a set of challenging control benchmarks with continuous state and action spaces demonstrates that it outperforms state-of-the-art baselines.

## Persistent Message Passing

### TL;DR

A persistency mechanism for GNNs to enable querying on past states.

### Abstract

Graph neural networks (GNNs) are a powerful inductive bias for modelling algorithmic reasoning procedures and graph-structured data. Their prowess was mainly demonstrated on tasks featuring Markovian dynamics, where querying any associated data structure depends only on its \emph{latest} state. For many tasks of interest, however, it may be highly beneficial to support efficient data structure queries dependent on previous states and connectivity. This requires tracking the data structure's evolution through time, placing significant pressure on the GNN's representations. We introduce Persistent Message Passing (PMP), a mechanism which endows GNNs with the capability to query a past state of itself by explicitly \emph{persisting} it: rather than overwriting node representations, it creates new nodes whenever required. We develop versions of PMP for both \emph{static} and \emph{dynamic} graphs, where the latter involves persisting inferred connectivity structure in addition to node states. PMP significantly improves performance of existing GNN models on a number of algorithmic reasoning tasks defined on both static and dynamically changing graph structures.

## Bayesian Learning to Discover Mathematical Operations in Governing Equations of Dynamic Systems 

### TL;DR

None

### Abstract

Discovering governing equations from data is critical for diverse scientific disciplines as they can provide insights into the underlying phenomenon of dynamic systems. This work presents a new representation for governing equations by designing the Mathematical Operation Network (MathONet) with a deep neural network-like hierarchical structure. Specifically, the MathONet is stacked by several layers of unary operations (e.g. $\sin, \cos, \log$) and binary operations (e.g. $+,-,\times$), respectively. An initialized MathONet is typically regarded as a super-graph with a redundant structure, a sub-graph of which can yield the governing equation. We develop a sparse group Bayesian learning algorithm to extract the sub-graph by employing structurally constructed priors over the redundant mathematical operations. By demonstrating on chaotic Lorenz system, Lotka-Volterra system, and Kolmogorov–Petrovsky–Piskunov system, the proposed method can discover the ordinary differential equations (ODEs) and partial differential equations (PDEs) from the observations given limited mathematical operations, without any prior knowledge on possible expressions of the ODEs and PDEs.

## cSTG: A Model-Agnostic Framework to Boost Graph Time Series Forecasting with Node Constraints

### TL;DR

We consider the problem of modeling the intrinsic constraints between nodes of spatio-temporal graph neural networks to improve the prediction accuracy.

### Abstract

We consider the problem of modeling the intrinsic constraints between nodes of spatio-temporal graph neural networks (GNNs) to improve the prediction accuracy. Such constraints typically exist for multivariate time series allocated in a network (a.k.a., graph time-series), including the urban traffic flow, the computer network flow, and cloud micro-services calling flow, yet are far from well studied. In this paper, we propose an algorithm to explicitly learn the constraints from the data. Afterwards, given an arbitrary backbone spatio-temporal GNN, we present a model-agnostic algorithm, named constrained spatio-temporal graph (cSTG), which enforces constraint satisfaction in three complementary ways: (1) constraining input augmentation in a self-supervised manner, (2) constraining feature maps as regularization, and (3) constraining output for error reduction. Experimental results on three datasets show that the prediction error can be reduced by 7.2% on average with the aid of the proposed cSTG framework, while being able to well recover the true constraints, and amenable to various network architectures.

## Graph Barlow Twins: A self-supervised representation learning framework for graphs

### TL;DR

None

### Abstract

The self-supervised learning (SSL) paradigm is an essential exploration area, which tries to eliminate the need for expensive data labeling. Despite the great success of SSL methods in computer vision and natural language processing, most of them employ contrastive learning objectives that require negative samples, which are hard to define. This becomes even more challenging in the case of graphs and is a bottleneck for achieving robust representations. To overcome such limitations, we propose a framework for self-supervised graph representation learning -- Graph Barlow Twins, which utilizes a cross-correlation-based loss function instead of negative samples. Moreover, it does not rely on non-symmetric neural network architectures -- in contrast to state-of-the-art self-supervised graph representation learning method BGRL. We show that our method achieves as competitive results as BGRL, best self-supervised methods, and fully supervised ones while requiring substantially fewer hyperparameters and converging in an order of magnitude training steps earlier.

## Hierarchical Graph Learning for Joint Centric 3D Part Assembly

### TL;DR

We define a new task of joint-centric 3D part assembly and propose a hierarchical graph learning based method to tackle the problem.

### Abstract

For vision-based part pose estimation algorithms to be employed by autonomous assembly systems, the algorithmic design needs to go beyond the part geometry reasoning, and consider the actual physical assembly process of matching and aligning joints.
Towards this end, we study the more realistic scenario of vision for furniture assembly by defining a new task, joint-centric 3D part assembly, to predict part poses with the bilateral objective of shape structure and joint matching optimization. The task difficulty lies in its interleaved nature of requiring combinatorial and continuous problem solving simultaneously. To address these challenges, we propose a hierarchical graph learning-based approach by learning a part graph and a joint graph to become experts for two task objectives. The two graphs are then synchronized to make the final pose prediction that is both structurally sound and joint optimized. Extensive empirical evidence demonstrates the effectiveness of our framework as compared with four baseline approaches.

## NEiT: Node-Edge interaction Transformer for Graph Representation Learning

### TL;DR

None

### Abstract

Graph Neural Networks (GNNs) provide a powerful approach to learning representations of graphs. However, existing GNNs mainly focus on aggregating node-wise information, in which the edge-wise information is auxiliary and treated simplistically. In this work, we design a GNN neural architecture that takes full use of the edge-wise information to store, pass, and aggregate the local feature. Our neural architecture is designed by neuralizing the gradient descent procedure for optimizing a structured energy function on and node and edge representations, which yields a new self-attention type structure that conducts the joint reasoning on both nodes and edges. In empirical studies, we show that our model can capture and reason with edge information to predict the basics graph-theoretic properties, such as number of edges, for which typical GNNs perform poorly. Further, our model achieves state-of-the-art performance on challenging real-world graph benchmarks in various domains such as computer vision and molecular chemistry. 

## A Sketch-Based Architecture For Modular Deep Learning

### TL;DR

None

### Abstract

While deep learning has produced enormous progress, basic architectural questions remain around how to make it into an intelligent system that can recognize different concepts, learn multiple functions, and remember things over time. For example, how are new modules created, how do they form some kind of a hierarchy, how is a knowledge graph created and how does that interact with the modules? In this work we present a conceptual sketch-based architecture for a deep learning system that can do this using an LSH based sketch memory table, with deep learned modules connected by edges that point across hash buckets. We prove under certain assumptions that our architecture can learn a DAG of latent functions under appropriate training data, can recreate a knowledge graph based on graph-like latent structures appearing in the generative process for the training inputs, incorporate RL algorithms, and even work out how to use combinations of the above to solve more complex problems.

## Actively Identifying Causal Effects with Latent Variables Given Only Response Variable Observable

### TL;DR

None

### Abstract

 effect identification plays an important role in guiding decision-making, and some criteria have been proposed to achieve it with the prior knowledge of \emph{ graph}. In many real tasks, however, we are merely concerned with the  effect of each decision variable on one specific target - \emph{response variable}. In this paper, we consider identifying such effects with the existence of \emph{latent confounders} when  graph is \emph{unknown}. Since some  effects may be unidentifiable with only observational data, we additionally allow active interventions but restrict that only the response variable can be observed under interventions. By further exploiting interventional data, our approach can learn the structure information of the variables and thus identify the desired  effects. Specifically, we provide the graphical characterization to efficiently estimate all possible  effects in a \emph{partially ancestral graph (PAG)} by generalized back-door criterion. The characterization guides learning a local structure with interventional data efficiently. The theoretical analysis and empirical studies validate the effectiveness of the proposed method.

## Graph-Constrained Structure Search for Tensor Network Representation

### TL;DR

This work is to study the task of structure search of tensor network decomposition under graph-constraints from the both theoretical and practical aspects.

### Abstract

Recent works paid effort on the structure search issue for tensor network (TN) representation, of which the aim is to select the optimal network for TN contraction to fit a tensor. In practice, however, it is more inclined to solve its sub-problem: searching TN structures from candidates with a similar topology like a cycle or lattice. We name this problem the graph-constrained structure search, and it remains open to this date. In this work, we conduct a thorough investigation of this issue from both the theoretical and practical aspects. Theoretically, we prove that the TN structures are generally irregular under graph constraints yet can be universally embedded into a low-dimensional regular discrete space. Guided by the theoretical results, we propose a simple algorithm, which can encode the graph-constrained TN structures into fixed-length strings for practical purposes by a ` “random-key" trick, and empirical results demonstrate the effectiveness and efficiency of the proposed coding method on extensive benchmark TN representation tasks.

## A Biased Graph Neural Network Sampler with Near-Optimal Regret

### TL;DR

We apply multi-armed bandits with a novel reward for neighbor sampling of graph neural networks with a near-optimal convergence guarantee. 

### Abstract

Graph neural networks (GNN) have recently emerged as a vehicle for applying deep network architectures to graph and relational data.  However, given the increasing size of industrial datasets, in many practical situations, the message passing computations required for sharing information across GNN layers are no longer scalable. Although various sampling methods have been introduced to approximate full-graph training within a tractable budget, there remain unresolved complications such as high variances and limited theoretical guarantees.  To address these issues, we build upon existing work and treat GNN neighbor sampling as a multi-armed bandit problem but with a newly-designed reward function that introduces some degree of bias designed to reduce variance and avoid unstable, possibly-unbounded pay outs.  And unlike prior bandit-GNN use cases, the resulting policy leads to near-optimal regret while accounting for the GNN training dynamics introduced by SGD. From a practical standpoint, this translates into lower variance estimates and competitive or superior test accuracy across several benchmarks. 

## EIGNN: Efficient Infinite-Depth Graph Neural Networks

### TL;DR

None

### Abstract

Graph neural networks (GNNs) are widely used for modelling graph-structured data in numerous applications. However, with their inherently finite aggregation layers, existing GNN models are unable to capture long-range dependencies in the underlying graphs. Motivated by this limitation, we propose a GNN model with infinite depth, which we call Efficient Infinite-Depth Graph Neural Network (EIGNN), to efficiently capture very long range dependencies. We theoretically derive a closed form solution of EIGNN which makes training an infinite-depth GNN model tractable. We then further show that we can achieve more efficient computation for training EIGNN by using eigendecomposition. The empirical results of comprehensive experiments on synthetic and real-world datasets show that EIGNN has a better ability to capture long-range dependencies than recent baselines, and consistently achieves state-of-the-art performance. Furthermore, we show that our model is also more robust against both noise and adversarial perturbations on node features.

## Adaptive Dual Graph Network for Pose Estimation in the Crowded and Occluded Scenes

### TL;DR

We propose the Adaptive Dual Graph Net to learn the pose structural information to help boost the pose estimation accuracy in crowded and occluded scenes.

### Abstract

Although using Graph Net to further refine the 2D pose estimation results in the occluded and crowded scenes has been explored in recent years, its full potential has yet to be unearthed. We propose the Adaptive Dual Graph Net (ADGN) to learn the structural information of the pose skeleton. Our model has two tightly connected graph branches to model the joints and the limbs, and the adaptive GNN layer at the end of each stage to generate the features for each pose skeleton input. The model is used in conjunction with the traditional CNN-based pose estimators to gain better results in crowded and heavily occluded scenes. With the same CNN-based pose estimator and training file as the current state-of-the-art OPEC-Net, our model boosts the estimation results from 67.9 mAP to 72.4 mAP in the CrowdPose dataset. It is 2 higher than the state-of-the-art (70.6 mAP).

## Self-Supervised Representation Learning via Latent Graph Prediction

### TL;DR

None

### Abstract

Self-supervised learning (SSL) of graph neural networks is emerging as a promising way of leveraging unlabeled data. Currently, most methods are based on contrastive learning adapted from the image domain, which requires view generation and a sufficient number of negative samples. In contrast, existing predictive models do not require negative sampling, but lack theoretical guidance on the design of pretext training tasks. In this work, we propose the LaGraph, a theoretically grounded predictive SSL framework based on latent graph prediction. Learning objectives of LaGraph are derived as self-supervised upper bounds to objectives for predicting unobserved latent graphs. In addition to its improved performance, LaGraph provides explanations for recent successes of predictive models that include invariance-based objectives. We provide theoretical analysis comparing LaGraph to related methods in different domains. Our experimental results demonstrate the superiority of LaGraph in performance and robustness to the decreasing of training samples on both graph-level and node-level tasks.

## Composite Association Fields with Supervised Deformable Convolutions for Scene Graph Generation

### TL;DR

A bottom-up approach method used for scene graph generation using association fields and supervised deformable convolution layer.

### Abstract

A scene graph of objects and relations in a visual scene serves as a powerful abstract representation of image semantics that further enhances various image understanding tasks. In this work, we present a novel end-to-end bottom-up scene graph generation (SGG) approach that extends Composite Association Fields to encode subject-object relationships. We further introduce a Component-Aware Refinement Module (C-ARM) using supervised deformable convolution for improved relationship representation. We evaluate our approach on the popular Visual Genome dataset, and demonstrate its effectiveness in detecting relationships.

## Q-Learning With One-Sided and Full Feedback Graphs for Operations Research Problems

### TL;DR

None

### Abstract

Motivated by several classical operations research problems, we propose a new Q-learning-based algorithm, Elimination-Based Half-Q-Learning (HQL), that removes the regret dependence of existing methods on the state-action space cardinality for a variety of problems with the one-sided-feedback graph. We establish that HQL incurs $ \tilde{O}(H^3\sqrt{ T})$ regret, and that a simpler variant of our algorithm, FQL, incurs $\tilde{O}(H^2\sqrt{ T})$ regret for the special case of full-feedback graph. Here H is the episode length and T is the horizon length. 

## Skew-Symmetric adjacency matrices for clustering directed graphs

### TL;DR

We introduce a new Spectral Clustering algorithm for finding Imbalanced Cuts in directed graphs by exploiting the properties of a Skew-Symmetric adjacency matrix. 

### Abstract

Graph clustering methods often critically rely on the symmetry of graph matrices. Developing analogous methods for digraphs often proves more challenging, because digraph matrices are typically asymmetric and not orthogonally diagonalizable. However, researchers have recently proposed several complex-valued Hermitian digraph matrices. In particular, one such representation has been utilized as an input to an algorithm for finding imbalanced cuts. In this work, we establish an algebraic relationship between this matrix and an associated real-valued matrix. We show using this real-valued matrix for imbalanced cut-finding algorithms is not only sufficient but advantageous. Our algorithm uses less memory and asymptotically less computation while provably preserving solution quality. We also show our method can be easily implemented using standing computational building blocks, possesses better numerical properties, and loans itself to a natural interpretation via an objective function relaxation argument. We empirically demonstrate these advantages on real world data sets and show how our algorithm can uncover meaningful cluster structure.

## EPQuant: An Efficient Quantization Methodology for Graph Neural Networks with Enhanced Product Quantization

### TL;DR

A quantization method applied to GNNs can achieve impressive storage reduction. 

### Abstract

Graph Neural Networks (GNNs) have been widely used in graph analysis due to the strong performance on a wide variety of tasks. Unfortunately, as the size of graphs keeps growing, large graphs can easily consume Terabytes, and training on such graphs may take days. Furthermore, the high memory footprint limits the usage of GNNs on resource constrained devices like smartphones and IoT devices. Hence, it is highly desirable to reduce the storage cost, training time, and inference latency. In this work, we apply Product Quantization (PQ) on GNNs for the first time to achieve superior memory capacity reduction than Scalar Quantization (SQ) based works. To eliminate the processing overhead brought by PQ and enable better efficiency, we propose Enhanced Product Quantization (EPQ). It can provide significant memory reduction to the input graph data, which tends to dominate the memory consumption, and speed up the clustering in PQ. Moreover, an efficient quantization framework for GNNs is proposed, which combines EPQ with SQ, to obtain further compression performance and computation acceleration on off-the-shelf hardware, and make it possible to deploy GNNs on resource constrained devices. In addition, the proposed quantization framework can apply to the existing GNNs without too much porting effort. Extensive experimental results show that the proposed quantization scheme can achieve $321.26\times$ and $184.03\times$ memory capacity compression, for input graph data and overall storage, respectively. This impressive memory reduction can come with accuracy loss less than $1\%$. 

## Motif-based Graph Self-Supervised Learning for Molecular Property Prediction

### TL;DR

None

### Abstract

Predicting molecular properties with data-driven methods has drawn much attention in recent years. Particularly, Graph Neural Networks (GNNs) have demonstrated remarkable success in various molecular generation and prediction tasks. In cases where labeled data is scarce, GNNs can be pre-trained on unlabeled molecular data to first learn the general semantic and structural information before being finetuned for specific tasks. However, most existing self-supervised pretraining frameworks for GNNs only focus on node-level or graph-level tasks. These approaches cannot capture the rich information in subgraphs or graph motifs. For example, functional groups (frequently-occurred subgraphs in molecular graphs)  often carry indicative information about the molecular properties. To bridge this gap, we propose Motif-based Graph Self-supervised Learning (MGSSL) by introducing a novel self-supervised motif generation framework for GNNs. First, for motif extraction from molecular graphs, we design a molecule fragmentation method that leverages a retrosynthesis-based algorithm BRICS and additional rules for controlling the size of motif vocabulary. Second, we design a general motif-based generative pretraining framework in which GNNs are asked to make topological and label predictions. 
This generative framework can be implemented in two different ways, i.e., breadth-first or depth-first. 
Finally, to take the multi-scale information in molecular graphs into consideration, we introduce a multi-level self-supervised pre-training. Extensive experiments on various downstream benchmark tasks show that our methods outperform all state-of-the-art baselines.

## Feature Structure Meta Learning by Graph Inference

### TL;DR

None

### Abstract

The advances in deep learning have enabled machine learning methods to outperform human beings in various areas, but it remains a great challenge for a well-trained model to quickly adapt to a new task, which is a key characteristic of human intelligence. One promising solution to realize this goal is through meta learning, also known as learning to learn, which has achieved promising results in few-shot learning.  However, current approaches are still enormously different from human beings' learning process, especially in the ability to extract structural and transferable knowledge. This drawback makes current meta learning frameworks non-interpretable and hard to extend to more complex tasks. We tackle this problem by defining a new meta learning problem called feature structure meta learning, which aims to learn additional structural information among the high-level data features by graph inference.  We use a simple yet effective method to solve this problem and it achieves both solid performance and more interpretability on the few-shot image classification task, which demonstrates the significance of the feature structure.

## Directional Feature Interactions with Graph-Based Explainers

### TL;DR

We introduce a bivariate explainer to explain directional feature interactions in black box models. 

### Abstract

As machine learning algorithms are deployed ubiquitously to a variety of domains, it is imperative to make these often black-box models transparent.  Several recent works explain black-box models by capturing the most influential features for prediction per instance; such explanation methods are univariate, as they characterize importance per feature.  We extend univariate explanation to a higher-order; this enhances explainability, as bivariate methods can capture feature interactions in black-box models, represented as a directed graph.  Analyzing this graph enables us to discover groups of features that are equally important (i.e., interchangeable), while the notion of directionality allows us to identify the most influential features.  We apply our bivariate method on Shapley value explanations, and experimentally demonstrate the ability of directional explanations to discover feature interactions. Using post-hoc accuracy, we show the superiority of our method against state-of-the-art on CIFAR10, IMDB, Census, Divorce, Drug, and gene data.  

## ROLAND: Graph Neural Networks for Dynamic Graphs

### TL;DR

None

### Abstract

Graph Neural Networks (GNNs) have been successfully applied to many real-world static graphs. However, the success on static graphs has not fully translated to dynamic graphs due to the limitations in model design, evaluation settings, and training strategies. Concretely, existing dynamic GNNs do not incorporate state-of-the-art designs from static GNNs, which limits their performance. Current evaluation settings for dynamic GNNs do not fully reflect the evolving nature of dynamic graphs. Finally, commonly used training methods for dynamic GNNs are not scalable.
Here we propose ROLAND, an effective graph representation learning framework for real-world dynamic graphs. At its core, ROLAND framework can help researchers easily repurpose any static GNN to dynamic graphs. Our insight is to view the node embeddings at different GNN layers as hierarchical node states, and then recurrently update them over time. We then introduce a live-update evaluation setting for dynamic graphs that mimics real-world use cases, where GNNs are making predictions and being updated on a rolling basis. Finally, we propose a scalable and efficient training approach for dynamic GNNs via incremental training and meta-learning.
We conduct experiments over 8 different dynamic graph datasets on future link prediction tasks. Models built using ROLAND framework achieve on average 62.7% relative mean reciprocal rank (MRR) improvement over state-of-the-art baselines under the standard evaluation settings on three datasets. We find state-of-the-art baselines experience out-of-memory errors for larger datasets, while ROLAND can easily scale to dynamic graphs with 56 million edges. After re-implementing these baselines using ROLAND training strategy, ROLAND models still achieve on average 15.5% relative MRR improvement over the baselines.

## Architecture Agnostic Federated Learning using Graph HyperNetworks

### TL;DR

A hypernetwork-based solution for federated learning where clients have different neural architectures that cannot be shared.

### Abstract

Standard Federated Learning (FL) techniques are limited to clients with identical network architectures. As a result, inter-organizational collaboration is severely restricted when both data privacy and architectural proprietary are required. In this work, we propose a new FL framework that removes this limitation by adopting a graph hyper-network as a shared knowledge aggregator. A property of the graph hyper network is that it can adapt to various computational graphs, thereby allowing meaningful parameter sharing across models. Unlike existing solutions, our framework makes no use of external data and does not require clients to disclose their model architecture. Compared with distillation-based baselines our method performs significantly better on standard benchmarks. We additionally show encouraging generalization performance to unseen architectures. 

## Boundary Graph Neural Networks for 3D Simulations

### TL;DR

Boundary Graph Neural Networks are able to model 3D simulations within complex geometries over hundreds of thousands of timesteps.

### Abstract

The abundance of data has given machine learning huge momentum in natural sciences and engineering. However, the modeling of simulated physical processes remains difficult. A key problem in doing so is the correct handling of geometric boundaries. While triangularized geometric boundaries are very common in engineering applications, they are notoriously difficult to model by machine learning approaches due to their heterogeneity with respect to size and orientation. In this work, we introduce Boundary Graph Neural Networks (BGNNs), which dynamically modify graph structures to address boundary conditions. Boundary graph structures are constructed via modifying edges, augmenting node features, and dynamically inserting virtual nodes. The new BGNNs are tested on complex 3D granular flow processes of hoppers and rotating drums which are standard parts of industrial machinery. Using precise simulations that are obtained by an expensive and complex discrete element method, BGNNs are evaluated in terms of computational efficiency as well as prediction accuracy of particle flows and mixing entropies. Even if complex boundaries are present, BGNNs are able to accurately reproduce 3D granular flows within simulation uncertainties over hundreds of thousands of simulation timesteps, and most notably particles completely stay within the geometric objects without using handcrafted conditions or restrictions.

## Instant Node Embeddings using Local Graph Information

### TL;DR

Embed nodes using only local graph information, in sublinear time and space.

### Abstract

In this paper, we introduce InstantEmbedding, an efficient method for generating single-node representations using local PageRank computations.
We theoretically prove that our approach produces globally consistent representations in sublinear time and demonstrate this empirically by conducting extensive experiments on real-world datasets with over a billion edges.
Our experiments confirm that InstantEmbedding requires drastically less computation time (over 7,500 times faster) and less memory (by over 2,500 times) to produce a single node's embedding than traditional methods.
We also show that our method produces high-quality representations, demonstrating results that meet or exceed state of the art for unsupervised representation learning on node classification and link prediction tasks.

## Active Learning with Graph Neural Network Dynamics

### TL;DR

In this paper, a active learning strategy for GNNs has been proposed from the view of Neural Networks' dynamics.

### Abstract

 Graph neural networks (GNNs) have seen success in various real-world applications. However, their performance largely depends on the amount of labeled data, and thus can be significantly affected by the label scarcity caused by expensive annotation costs. To make GNNs achieve better performance with limited labeled data, several active learning methods have been proposed. However, these methods are either data-centric, which ignore the current state of GNNs, or require fully labeled external data to pre-train. Motivated by the connection between the generalization performance and the training dynamics, in this paper, we propose a general-propose, model-centric active learning method named \dgal\ for GNNs, from the perspective of training dynamics. Different from previous works, our method does not introduce extra parameters or require external data. We empirically show that \dgal\ consistently outperforms other methods when applied to different types of GNNs. Furthermore, we analyze neural networks under the ultra-width condition, which provides insights for why `Train Faster' and `Generalize Better' are correlated, and why \dgal\ is able to achieve better performance.

## Regularizing GNNs via Consistency-Diversity Graph Augmentations

### TL;DR

We propose a new regularization, considering graph augmentations with good consistency and diversity, to improve the accuracy and generalization of GNNs

### Abstract

Despite the remarkable performance of graph neural networks (GNNs) in semi-supervised learning, it is criticized for not making full use of unlabeled data, suffering from over-fitting. Recently, graph data augmentation, used to improve both accuracy and generalization of GNNs, has received considerable attentions. However, one fundamental question is how to evaluate the quality of graph augmentations in principle? In this paper, we propose two metrics, Consistency and Diversity, from the augmentation correctness and generalization aspects. Unfortunately, we discover that existing methods fall into a dilemma between these two metrics. Can we find an augmentation satisfying both consistency and diversity? A well-informed answer can help us understand the mechanism behind data augmentation and improve the performance of GNNs. To tackle this challenge, we first prove that label propagation is a special case of consistency regularization, which provides a novel finding, i.e., neighbors are special augmentations. Therefore, the homophily of graphs naturally captures the consistency of neighbors. To further promote diversity, we randomly replace the immediate neighbors of each node with its remote neighbors. In order to fully utilize these augmentations, a neighbor-constrained regularization is proposed to enforce the predictions of augmented neighbors to be consistent with each other so that a large number of unlabeled nodes can be utilized. Extensive experiments on five real-world graphs validate the superiority of our method in improving the accuracy and generalization of GNNs.

## Learning in Dense Graph Mean Field Games

### TL;DR

None

### Abstract

Recent advances at the intersection of dense large graph limits and mean field games have begun to enable the scalable analysis of a broad class of dynamical sequential games with large numbers of agents. So far, results have been largely limited to graphon mean field systems with fully continuous diffusion-type dynamics and with little focus on computational methods. In this work, we propose a general formulation for discrete-time graphon mean field games as the limit of dense graph games with non-linear dynamics and weak interaction. We provide a general learning scheme for graphon mean field equilibria by considering the graphon mean field system as a classical mean field system and repeatedly computing a regularized optimal control solution and its generated mean field. We proceed to demonstrate empirically that the finite-agent objective comes increasingly close to the mean field objective for our computed equilibria as the number of agents grows. As a result, we successfully compute plausible approximate Nash equilibria in otherwise infeasible large dense graph games with many agents. More generally, one can substitute approximate techniques such as reinforcement learning with particle methods into our learning scheme, though in practice we find that the learning scheme is sensitive to such approximations.

## Memory Enhanced and Prudential Graph Neural Network

### TL;DR

By leveraging the connections of over-smoothing, non-robustness and overfitting during information propagation, we redesign the LSTM-based propagation rule, which addresses these issues in parallel with state-of-the-art performance.

### Abstract

Non-robustness, over-smoothing and overfitting are three dominant stumbling blocks for graph neural networks (GNNs). Though efforts have been devoted, these issues still remain fundamentally unresolved. To address these issues thoroughly, we propose a systematic GNN framework-Memory Enhanced and Prudential Graph Neural Network (MEPG), which leverages these issues connections during nodes propagation and addresses them in parallel. We redesign the propagation rule of GCN based on Long-Short-Term-Memory (LSTM) \cite{LSTM}, whose long-term memory ability is leveraged to prevent the nodes specific information being over-smoothed, and overfitting is also moderated due to higher-order neighborhood information brought into the final nodes representations. In addition, We also propose ExchangeNodes for data augmentation, and isolation regularization (IR) training which leverages the consistency between the prediction outputs and the outputs obtained after exchanged nodes being removed. By training with IR, the propagation of the adverse nodes can be effectively restricted by LSTM-based propagation module with the help of implemented gates, the risk of non-robustness is thus reduced. We finally provide empirical and theoretical evidence about thestate-of-the-art performance of MEPG on semi-supervised node classification task,and the substantial advantages in resisting over-smoothing, non-robustness and overfitting.

## On Structural Fairness of Graph Neural Networks

### TL;DR

We study the structural fairness-aware node representation learning with graph neural networks.

### Abstract

Despite the success of graph neural networks (GNNs) in many downstream tasks, they are usually confronted with fairness issues when learning node representations. The fairness issues, or biases, often stem from the input to GNNs, including node attributes and the local graph context surrounding a node. While several recent approaches have been proposed to eliminate the bias rooted in sensitive attributes, they ignore the other key input of GNNs, namely local contexts. The local context of a node can introduce structural bias, since GNNs hinge on local contexts to generate node representations via the core operation of neighborhood aggregation. In particular, the dramatic differences in local contexts among individual nodes give rise to their diverse behaviors and biased outcomes. Although the structural fairness of GNNs is a significant issue, it has not been explored to date. In this paper, we propose a novel GNN framework called Structural Fairness-aware Graph Neural Network (StructFairGNN), for structurally fair node representation learning. Specifically, we employ a learnable debiasing function to generate  debiasing contexts, which modulate the layer-wise neighborhood aggregation to remove the structural bias in each layer of the GNN. Extensive experiments on several benchmark datasets demonstrates the effectiveness of our proposed model on both performance and fairness metrics.

## Sharp Impossibility Results for Hyper-graph Testing

### TL;DR

Propose to use a degree matching strategy to derive sharp lower bounds for hypergraph global testing in a broad degree-corrected mixed-membership (DCMM) non-uniform hypergraph setting. 

### Abstract

In a broad Degree-Corrected Mixed-Membership (DCMM) setting, we test whether a non-uniform hypergraph has only one community or has multiple communities. Since both the null and alternative hypotheses have many unknown parameters, 
the challenge is, given an alternative, how to identify the null that is hardest to separate from the alternative. We approach this by proposing a degree matching strategy where the main idea is leveraging the theory for tensor scaling to create a least favorable pair of hypotheses. We present a  result on standard  minimax lower bound theory and a result on Region of Impossibility (which is more informative than the minimax lower bound). We show that our lower bounds are tight by introducing a new test that attains the lower bound up to a logarithmic factor. We also discuss the case where the hypergraphs may have mixed-memberships.

## Graph Neural Networks with Non-Recursive MessagePassing

### TL;DR

None

### Abstract

Graph neural networks (GNNs) have become the de-facto standard for learning on graphs. The recursive message passing (RMP) mechanism, i.e., recursively aggregating messages from adjacent nodes layer by layer, has dominated the implementations of existing GNN models. While RMP is in line with the topological structures and could achieve good performance, in this paper, we show that such recursive operation is unnecessary. By getting rid of recursive calculations via a novel aggregation mechanism, non-recursive message passing models (nrecGNN) could be more efficient and effective in graph learning. We theoretically prove that nrecGNN shares the same expression capacity as its recursive counterpart in exploiting network structure. We propose to consider neighbors with the same order as a hop set and combine messages within each set to obtain hop-level representations. Then, the final embedding representation of a node could be explicitly obtained by aggregating all hop representations (non-recursively). By doing so, messages from different orders of neighbors could be extracted independently, and each node can directly access its multiple-hop neighbors via these hop sets. Systematic experiments on academic, social, and web datasets demonstrate the superiority of nrecGNN to state-of-the-art GNN models.

## Graph Transformer Network for Complex Action Recognition

### TL;DR

None

### Abstract

 This paper introduces a new method for complex action recognition from videos using Graph Transformer Network. We assume a complex action consists of a set of sub-actions that interact with each other over time. We first use existing temporal action localization methods to recognize the sub-actions, and then use the Transformer network to recognize the complex actions by utilizing the sub-action recognition results. With such a framework, we are able to exploit the power of the Transformer in analyzing sequential data. Moreover, by utilizing the self-attention mechanism, we can also well capture the long-range dependency and increase the computation efficiency. We propose a graph head to replace the traditional head of Transformer, which can well capture the structural information within the reception field. Our approach is validated on the Breakfast Actions, MultiTHUMOS dataset and Olympic Sports dataset and achieve superior performance to SOTA methods for complex action recognition.

## Fair Disaster Containment via Graph-Cut Problems

### TL;DR

None

### Abstract

Graph cut problems form a fundamental problem type in combinatorial optimization, and are a central object of study in both theory and practice.  In addition, the study of fairness in Algorithmic Design and Machine Learning has recently received significant attention, with many different notions proposed and analyzed in a variety of contexts. In this paper we initiate the study of fairness for graph cut problems by giving the first fair definitions for them, and subsequently we demonstrate appropriate algorithmic techniques that yield a rigorous theoretical analysis. Specifically, we incorporate two different definitions of fairness, namely demographic and probabilistic individual fairness, in a particular cut problem modeling disaster containment scenarios. Our results include a variety of approximation algorithms with provable theoretical guarantees.

## Computer Network Modeling with Graph Neural Networks

### TL;DR

The paper describes RoPhLNet, the winning solution of the GNN Challenge co-organized by the BNNC and the ITU (estimation of computer network performance).

### Abstract

Quickly and accurately evaluating the performance of a computer network topology is necessary to deploy and update software defined networks. We describe RoPhLNet, a new deep learning architecture for that task. The key ideas of the model include the use of three bipartite graphs to propagate information between routeurs, paths and links as well as the use of a concatenation of aggregation function. RoPhLNet achieved an error of 1.53% and won first place at the recent GNN Challenge, part of a challenge series by the ITU. We evaluate the components of our approach and highlight some open questions.

## Quantum machine learning of graph-structured data

### TL;DR

None

### Abstract

Graph structures are ubiquitous throughout the natural sciences. Here we develop a novel approach which exploits the quantum source's graph structure to improve learning via an arbitrary quantum neural network (QNN) ansatz. In particular, we devise and optimise a self-supervised objective to capture the information-theoretic closeness of the quantum states in the training of a QNN. Numerical simulations show that our approach improves the learning efficiency and the generalization behavior of the base QNN. On a practical note, scalable quantum implementations of the learning procedure described in this paper are likely feasible on the next generation of quantum computing devices.

## Accurately Solving Rod Dynamics with Graph Learning

### TL;DR

We introduce a novel method to accelerate iterative solvers for physical systems with graph networks by predicting the initial guesses to reduce the number of iterations.

### Abstract

Iterative solvers are widely used to accurately simulate physical systems. These solvers require initial guesses to generate a sequence of improving approximate solutions. In this contribution, we introduce a novel method to accelerate iterative solvers for rod dynamics with graph networks (GNs) by predicting the initial guesses to reduce the number of iterations. Unlike existing methods that aim to learn physical systems in an end-to-end manner, our approach guarantees long-term stability and therefore leads to more accurate solutions. Furthermore, our method improves the run time performance of traditional iterative solvers for rod dynamics. To explore our method we make use of position-based dynamics (PBD) as a common solver for physical systems and evaluate it by simulating the dynamics of elastic rods. Our approach is able to generalize across different initial conditions, discretizations, and realistic material properties. We demonstrate that it also performs well when taking discontinuous effects into account such as collisions between individual rods. Finally, to illustrate the scalability of our approach, we simulate complex 3D tree models composed of over a thousand individual branch segments swaying in wind fields.

## Perturb-and-max-product: Sampling and learning in discrete energy-based models

### TL;DR

Perturb-and-MAP combined with max-product yields better performance than Gibbs sampling or LP-based MAP methods on discrete EBMs

### Abstract

Perturb-and-MAP offers an elegant approach to approximately sample from a energy-based model (EBM) by computing the maximum-a-posteriori (MAP) configuration of a perturbed version of the model. Sampling in turn enables learning. However, this line of research has been hindered by the general intractability of the MAP computation. In fact, very few works venture outside tractable models, and when they do, they use linear-programming approaches, which as we will show, have several limitations. In this work we present perturb-and-max-product (PMP), a parallel and scalable mechanism for sampling and learning in discrete EBMs. Models can be arbitrary as long as they are built using tractable factors. We show that (a) for Ising models, PMP is orders of magnitude faster than Gibbs and Gibbs-with-Gradients (GWG) at learning and generating samples of similar or better quality; (b) PMP is able to learn and sample from RBMs; (c) in a large, entangled graphical model in which Gibbs and GWG fail to mix, PMP succeeds.

## Graph Learning with 1D Convolutions on Random Walks

### TL;DR

Graph learning with random walks and 1D CNNs.

### Abstract

We propose CRaWl (CNNs for Random Walks), a novel neural network architecture for graph learning. It is based on processing sequences of small subgraphs induced by random walks with standard 1D CNNs. Thus, CRaWl is fundamentally different from typical message passing graph neural network architectures. It is inspired by techniques counting small subgraphs, such as the graphlet kernel and motif counting, and combines them with random walk based techniques in a highly efficient and scalable neural architecture. We demonstrate empirically that CRaWl matches or outperforms state-of-the-art GNN architectures across a multitude of benchmark datasets for classification and regression on graphs.

## LineGraphCode : A Source Code Embedding Network that learns Memory, Instructions, and Data dependencies Jointly for Programming Education

### TL;DR

Graph Tree Convolutional Neural Networks with source code embedding task for Programming Education

### Abstract

Like many datasets, representation learning of source code is still struggling. In this paper, we propose LineGraphCode, which represents the source code through software analysis methodology and then learns embedding with a neural network architecture. The source code representation process proceeds by extracting the source code from the instructional information, memory information, and graph of data dependencies generated in each line. In addition, we introduce Graph Tree Convolutional neural networks (GTC), Graph Tree Attention neural networks (GTAs), and Graph Tree AutoEncoders (GTAEs), which are new networks that can jointly learn relationships among sibling nodes and hierarchical relationships. Supervised, unsupervised, and semi-supervised learning are conducted using source code dataset of algorithm. To extract memory information, we employ Debugger to extract patterns in which variables change; then, feature extraction is performed via a convolutional neural network (CNN). To extract instructional information, we employ Dissembler(dis) to extract the sequence of operations; then, feature extraction is performed via Word2Vector and CNN. To extract data dependencies, an abstract syntax tree (AST) is used to create a graph of data flow dependencies; then, all of this information is jointly learned through GTC, GTAs, and GTAEs. It does not learn information such as variable or function names, which are user-defined in natural language in the source code. It is also designed to show the same output if two lines of independent source code change and the information in the memory does not. This network can be used for programming education as a recommendation system that automatically provides the source codes of similar and different algorithms by clustering students' assignments in the coding test on the same problem.  We scrapped the algorithms written to solve sorting problems and collected 6 algorithms (bubble, insertion, merge, quick, selection, and shell). This dataset is trained with a network verified by the k-fold method, and the distribution of the code is visualized through T-SNE.

## Graph Convolutional Recurrent Networks for Reward Shaping in Reinforcement Learning

### TL;DR

This paper proposes a potenital-based reward shaping by combining Graph Convolutional Recurrent Network, Augmented krylov, and Look-ahead Advice.

### Abstract

Reward shaping techniques are used to speed learning in Reinforcement Learning (RL), which is vital for modern applications. Building a reward shaping function is challenging and still requires research efforts to further improve the performance. In this paper, we propose the use of Graph Convolutional Recurrent Network (GCRN) to form the reward shaping function and further improve existing solutions. GCRN combines the Graph Convolutional Network (GCN) and the Recurrent Neural Network (RNN). In RL, we apply GCRN on the Markov Decision Process (MDP) design to capture spatial dependence using GCN and temporal dependence using the Bi-Directional Gated Recurrent Units (BiGRU). We build the loss function of the GCRN based on the message passing technique using the probabilistic inference view of RL. Furthermore, we use the Augmented Krylov method to build an estimate of the MDP transition matrix and the Look-Ahead Advice technique to produce more precise reward shaping values. We show the improvement achieved by our solution compared to a recent work utilizing GCN with graph Laplacian on Atari 2600 and Mujoco games.

## Accelerating Graph Neural Network Training with Sparse Moving Averages of Aggregated Features

### TL;DR

None

### Abstract

Neighbor sampling is a commonly used technique for training Graph Neural Networks (GNNs) on large graphs. Previous work has shown that sampling-based GNN training can be considered as Stochastic Compositional Optimization (SCO) problems and can be better solved by SCO algorithms. However, we find that SCO algorithms are impractical for training GNNs on large graphs because they need to store the moving averages of the aggregated features of all nodes in the graph. The moving averages can easily exceed the GPU memory limit and even the CPU memory limit. In this work, we propose a variant of SCO algorithms with sparse moving averages for GNN training. By storing the moving averages in the most recent iterations, our algorithm only requires a fixed size buffer, regardless of the graph size. We show that our algorithm can achieve $O(\sqrt{1/K})$ convergence rate when the buffer size satisfies certain conditions. Our experiments validate our theoretical results and show that our algorithm outperforms the traditional Adam SGD for GNN training with a small memory overhead.

## Interpreting Graph Neural Networks via Unrevealed Causal Learning

### TL;DR

None

### Abstract

This paper proposes a new eXplanation framework, called OrphicX, for generating causal explanations for any graph neural networks (GNNs) based on learned latent causal factors. Specifically, we construct a distinct generative model and design an objective function that encourages the generative model to produce causal, compact, and faithful explanations. This is achieved by isolating the causal factors in the latent space of graphs by maximizing the information flow measurements. We theoretically analyze the cause-effect relationships in the proposed causal graph, identify node attributes as confounders between graphs and GNN predictions, and circumvent such confounder effect by leveraging the backdoor adjustment formula. Our framework is compatible with any GNNs, and it does not require access to the process by which the target GNN produces its predictions. In addition, it does not rely on the linear-independence assumption of the explained features, nor require prior knowledge on the graph learning tasks. Empirically, OrphicX significantly outperforms its alternatives, and indicate that our method can effectively identify the causal semantics for generating causal explanations.

## Transformers Generalize DeepSets and Can be Extended to Graphs & Hypergraphs

### TL;DR

We generalize Transformers to any-order hypergraphs and reduce their complexity to linear to input.

### Abstract

We present a generalization of Transformers to any-order permutation invariant data (sets, graphs, and hypergraphs). We begin by observing that Transformers generalize DeepSets, or first-order (set-input) permutation invariant MLPs. Then, based on recently characterized higher-order invariant MLPs, we extend the concept of self-attention to higher orders and propose higher-order Transformers for order-$k$ data ($k=2$ for graphs and $k>2$ for hypergraphs). Unfortunately, higher-order Transformers turn out to have prohibitive complexity $\mathcal{O}(n^{2k})$ to the number of input nodes $n$. To address this problem, we present sparse higher-order Transformers that have quadratic complexity to the number of input hyperedges, and further adopt the kernel attention approach to reduce the complexity to linear. In particular, we show that the sparse second-order Transformers with kernel attention are theoretically more expressive than message passing operations while having an asymptotically identical complexity. Our models achieve significant performance improvement over invariant MLPs in graph classification and set-to-graph prediction tasks, and are competitive to state-of-the-art graph neural networks.

## Topological Experience Replay for Fast Q-Learning

### TL;DR

We organize the experience replay buffer as a graph to accelerate the value propagation in Q-learning.

### Abstract

State-of-the-art deep Q-learning methods update Q-values using state transition tuples sampled from the experience replay buffer. Often this strategy is to randomly sample or prioritize data sampling based on measures such as the temporal difference (TD) error. Such sampling strategies are agnostic to the structure of the Markov decision process (MDP) and can therefore be data inefficient at propagating reward signals from goal states to the initial state. To accelerate reward propagation, we make use of the MDP structure by organizing the agent's experience into a graph. Each edge in the graph represents a transition between two connected states. We perform value backups via a breadth-first search that expands vertices in the graph starting from the set of terminal states successively moving backward. We empirically show that our method is substantially more data-efficient than several baselines on a diverse range of sparse reward tasks. Notably, the proposed method also outperforms baselines that have the advantage of a much larger computational budget. 

## Graph based Reinforcement Learning for Automatic Sequential Recommendation System Search

### TL;DR

None

### Abstract

Sequential Recommendation (SR) System emerged recently as a powerful tool for suggesting users with the next item of interest. Despite their great success, the design of SR systems requires heavy manual work and domain knowledge. In this paper, we present AutoSR, an effective AutoML tool that enables automatic design of powerful SR systems based on Graph Convolutional Network (GCN) and Reinforcement Learning (RL). In AutoSR, we summarize the design process of the SR system and extract effective operations from the existing SR systems to construct our search space. Such experience-based search space generates diverse SR systems by integrating effective operations of different systems, providing a basic condition for the implementation of AutoML. Besides, we propose a graph-based RL method to efficiently explore the SR search space, where operations have complex and diverse application conditions. Compared with the existing AutoML methods, which ignore potential relations among operations, AutoSR can greatly avoid invalid SR system design and efficiently discover more powerful SR systems by analyzing the relation graph of various operations. The final experimental results demonstrate the superiority and significance of AutoSR.

## Multi-Task Classification using a Cross-Task Graph Neural Network Decoder

### TL;DR

A novel GNN based decoder for Multi-Task Classification

### Abstract

The majority of work in the multi-task classification in the computer vision domain focuses on improving the encoder part of the architecture or better utilizing the task gradients in the optimization loop. However, the effect of introducing cross-task information in the decoder part of the architecture is less well understood.  We present a novel decoder-focused multi-task classification architecture Cross-Task Graph Neural Network (CT-GNN), which refines the disjointed per-task predictions using cross-task information. The CT-GNN architecture extends the traditional disjointed task-heads decoder, by utilizing a cross-task graph and unique class node embeddings. The cross-task graph can either be determined a priori based on the conditional probability between the task classes or determined dynamically using self-attention. CT-GNN can be added to any backbone and trained end-to-end at a small increase in the parameter count. We evaluate the CT-GNN on the challenging CelebA and Sewer-ML multi-task classification datasets, and show that the CT-GNN achieves comparable or better performance than prior optimization methods and encoder-focused architectures.

## Learning to Pool in Graph Neural Networks for Extrapolation

### TL;DR

Proposed a learnable pooling function that enables graph neural networks extrapolate well on various tasks

### Abstract

Graph neural networks (GNNs) are one of the most popular approaches to using deep learning on graph-structured data, and they have shown state-of-the-art performances on a variety of tasks. However, according to a recent study, a careful choice of pooling functions, which are used for the aggregation or readout operation in GNNs, is crucial for enabling GNNs to extrapolate. Without the ideal combination of pooling functions, which varies across tasks, GNNs completely fail to generalize to out-of-distribution data, while the number of possible combinations grows exponentially with the number of layers. In this paper, we present GNP, a $L^p$ norm-like pooling function that is trainable end-to-end for any given task. Notably, GNP generalizes most of the widely-used pooling functions.   We verify experimentally that simply replacing all pooling functions with GNP enables GNNs to extrapolate well on many node-level, graph-level, and set-related tasks; and GNP sometimes performs even better than optimal combinations of existing pooling functions.

## Multi-Scale Representation Learning on Proteins

### TL;DR

Learning multi-scale protein representations.

### Abstract

Proteins are fundamental biological entities mediating key roles in cellular function and disease. This paper introduces a multi-scale graph construction of a protein –HoloProt– connecting surface to structure and sequence. The surface captures coarser details of the protein, while sequence as primary component and structure –comprising secondary and tertiary components– capture finer details. Our graph encoder then learns a multi-scale representation by allowing each level to integrate the encoding from level(s) below with the graph at that level. We test the learned representation on different tasks, (i.) ligand binding affinity (regression), and (ii.) protein function prediction (classification).
On the regression task, contrary to previous methods, our model performs consistently and reliably across different dataset splits, outperforming all baselines on most splits. On the classification task, it achieves a performance close to the top performing model while using 10x fewer parameters. To improve the memory efficiency of our construction, we segment the multiplex protein surface manifold into molecular superpixels, and substitute the surface with these superpixels at little to no performance loss.

## Multi-fidelity Stability and Sparse Graph Representation Learning

### TL;DR

We give a new form of stability to guarantee PAC learnability of Graph Representation Learning

### Abstract

Graph representation learning (GRL for short) is different from the classical methods in structured predictions by assuming that the hypothesis decides the label of a vertex based on a set of its neighborhood's features instead of only its own. We proposed a weaker form of uniform stability termed multi-fidelity stability to give the learning guarantee of GRL on inductive structured prediction. We also proved that ~\citet{london2016stability}'s observation on the unique 1-sample generalization when the graph is sparse. Besides, we provide a non-asymptotic bound for the multi-fidelity stability on networks trained with stochastic gradient descent (SGD for short) and a sharp bound for the 1-layer GNN. We also present discrepancy lower-bound justifying that multi-fidelity is non-trivial in non-invariant GNN. Our bound revealed that the generalization depends on the average degree while the previous bound on i.i.d. graph classification/regression only yields upper bounds based on the maximum degree. In the last, we exemplify our theoretical result through a learning guarantee of sparse model selection, shedding light on future empirical research.

## Unsupervised Causal Representation Learning via Structural Causal Variational Autoencoder

### TL;DR

None

### Abstract

Causal representation learning can be seen as a generalisation of disentangled representation learning. It aims to discover high-level causal latent variables from low-level observations, and has shown significant abilities for improving a set of downstream tasks of interest. Recent work has shown promising results in supervised causal representation learning, e.g., known causal graphs or labels. In many applications, however, obtaining such high-cost prior knowledge is often proved to be difficult. This work presents a novel model for causal representation learning in a completely unsupervised manner, termed Structural caUsAl Variational autoEncoder (SuaVE), which incorporates a novel additive Gaussian nonlinear structural causal model as prior to the nodes and structure of the latent causal graph. The proposed method directly learns the causal graph relying on a general causal supergraph in latent space and thus can discover an accurate causal graph together with the latent variables by incorporating a sparsity constraint. Experiments on two datasets show the ability of the proposed model to capture causal relations between latent variables in an unsupervised setting. 


## Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery

### TL;DR

We developed Distilled Graph Attention Policy Networks (DGAPNs) for the inverse design of molecules --- antivirals against SARS-CoV-2.

### Abstract

We developed Distilled Graph Attention Policy Networks (DGAPNs), a curiosity-driven reinforcement learning model to generate novel graph-structured chemical representations that optimize user-defined objectives by efficiently navigating a physically constrained domain. The framework is examined on the task of generating molecules that are designed to bind, noncovalently, to functional sites of SARS-CoV-2 proteins. We present a spatial Graph Attention Network (sGAT) that leverages self-attention over both node and edge attributes as well as encoding spatial structure --- this capability is of considerable interest in areas such as molecular and synthetic biology and drug discovery. An attentional policy network is then introduced to learn decision rules for a dynamic, fragment-based chemical environment, and state-of-the-art policy gradient techniques are employed to train the network with enhanced stability. Exploration is efficiently encouraged by incorporating innovation reward bonuses learned and proposed by random network distillation. In experiments, our framework achieved outstanding results compared to state-of-the-art algorithms, while increasing the diversity of proposed molecules and reducing the complexity of paths to chemical synthesis.

## Differentiable Unsupervised Feature Selection based on a Gated Laplacian

### TL;DR

None

### Abstract

Scientific observations may consist of a large number of variables (features). Selecting a subset of meaningful features is often crucial for identifying patterns hidden in the ambient space. In this paper, we present a method for unsupervised feature selection, and we demonstrate its advantage in clustering, a common unsupervised task. We propose a differentiable loss that combines a graph Laplacian-based score that favors low-frequency features with a gating mechanism for removing nuisance features. Our method improves upon the naive graph Laplacian score by replacing it with a gated variant computed on a subset of low-frequency features. We identify this subset by learning the parameters of continuously relaxed Bernoulli variables, which gate the entire feature space. We mathematically motivate the proposed approach and demonstrate that it is crucial to compute the graph Laplacian on the gated inputs rather than on the full feature set in the high noise regime. Using several real-world examples, we demonstrate the efficacy and advantage of the proposed approach over leading baselines.

## ChemVec: predicting polypharmacy side effects for newly developed drugs

### TL;DR

None

### Abstract

Polypharmacy is the administration of two or more drugs simultaneously. It has demonstrated effectiveness in treating many complex diseases but has a higher risk of adverse drug reactions. Hence, the prediction of polypharmacy side effects is an essential step in drug testing, especially for newly developed drugs. This paper shows that the current knowledge graph (KG) based state-of-the-art approach to polypharmacy side effect prediction does not work well for new drugs, as they have a low number of known connections in the knowledge graph. We propose a new method - ChemVec, that solves this problem by enhancing the knowledge graph structure with a chemistry-aware node initialization and weighted drug similarity edges. We also devise a new 3-step learning process, which iteratively updates node embeddings related to side effects edges, similarity edges, and drugs with limited knowledge. Our model significantly outperforms existing KG-based models. Additionally, we examine the problem of negative relations generation and show that the cache-based approach works best for polypharmacy tasks.

## Diverse and Natural Fashion Compatibility Retrieval

### TL;DR

A method to retrieval natural and diverse compatibility recommendations by constructing and learning through a bi-partite compatibility graph.

### Abstract

Recommending compatible garments is a valuable yet challenging problem for fashion retailers. Existing compatibility retrieval methods produce results that are homogenous because their item representations lack rich contextual information. Other context-aware approaches are not suitable for large-scale retrieval and cannot operate with arbitrary products outside of the predetermined compatibility graph.
We propose a novel framework to retrieve complementary items that are diverse and natural by building item representations that aggregate compatibility information. We first construct a bipartite graph with garments and outfits (contexts) such that an item is linked with many outfits it is compatible with. We learn a network to parse the graph neighborhood of each item and produce an embedding that captures its diverse set of compatibility contexts. Our framework can retrieve compatible items from a variety of contexts efficiently and operate with arbitrary garments previously not in the network. Extensive studies on large-scale real-world datasets demonstrate that our retrieval results are more natural and appealing, and can better meet the golden standard of fashion experts.

## RCGNN:Traffic Forecasting Based on Graph Nodes Spatiotemporal Relationship reConstruction

### TL;DR

None

### Abstract

Different temporal patterns and complex spatial influence relationships of traffic data bring challenges in traffic forecasting. In this paper, we propose Relationship reConstruction Graph Neural Network (RCGNN) to construct the static similarity relationship of road temporal patterns and the dynamic relationship of road spatial influence, which can achieve fine-grained traffic forecasting. 1) Graph Convolution Kernel Generation module (GCKG) is designed to discover static similarities in road temporal patterns and construct independent convolution parameters to process similar data. 2) Dynamic Relationship Reconstruction Graph Convolution Recurrent Network Module (RGCRN) is designed to capture the dynamic spatial influence relationships of nodes, and reconstruct a real-time adjacency matrix. RGCRN also captures the characteristics of observation data according to the constructed independent convolution parameters and the real-time adjacency matrix for fine-grained traffic forecasting. Experiment results show that our model outperforms the latest baselines by at least 0.87%, 1.07% and 1.29%, 0.7% on two real traffic datasets and two unstructured public datasets. Ablation experiments and adjacency matrix analysis also demonstrate the effectiveness of the model structure and node relationship reconstructing.

## SEEN: Sharpening Explanations for Graph Neural Networks using Explanations from Neighborhoods

### TL;DR

We propose a novel method to improve explanation quality for node classification tasks which can be used in post-hoc manner through aggregation of auxiliary explanations from important neighboring nodes.

### Abstract

Explaining the foundations for predictions obtained from graph neural networks (GNNs) is critical for credible use of GNN models for real-world problems. Owing to the rapid growth of GNN applications, recent progress in explaining predictions from GNNs, such as sensitivity analysis, perturbation methods, and attribution methods, showed great opportunities and possibilities for explaining GNN predictions. In this study, we propose a method to improve the explanation quality of node classification tasks that can be applied in a post hoc manner through aggregation of auxiliary explanations from important neighboring nodes, named SEEN. Applying SEEN does not require modification of a graph and can be used with diverse explainability techniques due to its independent mechanism. Experiments on matching motif-participating nodes from a given graph show great improvement in explanation accuracy of up to 12.71% and demonstrate the correlation between the auxiliary explanations and the enhanced explanation accuracy through leveraging their contributions. SEEN provides a simple but effective method to enhance the explanation quality of GNN model outputs, and this method is applicable in combination with most explainability techniques.

## On the Normalized Algebraic Connectivity of Graham’s Exponential Random Graph Model

### TL;DR

For Graham's ERGM, we give for the first time a dimension-free lower bound for its normalized algebraic connectivity.

### Abstract

This paper studies the nonasymptotic behavior of the second smallest eigenvalue in the normalized Laplacian matrix corresponding to Graham (2017)’s exponential random graph model that has recently received much interest in the network formation community. To this end, we establish for the first time a dimension-free probabilistic lower bound that is shown to hold with high probability as the number of the nodes in the graph increases to infinity. Implications to network analysis problems are then discussed to press the importance of the developed theory. We further provide simulation results to back up the established theory.

## Distributed Learning of Generalized Linear Causal Networks

### TL;DR

None

### Abstract

We consider the task of learning causal structures from data stored across different physical locations, and propose a novel structure learning method called distributed annealing on regularized likelihood score (DARLS) to solve this problem. DARLS searches over the space of topological sorts for a causal graph that has a high regularized likelihood score using annealing strategy, where the optimal graphical structure compatible with a sort is found by a distributed optimization method. We establish the convergence of the distributed optimization method to a global optimizer of the overall score computed on all data across local machines. To the best of our knowledge, DARLS is the first method using distributed data to learn causal structures. In our simulation studies, DARLS has demonstrated competing performance with distributed data against other existing methods using pooled data across local machines. DARCS also exhibits higher predictive power than other methods in a real-world application for modeling protein-DNA binding networks using ChIP-Sequencing data.


## Learned Predictability of Nodes in Network Time Series with Applications to Graph Sampling

### TL;DR

None

### Abstract

This paper deals with statistical analysis of time-dependent graph signals that are collected over a redundant network. For the purpose of understanding the distribution of this redundancy over the nodes of the network, we introduce the notion of learned predictability of nodes  that relies on reconstruction methods from graph signal processing. Intuitively, the higher the learned predictability of a node, the more redundant it is. We compare various supervised learning methods for reconstructing the signal at a given set of nodes from observations at other nodes that respectively use linear, kernel, and neural network predictors. For each method, we derive an algorithm which provides a ranking of the nodes with respect to their respective learned predictability. As an application of the ranking, we consider the problem of selecting an optimal sampling set of sensors for the purpose of signal recovery at non-observed sensors with a minimal reconstruction error. We compare this data-driven strategy to turn off a fixed number of sensors (or equivalently to select a sampling set of nodes) to standard sampling methods in graph signal processing that are not data-driven. To illustrate the performances of our approach, we report numerical experiments on the analysis of a real data which represents the bike sharing network in the city of Paris. The numerical results show the advantage of a data-driven approach for sampling signals over graphs. Moreover, we compare the distribution of learned predictability with the variable density sampling distribution in graph signal processing. Both strategies tend to sample the remote nodes located in the suburbs.

## Graph Neural Networks for Link Prediction: A Theoretical Perspective

### TL;DR

None

### Abstract

In this paper, we theoretically characterize graph neural network's representation power for high-order node set prediction problems (where a prediction is made over a set of more than 1 node). In particular, we focus on one most important second-order task---link prediction. There are two representative classes of GNN methods for link prediction: GAE and SEAL. GAE (Graph Autoencoder) first applies a GNN to the whole graph, and then aggregates the representations of the source and target nodes as their link representation. SEAL extracts a subgraph around the source and target nodes, labels the nodes in the subgraph, and then uses a GNN to learn a link representation from the labeled subgraph. At first glance, both GAE and SEAL use a GNN. However, their performance gap can be very large. On the recent Open Graph Benchmark datasets, SEAL achieved 3 first places out of 4 datasets, outperforming the best GAE method by up to 195% in Hits@100. In this paper, by studying this performance gap between GAE and SEAL, we first point out a key limitation of GAE caused by directly aggregating two node representations as a link representation. To address this limitation, we propose the labeling trick. Labeling trick unifies several recent successes to improve GNNs' representation power, such as SEAL, Distance Encoding, and Identity-aware GNN, into a single and most basic form. We prove that with labeling trick a sufficiently expressive GNN can learn the most expressive structural representations for node sets. Our work establishes a theoretical foundation for using GNNs for high-order node set prediction.

## INDIGO: GNN-Based Inductive Knowledge Graph Completion Using Pair-Wise Encoding

### TL;DR

We propose a novel GNN-based approach with pair-wise encoding for inductive knowledge graph completion problem.

### Abstract

The aim of knowledge graph (KG) completion is to extend an incomplete KG with missing triples. Popular approaches based on graph embeddings typically work by first representing the KG in a vector space, and then applying a predefined scoring function to the resulting vectors to complete the KG. These approaches work well in transductive settings, where predicted triples involve only constants seen during training; however, they are not applicable in inductive settings, where the KG on which the model was trained is extended with new constants or merged with other KGs. The use of Graph Neural Networks (GNNs) has recently been proposed as a way to overcome these limitations; however, existing approaches do not fully exploit the capabilities of GNNs and still rely on external heuristics and ad-hoc scoring functions. In this paper, we propose a novel approach, where the KG is fully encoded into a GNN in a transparent way, and where the predicted triples can be read out directly from the last layer of the GNN without the need for additional components or scoring functions. Our experiments show that our model outperforms state-of-the-art approaches on inductive KG completion benchmarks.

## Generalized Aggregation Functions for Deep Resiudal Graph Neural Networks

### TL;DR

We propose generalized aggregation for deep residual GNNs and show it achieves state-of-the-art results in several benchmarks from OGB across tasks and domains.

### Abstract

Graph Neural Networks (GNNs) have been drawing significant attention to the power of representation learning on graphs. Recent works developed frameworks to train deep GNNs. Such works show impressive results in tasks like point cloud classification and segmentation, and protein interaction prediction. In this work, we study the performance of such deep models in large-scale graph datasets from the Open Graph Benchmark (OGB). In particular, we look at the effect of adequately choosing an aggregation function and its effect on final performance. Common choices of aggregation are mean, max, and sum. It has been shown that GNNs are sensitive to such aggregations when applied to different datasets. We further validate this point on large-scale graphs and propose to alleviate it by introducing a novel Generalized Aggregation Function. Our new aggregation functions not only covers all commonly used ones, but also can be customized for different tasks. Our generalized aggregation functions are fully differentiable, and thus their parameters can be learned in an end-to-end fashion. We add our generalized aggregation into deep residual GNNs and show it achieves state-of-the-art results in several benchmarks from OGB across tasks and domains.

## Variational Causal Autoencoder for Interventional and Counterfactual Queries

### TL;DR

We propose the Variational Causal Autoencoder (VCAUSE), a novel class of variational graph autoencoders (VGAE), for causal inference in the absence of hidden confounders when only observational data and the causal graph are available .

### Abstract

We propose the Variational Causal Autoencoder (VCAUSE), a novel class of variational graph autoencoders for causal inference in the absence of hidden confounders, when only observational data and the causal graph are available. Without making any structural assumptions, VCAUSE mimics the necessary properties of a Structural Causal Model (SCM) to provide a framework  for performing interventions (do-operator) and  abduction-action-prediction steps. As a result, and as shown by our empirical results, VCAUSE provides a practical and accurate pipeline for estimating the interventional and counterfactual distributions of diverse SCMs. Finally, we apply VCAUSE to evaluate counterfactual fairness in classification problems and also to learn accurate and fair classifiers.

## Perturbation based molecular attributions using Graph Convolutional Networks

### TL;DR

None

### Abstract

Since the introduction of artificial intelligence in medicinal chemistry (Med Chem), the necessity has emerged to analyse how molecular property variation is modulated by either single atoms or chemical groups. In this paper, we propose a perturbation based approach to train Graph-to-Graph Neural Network (GGN) using semi-supervised learning for attributing structure-property relationships. As initial case studies the method was applied to solubility and molecular acidity while checking its consistency in comparison with known experimental chemical data. As final goal, the GGN approach could represent a valuable Med Chem tool aimed to deal with problems such as activity cliffs, lead optimization and de-novo drug design.

## Decoupling the Depth and Scope of Graph Neural Networks

### TL;DR

We improve the expressive power and scalability of existing GNN architectures by decoupling the model depth and receptive fields. 

### Abstract

State-of-the-art Graph Neural Networks (GNNs) have limited scalability with respect to the graph and model sizes. On large graphs, increasing the model depth often means exponential expansion of the receptive field (i.e., scope). Beyond just a few layers, two fundamental challenges emerge: 1. degraded expressivity due to oversmoothing, and 2. expensive computation due to neighborhood explosion. We propose a design principle to decouple the depth and scope of GNNs -- to generate representation of a target entity (i.e., a node or an edge), we first extract a localized subgraph as the bounded-size receptive field, and then apply a GNN of arbitrary depth on top of the subgraph. A properly extracted subgraph consists of a small number of critical neighbors, while excluding irrelevant ones. The GNN, no matter how deep it is, smooths the local signals into informative representation without oversmoothing the global graph  into ''white noise''. Theoretically, such decoupling improves the expressive power of popular GNN architectures, including GCN, GraphSAGE and GIN. Empirically, on seven graphs (up to 110M nodes) and six backbone GNN architectures, our design achieves state-of-the-art accuracy with an order of magnitude reduction in computation and hardware cost. 

## Learning to Schedule Learning rate with Graph Neural Networks

### TL;DR

We propose a novel Graph-Network-based Scheduler (GNS), aiming at obtaining a learning rate scheduler without restrictions to existing principles.

### Abstract

Recent decades have witnessed great development of stochastic optimization in training deep neural networks. Learning rate scheduling is one of the most important factors that influence the performance of stochastic optimizers like Adam. Traditional methods seek to find a relatively proper scheduling among a limited number of pre-defined rules and might not accommodate a particular target problem. Instead, we propose a novel Graph-Network-based Scheduler (GNS), aiming at learning a specific scheduling mechanism without restrictions to existing principles. By constructing a directed graph for the underlying neural network of the target problem, GNS encodes current dynamics with a graph message passing network and trains an agent to control the learning rate accordingly via reinforcement learning. The proposed scheduler can capture the intermediate layer information while being able to generalize to problems of varying scales. Besides, an efficient reward collection procedure is leveraged to speed up training. We evaluate our framework on benchmarking datasets, Fashion-MNIST and CIFAR10 for image classification, and GLUE for language understanding. GNS shows consistent improvement over popular  hand-crafted learning rate schedulers and a learning-based counterpart when training ResNet (CNN) and RoBERTa (Transformer) models.

## Detecting Modularity in Deep Neural Networks

### TL;DR

Neural networks show signs of modularity.

### Abstract

A neural network is modular to the extent that parts of its computational graph (i.e. structure) can be represented as performing some comprehensible subtask relevant to the overall task (i.e. functionality). Are modern deep neural networks modular? How can this be quantified? In this paper, we consider the problem of assessing the modularity exhibited by a partitioning of a network's neurons. We propose two proxies for this: importance, which reflects how crucial sets of neurons are to network performance; and coherence, which reflects how consistently their neurons associate with features of the inputs. To measure these proxies, we develop a set of statistical methods based on techniques conventionally used to interpret individual neurons. We apply the proxies to partitionings generated by spectrally clustering a graph representation of the network's neurons with edges determined either by network weights or correlations of activations. We show that these partitionings, even ones based only on weights (i.e. strictly from static analysis), reveal groups of neurons that are consistently important and coherent. These results suggest that graph-based partitioning can reveal modularity and help us understand how deep neural networks function.


## MoReL: Multi-omics Relational Learning

### TL;DR

None

### Abstract

Multi-omics data analysis has the potential to discover hidden molecular interactions, revealing potential regulatory and/or signal transduction pathways for cellular processes of interest when studying life and disease systems. One of critical challenges when dealing with real-world multi-omics data is that they may manifest heterogeneous structures and data quality as often existing data may be collected from different subjects under different conditions for each type of omics data. We propose a novel deep Bayesian generative model to efficiently infer a multi-partite graph encoding molecular interactions across such heterogeneous views, using a fused Gromov-Wasserstein (FGW) regularization between latent representations of corresponding views for integrative analysis. With such an optimal transport regularization in the deep Bayesian generative model, it not only allows incorporating view-specific side information, either with graph-structured or unstructured data in different views, but also increases the model flexibility with the distribution-based regularization. This allows efficient alignment of heterogeneous latent variable distributions to derive reliable interaction predictions compared to the existing point-based graph embedding methods. Our experiments on several real-world datasets demonstrate enhanced performance of MoReL in inferring meaningful interactions compared to existing baselines.

## DEGREE: Decomposition Based Explanation for Graph Neural Networks

### TL;DR

We propose a new decomposition based explanation for Graph Neural Networks.

### Abstract

Graph Neural Networks (GNNs) have received extensive attention due to their applicability to various domains where data is represented as graphs. However, GNNs are usually used as black-boxes, which prevents users from understanding the models. The lack of explanation could affect the application of GNNs in certain domains where black-box models are not fully trusted. Existing explanation methods such as surrogate based methods may suffer from faithfulness issues, while perturbation based methods could trigger unnatural artifacts in data. To tackle the problem, we propose DEGREE (Decomposition based Explanation for GRaph nEural nEtworks) to provide a faithful explanation for GNN predictions. Through decomposition, we could extract the relevant component and irrelevant component at each GNN layer, where the relevant component is expected to contribute to the output mostly. We show that DEGREE is applicable to those commonly used modules in GNNs. Instead of simply finding important node individuals, we also propose a subgraph-level explanation to consider the relation between nodes. We target both node classifiers and graph classifiers for the explanation. Quantitative and qualitative experiments on both synthetic and real-world datasets are conducted to demonstrate the effectiveness of DEGREE.

## RIM: Reliable Influence-based Active Learning on Graphs

### TL;DR

The first attempt to consider label noise in graph-based active learning.

### Abstract

Message passing is the core of most graph models such as Graph Convolutional Network (GCN) and Label Propagation (LP), which usually require a large number of clean labeled data to smooth out the neighborhood over the graph. However, the labeling process can be tedious, costly, and error-prone in practice. In this paper, we propose to unify active learning (AL) and message passing towards minimizing labeling costs, e.g., making use of few and unreliable labels that can be obtained cheaply. We make two contributions towards that end. First, we open up a novel perspective by drawing a connection between AL enforcing message passing and social influence maximization, ensuring that the selected samples effectively improve the model performance. Second, we propose an extension to the influence model that incorporates an explicit quality factor to model label noise. In this way, we derive a fundamentally new AL selection criterion for GCN and LP--reliable influence maximization (RIM)--by considering both quantity and quality of influence simultaneously. Empirical studies on public datasets demonstrate that RIM significantly outperforms the state-of-the-art AL methods in terms of accuracy and efficiency. 

## Generative Neural Architecture Search

### TL;DR

None

### Abstract

Neural architecture search (NAS) aims to automate the design of neural network architectures with searching algorithms. NAS works typically heavily rely on human's prior knowledge and suffer from search space and marginal improvements against random search. Recent works have demonstrated that the search space constrained by the existing works may have been over-restricted.
For example, recent works observed that even a randomly wired neural network architecture can achieve surprisingly competitive performance.  This stimulates us toward new methods that can explore much larger space with high efficiency and high expressiveness to identify the distribution of neural networks with ``good architectures''. To remedy this,  this paper formalizes a new framework named generative neural architecture search (gNAS) that learns the latent representation of graph-structured neural architecture and simultaneously identifies the ``sweet regions'' in the low-dimensional space much more efficiently than in original graph space. New optimization algorithms have been proposed to jointly learn the graph representation learning models and the neural network with candidate architectures. Theoretical merits such as the expressiveness of gNAS frameworks have also been analyzed. Extensive experiments on multiple benchmarks have demonstrated the superiority of our method. 


## Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss

### TL;DR

We propose a novel theoretical framework for studying self-supervised learning algorithms.

### Abstract

Recent works in self-supervised learning have advanced the state-of-the-art by relying on the contrastive learning paradigm, which learns representations by pushing positive pairs, or similar examples from the same class, closer together while keeping negative pairs far apart. Despite the empirical successes, theoretical foundations are limited -- prior analyses assume conditional independence of the positive pairs given the same class label, but recent empirical applications use heavily correlated positive pairs (i.e., data augmentations of the same image). Our work analyzes contrastive learning without assuming conditional independence of positive pairs using a novel concept of the augmentation graph on data.  Edges in this graph connect augmentations of the same data, and ground-truth classes naturally form connected sub-graphs. We propose a loss that performs spectral decomposition on the population augmentation graph and can be succinctly written as a contrastive learning objective on neural net representations. Minimizing this objective leads to features with provable accuracy guarantees under linear probe evaluation. By standard generalization bounds, these accuracy guarantees also hold when minimizing the training contrastive loss. In all, this work provides the first provable analysis for contrastive learning where the guarantees can apply to realistic empirical settings.

## GRAPHIX: A Pre-trained Graph Edit Model for Automated Program Repair

### TL;DR

We present a pre-trained graph edit model for automatically detecting and fixing bugs and code quality issues in Java programs.

### Abstract

We present GRAPHIX, a pre-trained graph edit model for automatically detecting and fixing bugs and code quality issues in Java programs. Unlike sequence-to- sequence models, GRAPHIX leverages the abstract syntax structure of code and represents the code using a multi-head graph encoder. Along with an autoregressive tree decoder, the model learns to perform graph edit actions for automated program repair. We devise a novel and efficient pre-training strategy for GRAPHIX, namely deleted sub-tree reconstruction, to enrich the model with implicit knowledge of program structures from unlabeled source code. The pre-training objective is made consistent with the bug fixing task to facilitate the downstream learning. We pre-train GRAPHIX on the CodeSearchNet dataset and fine-tune it on the Patches in The Wild Java benchmark. Experiments show that GRAPHIX achieves the state-of-the- art performance with the abstract code representation and significantly outperforms a wide range of pre-trained Transformer models including CodeBERT and PL-BART despite using one order of magnitude less parameters. Further analysis demonstrates strong inductive biases of GRAPHIX in learning meaningful structural and semantic code patterns, both in abstract and concrete source code.

## GAMER: A global-view graph transformer self-attention network

### TL;DR

None

### Abstract

Classic graph neural networks (GNNs) are mainly designed based on the assumption of local homophily, and they usually do not perform well on heterophilic graphs. Recently, transformer-like GNN models employ self-attention mechanism to learn more informative representations of nodes, but the dependency between nodes captured in these models is usually limited in local connectivity. To overcome the limitation of local connectivity and capture long-range dependency between distant nodes, in this paper we propose a global-view graph transformer self-attention network. Instead of passing the whole graph to self-attention computation, our framework samples a comprehensive context for each node to learn its representation. In particular, the node context is generated by developing several intimacy strategies to capture connectional, structural, and feature-based dependency in a global view. Furthermore, a multi-channel transformer framework is proposed to integrate multiple node contexts for representation learning in a more flexible way. The proposed framework is validated on six real-world public benchmarks in comparison with several state-of-the-art baseline algorithms, and the results illustrate its excellent performance on both homophilic and heterophilic graphs.

## Jointly Attacking Graph Neural Network and its Explanations

### TL;DR

None

### Abstract

Graph Neural Networks (GNNs) have boosted the performance for many graph-related tasks. Despite the great success, recent studies have shown that GNNs are highly vulnerable to adversarial attacks, where adversaries can mislead the GNNs' prediction by modifying graphs. On the other hand, the explanation of GNNs (GNNExplainer) provides a better understanding of a trained GNN model by generating a small subgraph and features that are most influential for its prediction. In this paper, we first perform empirical studies to validate that GNNExplainer can act as an inspection tool and have the potential to detect the adversarial perturbations for graphs. This finding motivates us to further initiate a new problem investigation: Whether a graph neural network and its explanations can be jointly attacked by modifying graphs with malicious desires? It is challenging to answer this question since the goals of adversarial attacks and bypassing the GNNExplainer essentially contradict each other. In this work, we give a confirmative answer to this question by proposing a novel attack framework (GEAttack), which can attack both a GNN model and its explanations by simultaneously exploiting their vulnerabilities. Extensive experiments on various real-world datasets demonstrate the effectiveness of the proposed method.

## BranchNet: Inverse Procedural Modeling with Hierarchical Graph Networks

### TL;DR

Using Hierarchical Graph Networks to implement the Procedural Modeling 

### Abstract


Research on modeling trees and plants has received a considerable amount of attention in recent years. Early procedural tree modeling can be categorized into four principal classes: rule-based algorithms, repetitive patterns, cellular automata, and particle systems. The level of realism produced by these methods is very high; however, creating millions of varied tree datasets manually is not logistically possible, even for professional 3D modeling artists. Trees created using these previous methods are typically static and the controllability of these procedural tree models is low. Deep generative models can learn to generate any class of shape automatically, making it possible to create 3D models at a large scale. In this paper, we introduce a novel deep generative model that generates 3D (botanical) tree models, which are not only edible but also have diverse shapes. Our proposed network denoted BranchNet, trains the tree branch structures on a hierarchical Variational Autoencoder (VAE) that learns new generative model structures. By directly encoding shapes into a hierarchy graph, BranchNet can generate diverse, novel, and realistic tree structures. To assist the creation of tree models, we create a domain-specific language with a GUI for modeling 3D shape structures, in which the continuous parameters can be manually edited in order to produce new tree shapes. The trees are interpretable and the GUI can be edited to capture the subset of shape variability.


## Projection-free Graph-based Classifier Learning using Gershgorin Disc Perfect Alignment

### TL;DR

A fast projection-free method by solving a sequence of linear programs for semi-supervised graph-based binary classifier learning

### Abstract

In semi-supervised graph-based binary classifier learning, a subset of known labels $\hat{x}_i$ are used to infer unknown labels, assuming that the label signal $\x$ is smooth with respect to a similarity graph specified by a Laplacian matrix.
When restricting labels $x_i$ to binary values, the problem is NP-hard.
While a conventional semi-definite programming (SDP) relaxation can be solved in polynomial time using, for example, the alternating direction method of multipliers (ADMM), the complexity of iteratively projecting a candidate matrix $\M$ onto the positive semi-definite (PSD) cone ($\M \succeq 0$) remains high.
In this paper, leveraging a recent linear algebraic theory called Gershgorin disc perfect alignment (GDPA), we propose a fast projection-free method by solving a sequence of linear programs (LP) instead.
Specifically, we first recast the SDP relaxation to its SDP dual, where a feasible solution $\H \succeq 0$ can be interpreted as a Laplacian matrix corresponding to a balanced signed graph sans the last node.
To achieve graph balance, we split the last node into two that respectively contain the original positive and negative edges, resulting in a new Laplacian $\bar{\H}$.
We repose the SDP dual for solution $\bar{\H}$, then replace the PSD cone constraint $\bar{\H} \succeq 0$ with linear constraints derived from GDPA---sufficient conditions to ensure $\bar{\H}$ is PSD---so that the optimization becomes an LP per iteration.
Finally, we extract predicted labels from our converged LP solution $\bar{\H}$. 
Experiments show that our algorithm enjoyed a $40\times$ speedup on average over the next fastest scheme while retaining comparable label prediction performance.

## Collaborative Graph Neural Networks for Unsupervised Representation Learning

### TL;DR

None

### Abstract

Graph neural networks (GNNs) have been intensively applied to analyze real-world networks, especially attributed networks. 
Existing GNNs studies mainly focus on exploiting network structures, while the exploitation of node attributes is rather limited as they only serve as node representations at the initial layer. Other than that, the training objectives of most GNNs also do not include node attributes. They perform training based on network structures in an unsupervised manner or the given node labels. However, node attributes are a major information source and need to be better exploited, as they are highly correlated with and complementary to the network. Thus, it is encouraging to deeply involve node attributes in the key components of GNNs, including the graph convolution operations and training objectives. But this is a nontrivial task since an appropriate way of integrating is required to maintain the merits of GNNs. Additionally, node attributes such as user posts are distinct from topological structures and not in line with graph convolutions. To bridge the gap, in this paper, we focus on unsupervised learning and propose COllaborative graph Neural Networks (CONN). It refines GNNs from two aspects. First, a collaborative aggregation mechanism is designed. It updates the representation of each node by aggregating the representations of not only its neighbors, but also its attribute categories. Second, a collaborative training objective is developed. It assesses both node-to-node and node-to-attribute-category interactions based on cross correlations. Experiments on real-world networks show that CONN consistently outperforms state-of-the-art embedding algorithms. It even outperforms an end-to-end baseline on social networks.

## Pooling by Sliced-Wasserstein Embedding

### TL;DR

We present a novel pooling method, called Pooling by Sliced-Wasserstein Embedding (PSWE), which leverages the (generalized) sliced-Wasserstein distances to map an input set of arbitrary cardinality to a fixed-size representation.

### Abstract

Learning representations from sets has become increasingly important with many applications in point cloud processing, graph learning, image/video recognition, and object detection. We introduce a geometrically-interpretable and generic pooling mechanism for aggregating a set of features into a fixed-dimensional representation. In particular, we treat elements of a set as samples from a probability distribution and propose an end-to-end trainable Euclidean embedding for Sliced-Wasserstein distance to learn from set-structured data effectively. We evaluate our proposed pooling method on a wide variety of set-structured data, including point-cloud, graph, and image classification tasks, and demonstrate that our proposed method provides superior performance over existing set representation learning approaches.

## Sampling Massive Graph Streams with Incubation for Triangle Counting

### TL;DR

None

### Abstract

Counting triangles is a fundamental problem in graph analytics with various applications such as anomaly detection and friend recommendation. However, real-world graphs tend to have massive sizes and evolve gradually, which makes the exact triangle counting impractical or even infeasible. A natural approach is to treat these graphs as streams and estimate the triangle counts via storing a small portion of edges. In this paper, we propose a novel single-pass stream sampling framework called Graph Incubation Sampling (GIS) to estimate the triangle counts in graph streams. Our framework consists of two sequential stages, i.e., incubating and sampling. The key advantage of our framework is that the incubating stage enables the sampling stage to capture the edges participating in more triangles with higher probabilities even when they appear early in the graph streams, while existing sampling methods fail to catch these early edges. The incubating stage allows edges to stay in the incubator until they collect enough topological information for the following sampling stage. For edges in the incubator, we propose an efficient incubating strategy to adjust their incubating time and monitor their ``potential'', which has a positive correlation with the number of triangles containing them. During the sampling stage, we sample edges coming out from the incubator with probabilities proportional to their potential in order to reduce estimation variance. To manifest the flexibility of our framework, we demonstrate two different sampling methods which accept normalized potential as the inclusion probabilities. We derive an unbiased estimator of the triangle counts through extended rigorous martingale formulation. Extensive experiment evaluation validates the unbiasedness and low variance of our proposed framework. Besides, our framework produces estimation with up to 71% smaller errors compared with the state-of-the-art counterparts.

## Adaptive Label Smoothing To Regularize Large-Scale Graph Training

### TL;DR

None

### Abstract

Graph neural networks (GNNs), which learn the node representations by recursively aggregating information from its neighbors, have become a predominant computational tool in many domains. To handle large-scale graphs, most of the existing methods partition the input graph into multiple sub-graphs (e.g., through node clustering) and apply batch training to save memory cost. However, such batch training will lead to label bias within each batch, and then result in over-confidence in model predictions. Since the connected nodes with positively related labels tend to be assigned together, the traditional cross-entropy minimization process will attend on the predictions of biased classes in the batch, and may intensify the overfitting issue. To overcome the label bias problem, we propose the adaptive label smoothing (ALS) method to replace the one-hot hard labels with smoothed ones, which learns to allocate label confidences from the biased classes to the others. Specifically, ALS propagates node labels to aggregate the neighborhood label distribution in a pre-processing step, and then updates the optimal smoothed labels online to adapt to specific graph structure. Experiments on the real-world datasets demonstrate that ALS can be generally applied to the main scalable learning frameworks to calibrate the biased labels and improve generalization performances. 

## Terra: Imperative-Symbolic Co-Execution of Imperative Deep Learning Programs

### TL;DR

None

### Abstract

Imperative programming allows users to implement their deep neural networks (DNNs) easily and has become an essential part of recent deep learning (DL) frameworks. Recently, several systems have been proposed to combine the usability of imperative programming with the optimized performance of symbolic graph execution. Such systems convert imperative Python DL programs to optimized symbolic graphs and execute them. However, they cannot fully support the usability of imperative programming. For example, if an imperative DL program contains a Python feature with no corresponding symbolic representation (e.g., third-party library calls or unsupported dynamic control flows) they fail to execute the program. To overcome this limitation, we propose Terra, an imperative-symbolic co-execution system that can handle any imperative DL programs while achieving the optimized performance of symbolic graph execution. To achieve this, Terra builds a symbolic graph by decoupling DL operations from Python features. Then, Terra conducts the imperative execution to support all Python features, while delegating the decoupled operations to the symbolic execution. We evaluated Terra’s performance improvement and coverage with ten imperative DL programs for several DNN architectures. The results show that Terra can speed up the execution of all ten imperative DL programs, whereas AutoGraph, one of the state-of-the-art systems, fails to execute five of them.

## TestRank: Bringing Order into Unlabeled Test Instances for Deep Learning Tasks

### TL;DR

A novel test input prioritization technique for efficient testing and debugging of deep neural network 

### Abstract

Deep learning (DL) systems are notoriously difficult to test and debug due to the lack of correctness proof and the huge test input space to cover. Given the ubiquitous unlabeled test data and high labeling cost, in this paper, we propose a novel test prioritization technique, namely TestRank, which aims at revealing more model failures with less labeling effort. TestRank brings order into the unlabeled test data according to their likelihood of being a failure, i.e., their failure-revealing capabilities. Different from existing solutions, TestRank leverages both intrinsic and contextual attributes of the unlabeled test data when prioritizing them. To be specific, we first build a similarity graph on both unlabeled test samples and labeled samples (e.g., training or previously labeled test samples). Then, we conduct graph-based semi-supervised learning to extract contextual features from the correctness of similar labeled samples. For a particular test instance, the contextual features extracted with the graph neural network and the intrinsic features obtained with the DL model itself are combined to predict its failure-revealing capability. Finally, TestRank prioritizes unlabeled test inputs in descending order of the above probability value. We evaluate TestRank on three popular image classification datasets, and results show that TestRank significantly outperforms existing test prioritization techniques. 
  

## Neighborhood Reconstructing Autoencoders

### TL;DR

We propose a new graph-based autoencoder, the Neighborhood Reconstructing Autoencoder, which learns manifolds that are robust to noise with the correct local geometry.

### Abstract

Vanilla autoencoders often produce manifolds that are highly sensitive to noisy training data, or have the wrong local connectivity and geometry. Recent autoencoder regularization methods that use neighborhood graphs have had some success in addressing the latter: connectivity information obtained from these graphs generally leads to fewer errors in the local geometry of the learned manifold. The focus of these methods is exclusively on the encoder -- the underlying premise being that the local geometry and topology of the data is captured in the latent space distribution -- but as we show, neglecting the decoder can lead to noise sensitivity and overfitting. In this paper we propose a new graph-based autoencoder, the Neighborhood Reconstructing Autoencoder, that addresses both the local geometry and overfitting issues of existing autoencoders. In lieu of a point reconstruction loss function, neighborhood graphs that capture the local geometry of the data distribution are combined with a local quadratic approximation to formulate a new neighborhood reconstruction loss, which as we show significantly improves the robustness of autoencoder training.  Compared to existing graph-based methods, our algorithm is easy to implement, scalable, and far more computationally efficient, requiring only a single prior construction of the graph. Extensive experiments with standard datasets demonstrate the performance advantages -- in some cases by significant margins -- of our method.

## ChebLieNet: Invariant spectral graph NNs turned equivariant by Riemannian geometry on Lie groups

### TL;DR

None

### Abstract

We introduce ChebLieNet, a group-equivariant method on (anisotropic) manifolds. Surfing on the success of graph- and group-based neural networks, we take advantage of the recent developments in the geometric deep learning field to derive a new approach to exploit any anisotropies in data. Via discrete approximations of Lie groups, we develop a graph neural network made of anisotropic convolutional layers (Chebyshev convolutions), spatial pooling and unpooling layers, and global pooling layers. Group equivariance is achieved via equivariant and invariant operators on graphs with anisotropic left-invariant Riemannian distance-based affinities encoded on the edges. Thanks to its simple form, the Riemannian metric can model any anisotropies, both in the spatial and orientation domains. This control on anisotropies of the Riemannian metrics allows to balance equivariance (anisotropic metric) against invariance (isotropic metric) of the graph convolution layers. Hence we open the doors to a better understanding of anisotropic properties. We empirically prove the existence of (data-dependent) sweet spots for anisotropic parameters on CIFAR10. This crucial result is evidence of the necessity of using tunable anisotropic kernels. We also evaluate the scalability of this approach on STL10 (image data) and ClimateNet (spherical data), showing its remarkable adaptability to diverse tasks and the benefit of defining anisotropic space.

## Towards Open-World Feature Extrapolation: An Inductive Graph Learning Approach

### TL;DR

We formulate a new problem called open-world feature extrapolation and target it via graph representation and learning based on observed feature data

### Abstract

We target open-world feature extrapolation problem where the feature space of input data goes through expansion and a model trained on partially observed features needs to handle new features in test data without further retraining. The problem is of much significance for dealing with features incrementally collected from different fields. To this end, we propose a new learning paradigm with graph representation and learning. Our framework contains two modules: 1) a backbone network (e.g., feedforward neural nets) as a lower model takes features as input and outputs predicted labels; 2) a graph neural network as an upper model learns to extrapolate embeddings for new features via message passing over a feature-data graph built from observed data. Based on our framework, we design two training strategies, a self-supervised approach and an inductive learning approach, to endow the model with extrapolation ability and alleviate feature-level over-fitting. We also provide theoretical analysis on the generalization error on test data with new features, which dissects the impact of training features and algorithms on generalization performance. Our experiments over several classification datasets and large-scale advertisement click prediction datasets demonstrate that our model can produce effective embeddings for unseen features and significantly outperforms baseline methods that adopt KNN and local aggregation.

## Towards Multi-Grained Explainability for Graph Neural Networks

### TL;DR

Towards multi-grained explainability of graph neural networks, we integrate the idea of pre-training and fine-tuning in the explainers.

### Abstract

When a graph neural network (GNN) made a prediction, one raises question about explainability: Which fraction of the input graph is most inﬂuential to the model’s decision? Producing an answer requires understanding the model’s inner workings in general and emphasizing the insights on the decision for an instance at hand. Nonetheless, most of current approaches focus only on one aspect: (1) local explainability, which interprets each instance independently, thus hardly exhibits the class-wise patterns; (2) global explainability, which systematizes the globally important patterns, but might be trivial in the local context. This dichotomy greatly limits the ﬂexibility and effectiveness of explainers. A performant paradigm towards multi-grained explainability is until-now lacking and thus a focus of our work. In this work, we exploit the pre-training and ﬁne-tuning techniques to develop our explainer and generate multi-grained explanations. Speciﬁcally, the pre-training phase accounts for the contrastivity among different classes, so as to highlight the class-wise characteristics from a global view; afterward, the ﬁnetuning phase adapts the explanations in the local context. Experiments on both synthetic and real-world datasets show the superiority of our explainer, in terms of AUC on explaining graph classiﬁcation over the leading baselines.

## Graph Convolutional Networks with Feature Clustering for Neuropsychiatric Disease Classification using Functional Connectomics

### TL;DR

None

### Abstract

Studies using Graph Convolutional Networks (GCNs) to classify patients from health controls has been increasing interest in field of neuroscience or medical imaging recently. The nodes in graph represent subjects by extracting a set of features (e.g. functional connectivity data) from the imaging data and the weighted edges incorporating the association between subjects (e.g. pairwise similarities in imaging data or phenotypic information in non-imaging data). In practice, the graph for neuropsychiatric disease prediction or classification consists of smaller size of nodes compared with the feature dimension due to costly data collection and more complex connectivity structure than other graphs. Furthermore, most studies have used a specifically crafted subset of high-dimensional feature vectors for training GCNs in such applications. In this work, we investigate a novel GCN framework coupled with intelligent feature clustering to capture structures in high-dimensions. A group of feature clusters are created by randomly sampling from different sites of data collection with a small sampling ratio to project the original feature vectors, allowing the GCN is generalized without overfitting to the high-dimensional data. We reported our results on ABIDE dataset, which is a large, open-source dataset for neuropsychiatric disease classification. The proposed framework can achieve better performance than other state-of-art methods. 

## Towards Feature Overcorrelation in Deeper Graph Neural Networks

### TL;DR

We observe a new issue in deeper GNNs, i.e., feature overcorrelation, and perform a thorough study to deepen our understanding on this issue. 

### Abstract

Graph neural networks (GNNs) have achieved great success in graph representation learning, which has tremendously facilitated various real-world applications. Nevertheless, the performance of GNNs significantly deteriorates when the depth increases. Recent researches have attributed this phenomenon to the oversmoothing issue, which indicates that the learned node representations are highly indistinguishable. In this paper, we observe a new issue in deeper GNNs, i.e., feature overcorrelation, and perform a thorough study to deepen our understanding on this issue. In particular, we demonstrate the existence of feature overcorrelation in deeper GNNs, reveal potential reasons leading to this issue, and validate that overcorrelation and oversmoothing present different patterns though they are related. Since feature overcorrelation indicates that GNNs encode less information and can harm the downstream tasks, it is of great significance to mitigate this issue. Therefore, we propose the DeCorr, a general framework to effectively reduce feature correlation for deeper GNNs. Experimental results on various datasets demonstrate that DeCorr can help train deeper GNNs effectively and is complementary to methods tackling oversmoothing.

## Graph Regularized Latent Embedding for Multi-Modal Problems

### TL;DR

None

### Abstract

Learning a common latent space is a key task in various problems that requires learning a projection matrix. We propose to learn this projection matrix based on a novel matrix decomposition that implicitly induces a regularization based on rows and columns graph of data matrix. Our resulting representation leads to competitive clustering  performance  on several benchmarks. We then extend our framework to problems that learns the resulting representation sequentially by learning multiple projection matrices (e.g. compositional zero shot learning and cross modal video retrieval).  We validate our framework on several benchmarks where it  outperforms strong baseline methods at a fraction of the computational effort of prior work. 

## Learning Distilled Collaboration Graph for Multi-Agent Perception

### TL;DR

Knowledge distillation on a collaboration graph achieves better performance-bandwidth trade-off for multi-agent perception.

### Abstract

To promote better performance-bandwidth trade-off for multi-agent perception, we propose a novel distilled collaboration graph (DiscoGraph) to model trainable, pose-aware, and adaptive collaboration among agents. Our key novelties lie in two aspects. First, we propose a teacher-student framework to train DiscoGraph via knowledge distillation. The teacher model employs an early collaboration with holistic-view inputs; the student model is based on intermediate collaboration with single-view inputs. Our framework trains DiscoGraph by constraining post-collaboration feature maps in the student model to match the correspondences in the teacher model. Second, we propose a matrix-valued edge weight in DiscoGraph. In such a matrix, each element reflects the inter-agent attention at a specific spatial region, allowing an agent to adaptively highlight the informative regions. During inference, we only need to use the student model, which we call the distilled collaboration network (DiscoNet). Attributed to the teacher-student framework, multiple agents with the shared DiscoNet could collaboratively approach the performance of a hypothetical teacher model with a holistic view. Our approach is validated on a large-scale multi-agent 3D object detection dataset that we synthesized using CARLA and SUMO co-simulation. Our experiments show that the proposed DiscoNet could achieve 192 times less communication volume and still outperforms the state-of-the-art methods for both AP@0.5 and 0.7.

## Is Homophily a Necessity for Graph Neural Networks?

### TL;DR

None

### Abstract

Graph convolutional networks (GCNs) have shown their great power in representation learning on graphs and thus facilitate numerous downstream tasks. When applying to the semi-supervised node classification task, GCNs are widely believed to only work for graphs with strong homophily but fail to generalize to heterophilous graphs where dissimilar nodes tend to connect with each other. Recent efforts have been made to design new architectures to overcome this limitation. However, we empirically find that the GCN model can achieve better performance than these carefully designed methods on some commonly used heterophilous graphs. This motivates us to reconsider whether the claim ``Homophily is a necessity for GNNs to work well.'' is true or not. Our answer to this question is no. GCNs can achieve reasonable performance on heterophilous graphs under certain conditions. We carefully characterize these conditions, then provide theoretical understandings and empirical observations to support our arguments under these conditions. Finally, we examine the existing heterophilous graphs and provide explanations on why the GCN model works well or not on them based on our theoretical understanding.  

## Lifelong Generative Modelling Using Dynamic Expansion Graph Model

### TL;DR

Expanding the mixture VAE architecture while minimizing the risk bound for lifelong learning

### Abstract

 Variational Autoencoders (VAEs) suffer from degenerated performance, when learning several successive tasks, caused by catastrophic forgetting. VAEs are using either Generative Replay (GR) mechanisms or Expanding Network Architectures (ENA) in order to address the knowledge loss. In this paper we study the forgetting behaviour of VAEs using a joint GR and ENA methodology, by deriving an upper bound on the negative marginal log-likelihood, which provides new insights into how VAEs forget knowledge during lifelong learning. The analysis indicates the best performance achieved by using ENA when there are no restrictions on the number of components. However, an ENA-based approach would require an excessive number of parameters. This motivates us to propose a novel Dynamic Expansion Graph Model (DEGM). DEGM expands its architecture, according to the novelty of information associated with new databases, when compared with the information learnt by the network from previous tasks. DEGM training optimizes knowledge structuring, characterizing the joint probabilistic representations corresponding to the past and more recently learned tasks. We theoretically and empirically demonstrate that DEGM guarantees optimal performance for each task while also minimizing the required number of parameters.

## Rotation Invariant Graph Neural Networks using Spin Convolutions

### TL;DR

Graph neural network for predicting atomic energies and forces with an angular encoding that is invariant to rotations.

### Abstract

Progress towards the energy breakthroughs needed to combat climate change can be significantly accelerated through the efficient simulation of atomic systems. Simulation techniques based on first principles, such as Density Functional Theory (DFT), are limited in their practical use due to their high computational expense. Machine learning approaches have the potential to approximate DFT in a computationally efficient manner, which could dramatically increase the impact of computational simulations on real-world problems.

Approximating DFT poses several challenges. These include accurately modeling the subtle changes in the relative positions and angles between atoms, and enforcing constraints such as rotation invariance or energy conservation. We introduce a novel approach to modeling angular information between sets of neighboring atoms in a graph neural network. Rotation invariance is achieved for the network's edge messages through the use of a per-edge local coordinate frame and a novel spin convolution over the remaining degree of freedom. Two model variants are proposed for the applications of structure relaxation and molecular dynamics. State-of-the-art results are demonstrated on the large-scale Open Catalyst 2020 dataset. Comparisons are also performed on the MD17 and QM9 datasets.

## Hierarchical Prototype Network for Continual Graph Representation Learning

### TL;DR

None

### Abstract

Despite significant advances in graph representation learning, little attention has been paid to graph data in which new categories of nodes (e.g., new research areas in citation networks or new types of products in co-purchasing networks) and their associated edges are continuously emerging. The key challenge is to incorporate the feature and topological information of new nodes in a continuous and effective manner such that performance over existing nodes is uninterrupted. To this end, we present Hierarchical Prototype Networks (HPNs) which can adaptively extract different levels of abstract knowledge in the form of prototypes to represent continually expanded graphs. Specifically, we first leverage a set of Atomic Feature Extractors (AFEs) to generate basic features which can
encode both the elemental attribute information and the topological structure of the target node. Next, we develop HPNs by adaptively selecting relevant AFEs and represent each node with three-levels of prototypes, i.e., atomic-level, node-level, and class-level. In this way, whenever a new category of nodes is given, only the relevant AFEs and prototypes at each level will be activated and refined, while others remain uninterrupted. Finally, we provide the theoretical analysis on memory consumption bound and the continual learning capability of HPNs. Extensive empirical studies on eight different public datasets justify that HPNs are memory efficient and can achieve state-of-the-art performance on different continual graph representation learning tasks.

## Metropolis-Hastings Data Augmentation for Graph Neural Networks

### TL;DR

None

### Abstract

Graph Neural Networks (GNNs) often suffer from weak-generalization due to sparsely labeled data despite their promising results on various graph-based tasks. Data augmentation is a prevalent remedy to improve the generalization ability of models in many domains. However, due to the non-Euclidean nature of data space and the dependencies between samples, designing effective augmentation on graphs is challenging. In this paper, we propose a novel framework Metropolis-Hastings Data Augmentation (MH-Aug) that draws augmented graphs from an explicit target distribution for semi-supervised learning. MH-Aug produces a sequence of augmented graphs from the target distribution enables flexible control of the strength and diversity of augmentation. 
Since the direct sampling from the complex target distribution is challenging, we adopt the Metropolis-Hastings algorithm to obtain the augmented samples. We also propose a simple and effective semi-supervised learning strategy with generated samples from MH-Aug. Our extensive experiments demonstrate that MH-Aug can generate a sequence of samples according to the target distribution to significantly improve the performance of GNNs.

## Predicting Molecular Conformation via Dynamic Graph Score Matching

### TL;DR

None

### Abstract

Predicting stable 3D conformations from 2D molecular graphs has been a long-standing challenge in computational chemistry. Recently, machine learning approaches have demonstrated very promising results compared to traditional experimental and physics-based simulation methods. These approaches mainly focus on modeling the local interactions between neighboring atoms on the molecular graphs and overlook the long-range interactions between non-bonded atoms. However, these non-bonded atoms may be proximal to each other in 3D space, and modeling their interactions is of crucial importance to accurately determine molecular conformations, especially for large molecules and multi-molecular complexes. In this paper, we propose a new approach called Dynamic Graph Score Matching (DGSM) for molecular conformation prediction, which models both the local and long-range interactions by dynamically constructing graph structures between atoms according to their spatial proximity during both training and inference. Specifically, the DGSM directly estimates the gradient fields of the logarithm density of atomic coordinates according to the dynamically constructed graphs using score matching methods. The whole framework can be efficiently trained in an end-to-end fashion. Experiments across multiple tasks show that the DGSM outperforms state-of-the-art baselines by a large margin, 
and it is capable of generating conformations for a broader range of systems such as proteins and multi-molecular complexes.

## CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks

### TL;DR

This paper presents a method for generating counterfactual explanations for graph neural network, which come in the form of the minimal perturbation to the input such that the prediction changes. 

### Abstract

Given the increasing promise of Graph Neural Networks (GNNs) in real-world applications, several methods have been developed for explaining their predictions. 
So far, these methods have primarily focused on generating subgraphs that are especially relevant for a particular prediction. 
However, such methods do not provide a clear opportunity for recourse: given a prediction, we want to understand how the prediction can be changed in order to achieve a more desirable outcome. 
In this work, we propose a method for generating counterfactual (CF) explanations for GNNs: the minimal perturbation to the input (graph) data such that the prediction changes. 
Using only edge deletions, we find that our method, CF-GNNExplainer, can generate CF explanations for the majority of instances across three widely used datasets for GNN explanations, while removing less than 3 edges on average, with at least 94% accuracy. 
This indicates that CF-GNNExplainer primarily removes edges that are crucial for the original predictions, resulting in minimal CF explanations.

## Structured Graph Neural Networks for Inductive Node Classification

### TL;DR

None

### Abstract

This paper studies node classification in the inductive setting, i.e., aiming to learn a model on labeled training graphs and generalize it to infer node labels on unlabeled test graphs. This problem has been extensively studied with graph neural networks (GNNs) by learning effective node representations, as well as traditional structured prediction methods for modeling the structured output of node labels, e.g., conditional random fields (CRFs). In this paper, we present a new approach called the Structured Graph Neural Network (StructGNN), which combines the advantages of both worlds. StructGNN defines flexible potential functions of CRFs with GNNs. However, learning such a model is nontrivial as it involves optimizing a maximin game with high-cost inference. Inspired by the underlying connection between the joint and marginal distributions defined by Markov networks, we propose to solve an approximate version of the optimization problem as proxy, which yields a near-optimal solution, making learning more efficient. Extensive experiments show our approach outperforms many competitive baselines.

## Graph Neural Networks for DOM Tree Element Prediction

### TL;DR

We adapt and evaluate graph neural networks for DOM tree element prediction.

### Abstract

This paper tackles the under-explored problem of DOM tree node (i.e., webpage element) prediction and how the prediction performance is affected by the underlying representation learning. We advance the field of machine learning-based web automation and hope to spur further research regarding this crucial area, with two contributions. First, we adapt several popular Graph-based Neural Network models and apply them to predict elements in website DOM trees. Second, we present a large-scale and realistic dataset of webpages. By providing this open-access resource, we lower the entry barrier to this area of research. The dataset contains $51,701$ manually labeled product pages from $8,175$ real e-commerce websites. The pages can be rendered entirely in a web browser and are suitable for computer vision applications. This makes it substantially richer and more diverse than other datasets proposed for element classification and prediction on the web. Finally, using our proposed dataset, we show that Graph Convolutional Neural Networks outperform other state-of-the-art methods on a web element prediction task.

## Subgroup Generalization and Fairness of Graph Neural Networks

### TL;DR

We present a novel PAC-Bayesian analysis for the generalization ability of graph neural networks on non-IID node-level tasks, which has some implications on the fairness of graph neural networks.

### Abstract

Despite enormous successful applications of graph neural networks (GNNs) recently, theoretical understandings of their generalization ability, especially for node-level tasks where data are not independent and identically-distributed (IID), have been sparse. The theoretical investigation of the generalization performance is beneficial for understanding fundamental issues (such as fairness) of GNN models and designing better learning methods. In this paper, we present a novel PAC-Bayesian analysis for GNNs under a non-IID semi-supervised learning setup. Moreover, we analyze the generalization performances on different subgroups of unlabeled nodes, which allows us to further study an accuracy-(dis)parity-style (un)fairness of GNNs from a theoretical perspective. Under reasonable assumptions, we demonstrate that the distance between a test subgroup and the training set can be a key factor affecting the GNN performance on that subgroup, which calls special attention to the training node selection for fair learning. Experiments across multiple GNN models and datasets support our theoretical results.

## Deep Contrastive Graph Representation via Adaptive Homotopy Learning

### TL;DR

None

### Abstract

Homotopy model is an excellent tool exploited by diverse research works in the field of machine learning. However, its flexibility is limited due to lack of adaptiveness, i.e., manual fixing or tuning the appropriate homotopy coefficients. To address the problem above, we propose a novel adaptive homotopy framework (AH) in which the Maclaurin duality is employed, such that the homotopy parameters can be adaptively obtained. Accordingly, the proposed AH can be widely utilized to enhance the homotopy-based algorithm. In particular, in this paper, we apply AH to contrastive learning (AHCL) such that it can be effectively transferred from weak-supervised learning (given label priori) to unsupervised learning, where soft labels of contrastive learning are directly and adaptively learned.  Accordingly, AHCL has the adaptive ability to extract deep features without any sort of prior information. Consequently, the affinity matrix formulated by the related adaptive labels can be constructed as the deep Laplacian graph that incorporates the topology of deep representations for the inputs. Eventually, extensive experiments on benchmark datasets validate the superiority of our method.

## SARA: Semantics-Approximation RegulArizer for Knowledge Graph Completion

### TL;DR

This paper uses latent semantic relations to propose a semantics-approximation regularizer for knowledge graph completion  to solve the problem of overfitting.

### Abstract

Tensor factorization and distanced based models play important roles in knowledge graph completion (KGC). However, a major challenge springs from the phenomenon that their performance is usually affected by the overfitting severely. Then a variety of different regularizers emerge to suppress overfitting—such as the squared Frobenius norm, the tensor nuclear norm and duality-induced regularizers. Nonetheless, the applicability of specific conditions significantly limits their practical application. To this end, we propose a novel regularizer—namely, Semantics-Approximation RegulArizer (SARA)—which is not only of great benefit to existing models, but also can be extended to other fields. The major novelty of SARA is based on the observation that the previous work only focuses on the constraints on the single entity and relation while ignoring the semantics constraints generated by latent semantic relation connected to entities, which is not conducive to improve the performance. Therefore, SARA can suppress overfitting by means of semantics order constraints. As a practical extension, it is flexible to use and widely applied to both distance based models and tensor factorization based models. Our experimental results indicate a clear and substantial improvement over state-of-the-art relation prediction methods.

## Reducing Classifier Overconfidence Against Adversaries through Graph Algorithms

### TL;DR

We replace the last-layer+softmax of CNNs with a graph-based estimator built over the embedding from validation examples to reduce overconfidence of the models under strong adversarial shifts

### Abstract

In this work we show that deep learning classifiers tend to become overconfident1in their answers under adversarial attacks, even when the classifier is optimized to survive such attacks.  Our work draws upon stochastic geometry and graph algorithms to propose a general framework to replace the last fully connected layer and softmax output.  This framework (a) can be applied to any classifier and (b) significantly reduces the classifier’s overconfidence in its output without much of an impact on its accuracy when compared to original adversarially-trained classifiers. Its relative effectiveness increases as the attacker becomes more powerful. Our use of graph algorithms in adversarial learning is new and of independent interest. Finally, we show the advantages of this last-layer softmax replacement over image tasks under common adversarial attacks.

## Node Dependent Local Smoothing for Scalable Graph Learning

### TL;DR

A simple, scalable, flexible and efficient method for graph learning.

### Abstract

Recent works reveal that feature or label smoothing lies at the core of Graph Neural Networks (GNNs). SGC shows feature smoothing combined with simple linear regression achieves comparable performance with the carefully designed GNNs while APPNP shows a simple MLP model with label smoothing of its prediction can outperform the vanilla GCN. Though an interesting finding, smoothing has not been well understood, especially regarding how to control the extent of smoothness. Intuitively, too small or too large smoothing iterations may cause under-smoothing or over-smoothing and can lead to sub-optimal performance. Moreover, the extent of smoothness is node-specific, depending on its degree and local structure. To this end, we propose a novel algorithm called node-dependent local smoothing (NDLS), which aims to control the smoothness of every node by setting a node-specific smoothing iteration. Specifically, NDLS computes influence scores based on the adjacency matrix and selects the iteration number by setting a threshold on the scores. Once selected, the iteration number can be applied to both feature smoothing and label smoothing. Experimental results demonstrate that NDLS enjoys high accuracy -- state-of-the-art performance on node classifications tasks, flexibility -- can be incorporated with any models, scalability and efficiency -- can support large scale graphs with fast training.

## Few-shot Knowledge Graph Reasoning with Logic Rules

### TL;DR

Solving few-shot knowledge graph reasoning problem by learning and using rules.

### Abstract

This paper studies few-shot knowledge graph reasoning, aiming at learning reasoning models for new relations with only a few examples as demonstration. Most existing methods learn a single embedding from given examples to represent each relation for reasoning, which cannot well deal with the diverse semantics of a relation. This paper addresses the problem by learning chain-like logic rules, where each rule provides a description of a relation, and combining multiple such rules further yields a more comprehensive description for each relation. To model logic rules, we introduce a rule generator and a reasoning model, where the rule generator generates logic rules for the reasoning model to make predictions with. Given a new relation, we first tune the rule generator and reasoning model to the relation by maximizing the likelihood of the support set. Then the adapted rule generator and reasoning model are further used to answer questions in the query set. The whole framework can be efficiently trained by combining FOMAML and an EM algorithm. Extensive experimental results on four knowledge graphs prove the effectiveness of the proposed approach over many competitive baselines.

## GemNet: Universal Directional Graph Neural Networks for Molecules

### TL;DR

We prove the universality of GNNs with geometric message passing for rotationally invariant predictions, and propose the GemNet architecture for molecular dynamics predictions

### Abstract

Effectively predicting molecular interactions has the potential to accelerate molecular dynamics by multiple orders of magnitude and thus revolutionize chemical simulations. Graph neural networks (GNNs) have recently shown great successes for this task, overtaking classical methods based on fixed molecular kernels. However, they still appear very limited from a theoretical perspective, since regular GNNs cannot distinguish certain types of graphs. In this work we close this gap between theory and practice. We show that GNNs with directed edge embeddings and two-hop message passing are indeed universal approximators for predictions that are invariant to global rotation and translation, and equivariant to permutation. We then leverage these insights and multiple structural improvements to propose the geometric message passing neural network (GemNet). We demonstrate the benefits of the proposed changes in multiple ablation studies. GemNet outperforms previous models on the COLL and MD17 molecular dynamics datasets by 36%, performing especially well on the most challenging molecules.

## Recursive Reasoning Graph for Multi-Agent Reinforcement Learning

### TL;DR

A centralized-training-decentralized-execution deep multi-agent reinforcement learning algorithm with recursive reasoning for multi-player general-sum games.

### Abstract

Multi-agent reinforcement learning (MARL) provides an efficient way for simultaneously learning policies for multiple agents interacting with each other. However, in scenarios requiring complex interactions, existing algorithms can suffer from an inability to accurately anticipate the influence of self-actions on other agents. Incorporating an ability to reason about other agents' potential responses can allow an agent to formulate more effective strategies. This paper adopts a recursive reasoning model in a centralized-training-decentralized-execution framework to help learning agents better cooperate with or compete against others. The proposed algorithm, referred as the Recursive Reasoning Graph (R2G), shows state-of-the-art performance on multiple multi-agent particle and robotics games.

## Deconfounding to Explanation Evaluation in Graph Neural Networks

### TL;DR

To evaluate explanations of graph neural networks faithfully, we devise a deconfounding subgraph evaluation framework.

### Abstract

Explainability of graph neural networks (GNNs) aims to answer "Why the GNN made a certain prediction?", which is crucial to interpret the model prediction. The feature attribution framework distributes a GNN's prediction to its input features (e.g., edges), identifying an influential subgraph as the explanation. When evaluating the explanation faithfulness (i.e., subgraph importance), a standard way is to audit the model prediction based on the subgraph solely. However, we argue that a distribution shift exists between the full graph and the subgraph, causing the remarkable out-of-distribution problem. With an in-depth causal analysis, we find the OOD effect acts as the confounder --- bringing spurious associations between the subgraph importance and model prediction, and making the evaluation less reliable. In this work, we propose a novel paradigm, Deconfounded Subgraph Evaluation (DSE), which assesses the causal effect of an explanatory subgraph on the model prediction. While the distribution shift is generally intractable, we employ the front-door adjustment where a surrogate variable of the subgraphs is introduced. Specifically, we devise a generative model to generate the plausible surrogates, which make the explanatory subgraphs conform to the data distribution, to approach the unbiased estimation of subgraph importance. Empirical results demonstrate the effectiveness of DSE in terms of explanation fidelity.

## Realistic molecule optimization on a learned graph manifold

### TL;DR

None

### Abstract

Deep learning based molecular graph generation and optimization has recently been attracting attention due to its great potential for de novo drug design. On the one hand, recent models are able to efficiently learn a given graph distribution, and many approaches have proven very effective to produce a molecule that maximizes a given score. On the other hand, it was shown by previous studies that generated optimized molecules are often unrealistic, even with the inclusion of mechanics to enforce similarity to a dataset of real drug molecules. In this work we use a hybrid approach, where the dataset distribution is learned using an autoregressive model while the score optimization is done using the Metropolis algorithm, biased toward the learned distribution. We show that the resulting method, that we call learned realism sampling (LRS), produces empirically more realistic molecules and outperforms all recent baselines in the task of molecule optimization with similarity constraints.

## EchoEA: Echo Information between Entities and Relations for Entity Alignment

### TL;DR

None

### Abstract

Entity alignment (EA) is to discover entities referring to the same object in the real world from different knowledge graphs (KGs). It plays an important role in automatically integrating KGs from multiple sources.  Existing knowledge graph embedding (KGE) methods based on Graph Neural Networks (GNNs) have achieved promising results, which enhance entity representation with relation information unidirectionally. Besides, more and more methods introduce semi-supervision to ask for more labeled training data.  However, two challenges still exist in these methods: (1) Insufficient interaction: The interaction between entities and relations is insufficiently utilized. (2) Low-quality bootstrapping: The generated semi-supervised data is of low quality.
  In this paper, we propose a novel framework, Echo Entity Alignment (EchoEA), which leverages self-attention mechanism to spread entity information to relations and echo back to entities. The relation representation is dynamically computed from entity representation. Symmetrically, the next entity representation is dynamically calculated from relation representation, which shows sufficient interaction.  Furthermore, we propose attribute-combined bi-directional global-filtered strategy (ABGS) to improve bootstrapping, reduce false samples and generate high-quality training data.  The experimental results on three real-world cross-lingual datasets are stable at around 96% at hits@1 on average, showing that our approach not only significantly outperforms the state-of-the-art methods, but also is universal and transferable for existing KGE methods.

## Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions

### TL;DR

None

### Abstract

Graph generative models are a highly active branch of machine learning. Given the steady development of new models of ever-increasing complexity, it is necessary to provide a principled way to \emph{evaluate} and \emph{compare} them. In this paper, we enumerate the desirable criteria for comparison metrics, discuss the development of such metrics, and provide a comparison of their respective expressive power. We perform a systematic evaluation of the main metrics in use today, highlighting some of the challenges and pitfalls researchers inadvertently can run into. We then describe a collection of suitable metrics, give recommendations as to their practical suitability, and analyse their behaviour on synthetically generated perturbed graphs as well as on recently proposed graph generative models.

## Improving Robustness of Graph Neural Networks with Heterophily-Inspired Designs

### TL;DR

We formally show that adversarial perturbations of graph structure typically reduce the level of homophily in the graph, and explain how simple heterophily-inspired architecture changes can improve robustness orthogonally to existing vaccinations.

### Abstract

Recent studies have exposed that many graph neural networks (GNNs) are sensitive to adversarial attacks, and can suffer from performance loss if the graph structure is intentionally perturbed. A different line of research has shown that many GNN architectures implicitly assume that the underlying graph displays homophily, i.e., connected nodes are more likely to have similar features and class labels, and perform poorly if this assumption is not fulfilled. In this work, we formalize the relation between these two seemingly different issues. We theoretically show that in the standard scenario in which node features exhibit homophily, impactful structural attacks always lead to increased levels of heterophily. Then, inspired by GNN architectures that target heterophily, we present two designs—(i) separate aggregators for ego- and neighbor-embeddings, and (ii) a reduced scope of aggregation—that can significantly improve the robustness of GNNs. Our extensive empirical evaluations show that GNNs featuring merely these two designs can achieve significantly improved robustness compared to the best-performing unvaccinated model with 24.99% gain in average performance under targeted attacks, while having smaller computational overhead than existing defense mechanisms. Furthermore, these designs can be readily combined with explicit defense mechanisms to yield state-of-the-art robustness with up to 18.33% increase in performance under attacks compared to the best-performing vaccinated model.

## How to transfer algorithmic reasoning knowledge to learn new algorithms?

### TL;DR

Studying how to transfer algorithmic knowledge when intermediate steps are missing.

### Abstract

Learning to execute algorithms is a fundamental problem that has been widely studied. Prior work (Veličković et al., 2019) has shown that to enable systematic generalisation on graph algorithms it is critical to have access to the intermediate steps of the program/algorithm. In many reasoning tasks, where algorithmic-style reasoning is important, we only have access to the input and output examples. Thus, inspired by the success of pre-training on similar tasks or data in Natural Language Processing (NLP) and Computer vision, we set out to study how we can transfer algorithmic reasoning knowledge. Specifically, we investigate how we can use algorithms for which we have access to the execution trace to learn to solve similar tasks for which we do not. We investigate two major classes of graph algorithms, parallel algorithms such as breadth-first search and Bellman-Ford and sequential greedy algorithms such as Prims and Dijkstra. Due to the fundamental differences between algorithmic reasoning knowledge and feature extractors such as used in Computer vision or NLP, we hypothesis that standard transfer techniques will not be sufficient to achieve systematic generalisation. To investigate this empirically we create a dataset including 9 algorithms and 3 different graph types. We validate this empirically and show how instead multi-task learning can be used to achieve the transfer of algorithmic reasoning knowledge.

## Learning optimal policies for combinatorial optimization problems using capsule networks and attention mechanism

### TL;DR

None

### Abstract

The advancement of Reinforcement learning (RL) has been instrumental in solving a wide variety of problems involving online decision-making. Similarly, over the recent few years, there has been a significant advancement in Graph Neural Networks for learning over unstructured data, mainly for classification problems. While drawing motivation from recent graph learning methods that learn to solve combinatorial optimization problems such as multi-Traveling Salesman and Vehicle Routing Problems (mTSP and VRP), this paper introduces a new graph learning architecture that seeks to provide promising convergence and improved scalability (w.r.t. other learning based methods), and better run-time performance (compared to non-learning methods). The proposed neural architecture consists of an encoder-decoder architecture, where the encoder is based on Capsule networks for a better representation of local and global information with permutation invariant node embeddings, and the decoder is based on the Multi-head attention (MHA) mechanism. The advantages of this approach is demonstrated for a Capacitated Vehicle Routing Problem (CVRP), and also for a more realistic complex problem of Multi-Robot Task Allocation (MRTA). The performance of our approach is favorably compared with some of the state-of-the-art methods for both CVRP and MRTA, especially when tested over problem scenarios that are increasingly more complex than those used for training the policies.

## Enhancing Aspect-based Sentiment Classification with Local Semantic Information

### TL;DR

We propose a model to make rational use of local semantic information and design a local semantic feature extraction module.

### Abstract

The research goal of aspect-level sentiment Classification is to analyze the sentiment polarity expressed by a given sentence according to its specific aspect. The Graph Convolutional Network (GCN) model based on attention mechanism performs better among the existing solutions. This kind of method uses syntactic dependency information and semantic information to adjust and optimize attention. Still, the experimental results show that this kind of model does not perform well in complex sentences. We consider that attention mechanism is good at capturing global feature information, while Convolutional Neural Networks can effectively utilize local features. In this work, we study how to use Convolutional Neural Networks and attention mechanism to extract sentiment features better and propose a Graph Convolutional Network Model (DWGCN) of Depthwise separable convolution. Moreover, the basic design idea is to obtain syntactic dependency information utilizing graph convolution network learning and use the corresponding attention mechanisms to interact syntactic features with contextual information about word order to get semantic information. Then, we extract the emotional statement about the given Aspect-term from the local semantic information. Therefore, we design a Feature extraction module based on a Depthwise separable convolution network and GLU (FMDG). To verify the effectiveness of the model, we test it on five benchmark datasets. The experimental results show that the proposed model outperforms the current relevant work in classification accuracy and generalization ability. 

## Reasonable Object Detection Guided by Knowledge of Global Context and Category Relationship

### TL;DR

None

### Abstract

The mainstream object detectors usually treat each region separately, which overlooks the important global context information and the associations between object categories. Existing methods model global context via attention mechanism, which requires ad hoc design and prior knowledge. Some works combine CNN features with label dependencies learned from a pre-defined graph and word embeddings, which ignore the gap between visual features and textual corpus and are usually task-specific (depend on RoIPool/RoIAlign). In this paper, we propose KROD (Knowledge-guided Reasonable Object Detection), which consists of the GKM (Global Category Knowledge Mining) module and CRM (Category Relationship Knowledge Mining) module, to improve detection performance by mimicking the processes of human reasoning. For a given image, GKM introduces global category knowledge into the detector by simply attaching a multi-label image classification branch to the backbone. Meanwhile, CRM input the raw detection outputs to the object category co-occurrence based knowledge graph to further refine the original results, with the help of GCN (Graph Convolutional Network). We also propose a novel loss-aware module to distinctively correct the classification probability of different detected boxes. Without bells and whistles, extensive experiments show that the proposed KROD can improve different baseline models (both anchor-based and anchor-free) by a large margin (1.2\% $\sim$ 1.8\% higher AP) with marginal loss of efficiency on MS COCO.

## Structure Inducing Pre-training

### TL;DR

We introduce Structure-inducing Pre-training, a novel pre-training framework that imposes structural constraints on the output embeddings and permits theoretical analysis of how pre-training structure impacts expected FT performance.

### Abstract

Pre-training (PT) has been very successful in domains like natural language processing. However, PT is much less successful in other domains, including protein sequences or biomedical networks. In this work, we postulate that this is because existing PT algorithms fail to effectively model inter-sample relationships. We thus propose Structure-inducing Pre-training (SIPT), a novel PT framework that leverages a specifiable pre-training graph to induce global, inter-sample structure in the output space. We show theoretically that this framework generalizes beyond many existing PT methods and that in select settings one can predict when PT will improve downstream-task performance based on the structure of the pre-training graph. We validate this system empirically with synthetic experiments and on real-world datasets spanning 10 fine-tuning tasks across 3 data modalities.

## Graph Posterior Network: Bayesian Predictive Uncertainty for Node Classification

### TL;DR

None

### Abstract

The interdependence between nodes in graphs is key to improve class prediction on nodes, utilized in approaches like Label Probagation (LP) or in Graph Neural Networks (GNNs). Nonetheless, uncertainty estimation for non-independent node-level predictions is under-explored.  In this work, we explore uncertainty quantification for node classification in three ways: (1) We derive three axioms explicitly characterizing the expected predictive uncertainty behavior in homophilic attributed graphs.(2) We propose a new model Graph Posterior Network (GPN) which explicitly performs Bayesian posterior updates for predictions on interdependent nodes. GPN provably obeys the proposed axioms. (3) We extensively evaluate GPN and a strong set of baselines on semi-supervised node classification including detection of anomalous features, and detection of left-out classes. GPN outperforms existing approaches for uncertainty estimation in the experiments.

## AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning

### TL;DR

A principled adaptive RL approach that adapts reliably to changes across domains in dynamics and observation function with only a few samples.

### Abstract

Most approaches in reinforcement learning (RL) are data-hungry and specific to fixed environments. In this paper, we propose a principled framework for adaptive RL, called AdaRL, that adapts reliably to changes across domains with only a few samples. Specifically, we construct a generative environment model for the structural relationships among variables in the system and embed the changes in a compact way, which provides a clear and interpretable picture for locating what and where the changes are and how to adapt. Based on the environment model, we characterize a minimal set of representations, including both domain-specific factors and domain-invariant state representations, that suffice for reliable and low-cost transfer. Moreover, we show that by explicitly leveraging a compact representation to encode changes, we can adapt the policy with few samples without further policy optimization in the target domain. We illustrate the efficacy of AdaRL through a series of experiments that allow for changes in different components of Cartpole and Atari games.

## Scaling up Logical Query Embeddings on Knowledge Graphs

### TL;DR

We proposed scalable multi-hop reasoning KG embedding, which scales a series of existing multi-hop query embedding approaches to 1500 times larger knowledge graphs in a single machine.

### Abstract

Embedding-based methods have demonstrated promising results for answering complex multi-hop logical queries on noisy and incomplete knowledge graphs (KGs). However, these new approaches bring unique algorithmic and system challenges for scalability. On the algorithmic side, when generating training data, first-order logical formulas need to be verified in order to find positive/answer entities as well as negative/non-answer entities. The complexity of this verification grows exponentially with the query size. On the system side, executing a multi-hop query on a KG covers a large subgraph, which makes the parallel training via graph partitioning prohibitive in a multi-GPU environment. Here we propose Scalable multi-hop Reasoning KG embedding (SrKG), a versatile framework scaling multi-hop query embeddings to KGs that are 1,500x larger than considered before. SrKG can easily train embeddings on a Freebase KG with more than 86M nodes and 338M edges on a single machine. SrKG develops a novel bidirectional rejection sampling, achieving a square root reduction of the complexity of online query generation. Furthermore, SrKG designs asynchronous scheduling, which overlaps the CPU-based query sampling, GPU-based embedding computation and frequent CPU/GPU IO. SrKG significantly increases the throughput by over 99.5% with minimum GPU requirement (2GB for training 400-dim embeddings on 86M-node Freebase) and achieves almost linear speed-up with the number of GPUs.

## Selecting Treatment Effects Models for Domain Adaptation Using Causal Knowledge

### TL;DR

We take advantage of the invariance of causal graphs across domains and propose a novel model selection metric for individualized treatment effect models in the unsupervised domain adaptation setting. 

### Abstract

While a large number of causal inference models for estimating individualized treatment effects (ITE) have been developed, selecting the best one poses a unique challenge since the counterfactuals are never observed. The problem is challenged further in the unsupervised domain adaptation (UDA) setting where we have access to labeled samples in the source domain, but desire selecting an ITE model that achieves good performance on a target domain where only unlabeled samples are available. Existing selection techniques for UDA are designed for predictive models and are sub-optimal for causal inference because they (1) do not account for the missing counterfactuals and (2)  only examine the discriminative density ratios between the input covariates in the source and target domain and do not factor in the model's predictions in the target domain. We leverage the invariance of causal structures across domains to introduce a novel model selection metric specifically designed for ITE models under the UDA setting. We propose selecting models whose predictions of the effects of interventions satisfy invariant causal structures in the target domain. Experimentally, our method selects ITE models that are more robust to covariate shifts on several synthetic and real healthcare datasets, including estimating the effect of ventilation in COVID-19 patients from different geographic locations.

## Explainability hypergraphs: a new model for axiomatic explainability scores

### TL;DR

None

### Abstract

The Shapley value is one of the most popular explainability methods.
Key to its success is a set of axioms which characterizes it. However, these axioms were originally designed for attribution in transferable utility games and we argue they are overly restrictive in the practical context of XAI.
We revisit these axioms and isolate the subset of most essential properties for XAI and we construct a directed hypergraph which captures these properties. Then, we use recent techniques from network theory to compute a score for each feature. These allow us to design new explainability methods as well as recovering numerous existing ones, including multiple variants of Shapley values.
This framework offers a graph theoretic perspective on model explanations alternative to the game theoretic one and further opens up to the design of new explainability methods that exploit graph mining techniques.

## Structured Message Passing Networks

### TL;DR

None

### Abstract

Convolutional Neural Networks (CNNs) and self-attentions have achieved 
great success on data like images, natural languages and time series, 
thanks to the inherent spatial or sequential structures underlying those data. 
However, most data lie in $d$-dimensional feature space without such 
explicit structures, thus hindering their applications. In this paper, we propose 
Structured Message Passing Networks (SMPNet), a general learning architecture 
to model the non-linearity of data in $d$-dimension without spatial
or sequential structures known a priori. Basically, SMPNet adaptively learns the latent 
graph structure among variables, and does message passing via the latent graph. 
We further study the expressive power of SMPNet, and show that it can simply 
reduce to classic multi-layer perceptrons and factorization machines. 
Extensive experiments on image, recommendation, and other benchmark 
data show the superiority of SMPNet compared with classic learning approaches.

## ScheduleNet: Learn to solve multi-agent scheduling problems with reinforcement learning

### TL;DR

None

### Abstract

We propose ScheduleNet, a RL-based real-time scheduler, that can solve various types of multi-agent scheduling problems. We formulate these problems as a semi-MDP with episodic reward (makespan) and learn ScheduleNet, a decentralized decision-making policy that can effectively coordinate multiple agents to complete tasks. The decision making procedure of ScheduleNet includes: (1) representing the state of a scheduling problem with the agent-task graph, (2) extracting node embeddings for agent and tasks nodes, the important relational information among agents and tasks, by employing the type-aware graph attention (TGA), and (3) computing the assignment probability with the computed node embeddings. We validate the effectiveness of ScheduleNet as a general learning-based scheduler for solving various types of multi-agent scheduling tasks, including multiple salesman traveling problem (mTSP) and job shop scheduling problem (JSP).


## Spatiotemporal Modeling via Physics-aware Causality

### TL;DR

We propose physics-aware graph-based spatiotemporal networks with a causal module, which leverages additional causal information described in partial differential equations (PDEs) in physical systems.

### Abstract

Deep neural networks are highly expressive and able to learn unspecified representations by minimizing a specified objective from a given task. Despite its efficient data-driven learning, interpretable inductive biases can be beneficial for constructing robust models as well as learning process. In this work, we propose physics-aware graph-based spatiotemporal networks with a causal module, which leverages additional causal information described in partial differential equations (PDEs) in physical systems. With the partially provided causality labels, we enable to specify causal weights from spatially close and temporally past observations to current observations via semi-supervised learning, and differentiate the importance of each relation without requiring costly computation. Extensive experiments on simulated time series based on causal relations and real-world graph signals show that the proposed model improves prediction performance by utilizing physics-based domain knowledge.

## Adversarial Attack on Graph Neural Networks as An Influence Maximization Problem

### TL;DR

We establish a connection between adversarial attack on graph neural networks and the influence maximization problem and propose a group of effective black-box attack strategies based on this connection.

### Abstract

Graph neural networks (GNNs) have attracted increasing interests. With broad deployments of GNNs in real-world applications, there is an urgent need for understanding the robustness of GNNs under adversarial attacks, especially in realistic setups. In this work, we study the problem of attacking GNNs in a restricted and realistic setup, by perturbing the features of a small set of nodes,  with no access to model parameters and model predictions. Our formal analysis draws a connection between this type of attacks and an influence maximization problem on the graph. This connection not only enhances our understanding on the problem of adversarial attack on GNNs, but also allows us to propose a group of effective and practical attack strategies. Our experiments verify that the proposed attack strategies significantly degrade the performance of three popular GNN models and outperform baseline adversarial attack strategies. 

## FlowX: Towards Explainable Graph Neural Networks via Message Flows

### TL;DR

None

### Abstract

We investigate the explainability of graph neural networks (GNNs) as a step towards elucidating their working mechanisms. While most current methods focus on explaining graph nodes, edges, or features, we argue that, as the inherent functional mechanism of GNNs, message flows are more natural for performing explainability. To this end, we propose a novel method here, known as FlowX, to explain GNNs by identifying important message flows. To quantify the importance of flows, we propose to employ the concept of Shapley values from cooperative game theory. To tackle the complexity of computing Shapley values, we propose an approximation scheme to compute Shapley values as initial assessments of flow importance. We then propose a learning algorithm to refine scores and improve explainability. Experimental studies on both synthetic and real-world datasets demonstrate that our proposed FlowX leads to improved explainability of GNNs.

## Adaptive and Interpretable Graph Convolution Networks Using Generalized Pagerank

### TL;DR

None

### Abstract

We investigate adaptive layer-wise graph convolution in deep GCN models. We propose AdaGPR to learn generalized Pageranks at each layer of a GCNII network to induce adaptive convolution. We show that the generalization bound for AdaGPR is bounded by a polynomial of the eigenvalue spectrum of the normalized adjacency matrix in the order of the number of generalized Pagerank coefficients.  By analysing the generalization bounds we show that  oversmoothing depends on both the  convolutions by the higher orders of the normalized adjacency matrix and the depth of the model.  We performed evaluations on node-classification using benchmark real data and show that AdaGPR provides improved accuracies compared to existing graph convolution networks while demonstrating robustness against oversmoothing. Further, we demonstrate that analysis of coefficients of layer-wise generalized Pageranks allows us to qualitatively understand  convolution at each layer enabling model interpretations.  

## Topology-Aware Network Pruning using Multi-stage Graph Embedding and Reinforcement Learning

### TL;DR

None

### Abstract

Model compression is an essential technique for deploying deep neural networks (DNNs) on power and memory-constrained resources. However,  existing model-compression methods often rely on human expertise and focus on parameters' local importance, ignoring the rich topology information within DNNs. 
In this paper, we propose a novel multi-stage graph embedding technique based on graph neural networks (GNNs) to identify DNN topologies and use reinforcement learning (RL) to find a suitable compression policy. We performed resource-constrained (i.e., FLOPs) channel pruning and compared our approach with state-of-the-art model compression methods.
We evaluated our method on various models from typical to mobile-friendly networks, such as ResNet family, VGG-16, MobileNet-v1/v2, and ShuffleNet. 
Results show that with minimal fine-tuning, our method can achieve higher compression ratios with outstanding and competitive performance.

## Learn Locally, Correct Globally: A Distributed Algorithm for Training Graph Neural Networks

### TL;DR

None

### Abstract

Despite the recent success of Graph Neural Networks (GNN) in learning graph-based structures for a wide variety of applications, training a GNN on large graphs still remains challenging. This is mainly due to the limited resource capacities of state-of-the-art compute/server platforms and the unique structure of GNNs, which has spurred the need for the design of an effective distributed algorithm for training these networks. However, employing current distributed deep neural networks techniques to GNNs is not straightforward, due to dependency of data samples (nodes) distributed among different workers. Current approaches for training distributed GNNs impose either excessive communication costs or large memory overheads to train the network. In this paper, we propose and evaluate a communication-efficient technique for training distributed GNNs, by applying correction in the parameter server to refine the locally-learned models. Unlike existing attempts, our method does not require additional feature transmissions between server and workers, and also keeps the local workers completely unchanged. Furthermore, to the best of our knowledge, this is the first work that theoretically analyzes the convergence of distributed GNN with periodic averaging, and shows that, by carefully choosing the number of server correction steps, the server can preserve the global graph structure by trading computation for communication and adjusting the locally-learned models accordingly. Additionally, we provide corroborating empirical evidence that the global correction can significantly improve the performance of distributed GNNs on large, real-world datasets. 

## A Thorough View of Exact Inference in Graphs from the Degree-4 Sum-of-Squares Hierarchy

### TL;DR

None

### Abstract

Performing inference in graphs is a common task within several machine learning problems, e.g., image segmentation, community detection, among others. 
	For a given undirected connected graph, we tackle the statistical problem of \textit{exactly} recovering an unknown ground-truth binary labeling of the nodes from a \textit{single} corrupted observation of each edge.
	Such problem can be formulated as a quadratic combinatorial optimization problem over the boolean hypercube, where it has been shown before that one can (with high probability and in polynomial time) exactly recover the ground-truth labeling of graphs that have an isoperimetric number that grows with respect to the number of nodes (e.g., complete graphs, regular expanders).
	In this work, we apply a powerful hierarchy of relaxations, known as the sum-of-squares (SoS) hierarchy, to the combinatorial problem.
	Motivated by empirical evidence on the improvement in exact recoverability, we center our attention on the degree-4 SoS relaxation and set out to understand the origin of such improvement from a graph theoretical perspective.
	We show that the solution of the dual of the relaxed problem is related to finding edge weights of the Johnson and Kneser graphs, where the weights fulfill the SoS constraints and intuitively allow the input graph to increase its algebraic connectivity.
	Finally, as byproduct of our analysis, we derive a novel Cheeger-type lower bound for the algebraic connectivity of graphs with \textit{signed} edge weights.

## Triangle and Four Cycle Counting with Predictions in Graph Streams

### TL;DR

We propose algorithms for counting triangles and four cycles in graph streams with the aid of ML predictors.

### Abstract

We propose data-driven one-pass streaming algorithms for estimating the number of triangles and four cycles, two fundamental problems in graph analytics that are widely studied in the graph data stream literature. Recent works have applied machine learning techniques in other data stream problems, developing a trained oracle that can predict certain properties of an item and using it to improve on prior "classical" algorithms that did not use oracles. In this paper, we explore the power of a "heavy edge" oracle in multiple graph edge streaming models. In the adjacency list model, we present a one-pass triangle counting algorithm improving upon the previous space upper bounds without such an oracle. In the arbitrary order model, we present algorithms for both triangle and four cycle estimation with fewer passes and the same space complexity as in previous algorithms, and we show several of these bounds are optimal even with such an oracle. We analyze our algorithms under several noise models, showing that the algorithms perform well even when the oracle errs. Our methodology expands upon prior work on "classical" streaming algorithms, as previous multi-pass and random order streaming algorithms can be seen as special cases of our algorithms, where the first pass or random order was used to implement the heavy edge oracle. Lastly, our experiments demonstrate advantages of the proposed method compared to state-of-the-art streaming algorithms.

## Learning NP-Hard Joint-Assignment Scheduling: Inference on a Random Graph and Provable Auction-Fitted Q-iteration

### TL;DR

A GNN-based random graph embedding theory is developed, motivated by the problem of learning joint-assignment scheduling. Towards scalable joint assignment and training, an approximation algorithm with a provable performance guarantee was developed.

### Abstract

We develop a theory of inference on a random graph using graph neural networks (GNN) and illustrate its capability to solve NP-hard scheduling problems. We apply the theory to address the challenge of developing a near-optimal learning algorithm to solve the NP-hard problem of scheduling multiple robots/machines with time-varying rewards. In particular, we consider a class of robot/machine scheduling problems called the multi-robot reward collection problem (MRRC). Such MRRC problems well model ride-sharing, pickup-and-delivery, and a variety of related problems. In representing the MRRC problem as a sequential decision-making problem, we observe that each state can be represented as an extension of probabilistic graphical models (PGMs), which we refer to as random PGMs. We then develop a mean-field inference method for random PGMs. We prove that a simple modification of a typical GNN embedding is sufficient to embed a random graph even when the edge presence probabilities are interdependent. We then propose (1) an order-transferable Q-function estimator and (2) an order-transferability-enabled auction to select a joint assignment in polynomial-time. These result in a reinforcement learning framework with at least 1-1/e optimality. Experimental results on solving MRRC problems highlight the near-optimality and transferability of the proposed methods. We also consider minimax multiple traveling salesman problems (minimax-mTSP) and identical parallel machine scheduling problems (IPMS) in the Appendix. 

## Auto-Encoding Knowledge Graph for Unsupervised Medical Report Generation

### TL;DR

We make the first attempt to train a medical report generation model in an unsupervised manner.

### Abstract

Medical report generation, which aims to automatically generate a long and coherent report of a given medical image, has been receiving growing research interests. Existing approaches mainly adopt a supervised manner and heavily rely on coupled image-report pairs. However, in the medical domain, building a large-scale image-report paired dataset is both time-consuming and expensive. To relax the dependency on paired data, we propose an unsupervised model Knowledge Graph Auto-Encoder (KGAE) which accepts independent sets of images and reports in training. KGAE consists of a pre-constructed knowledge graph, a knowledge-driven encoder and a knowledge-driven decoder. The knowledge graph works as the shared latent space to bridge the visual and textual domains; The knowledge-driven encoder projects medical images and reports to the corresponding coordinates in this latent space and the knowledge-driven decoder generates a medical report given a coordinate in this space. Since the knowledge-driven encoder and decoder can be trained with independent sets of images and reports, KGAE is totally unsupervised. The experiments show that the unsupervised KGAE generates desirable medical reports without using any image-report training pairs, and even outperforms several supervised models. Moreover, KGAE can also work in both semi-supervised and supervised settings, and accept paired images and reports in training. By further fine-tuning with image-report pairs, KGAE consistently outperforms the current state-of-the-art models over different metrics on two benchmark datasets.

## Relational Multi-Task Learning: Modeling Relations between Data and Tasks

### TL;DR

None

### Abstract

A key assumption in multi-task learning is that test data points have no access to their labels from other tasks when predicting labels on a new task. This presents an opportunity to utilize available auxiliary task labels and this way improve the performance on the new task. Here we introduce a new relational multi-task learning setting where we leverage data point labels on auxiliary tasks to predict a new task. We develop MetaLink, where our key innovation is to build a knowledge graph that connects data points and tasks and thus allows us to leverage auxiliary labels.  The knowledge graph consists of two types of nodes: (1) data nodes, where node features are data embeddings computed by the neural network, and (2) task nodes, with the last layer's weights for each task as node features. The edges in this knowledge graph capture data-task relationships, and the edge label captures the label of a data point on a particular task. Under MetaLink, we reformulate the new task as a link label prediction problem between a data node and a task node. MetaLink framework provides flexibility to model knowledge transfer from auxiliary task labels to the task of interest. We evaluate MetaLink on 6 benchmark datasets in both biochemical and vision domains. Experiments demonstrate that MetaLink can successfully utilize the relations among different tasks, outperforming the state-of-the-art methods under the proposed relational multi-task learning setting, with up to 27% improvement in ROC AUC.

## GIST: Distributed Training for Large-Scale Graph Convolutional Networks

### TL;DR

We propose a novel distributed training methodology for GCN models, which significantly reduces wall-clock training time and allows GCNs of markedly large hidden dimensions to be trained. 

### Abstract

The graph convolutional network (GCN) is a go-to solution for machine learning on graphs, but its training is notoriously difficult to scale both in terms of graph size and the number of model parameters. Although some work has explored training on large-scale graphs (e.g., GraphSAGE, ClusterGCN, etc.), we pioneer efficient training of large-scale GCN models (i.e., ultra-wide, overparameterized models) with the proposal of a novel, distributed training framework. Our proposed training methodology, called GIST, disjointly partitions the parameters of a GCN model into several, smaller sub-GCNs that are trained independently and in parallel. In addition to being compatible with any GCN architecture, GIST improves model performance, scales to training on arbitrarily large graphs, significantly decreases wall-clock training time, and enables the training of markedly overparameterized GCN models. Remarkably, with GIST, we train an astonishingly wide 32,768-dimensional GraphSAGE model to SOTA performance on the Amazon2M dataset.

## 3DP3: 3D Scene Perception via Probabilistic Programming

### TL;DR

3D Vision Architecture based on Probabilistic Programs

### Abstract

Humans learn to parse scenes more robustly than deep learning vision systems, generalizing across large variations in viewpoint, occlusion, lighting, and clutter. In this work, we present an probabilistic programming architecture for parsing scenes using object models learned from 5 or fewer views. Our architecture called 3DP3 uses voxelized models of object shape, a generative 3D scene graph prior that compositionally represents scenes using shapes and contacts between them, and a likelihood model based on real-time graphics. Our inference algorithm accurately parses scenes, using fast bottom-up pose proposals and novel involutive MCMC updates on scene graph structure. We show this approach allows for rapid object learning, and scene parsing that leverages physical constraints. Our quantitative results demonstrate better generalization to scenes with novel viewpoints, contact, and occlusions than is achieved by deep learning baselines, and accurate parsing of real scenes using neural bottom-up proposals.

## EvoGrad: Efficient Gradient-Based Meta-Learning and Hyperparameter Optimization

### TL;DR

Efficient gradient-based meta-learning and hyperparameter optimization inspired by evolutionary methods

### Abstract

Gradient-based meta-learning and hyperparameter optimization have seen significant progress recently, enabling practical end-to-end training of neural networks together with many hyperparameters. Nevertheless, existing approaches are relatively expensive as they need to compute second-order derivatives and store a longer computational graph. This cost prevents scaling them to larger network architectures. We present EvoGrad, a new approach to meta-learning that draws upon evolutionary techniques to more efficiently compute hypergradients. EvoGrad estimates hypergradient with respect to hyperparameters without calculating second-order gradients, or storing a longer computational graph, leading to significant improvements in efficiency. We evaluate EvoGrad on two substantial recent meta-learning applications, namely cross-domain few-shot learning with feature-wise transformations and noisy label learning with MetaWeightNet. The results show that EvoGrad significantly improves efficiency and enables scaling meta-learning to bigger CNN architectures such as from ResNet18 to ResNet34. 

## Preserve, Promote, or Attack? GNN Explanation via Topology Perturbation

### TL;DR

Proposed an interactive and multi-purpose visualization system, termed GNNViz, which can Preserve, Promote, or Attack GNN’s predictions

### Abstract

Prior works on formalizing explanations of a graph neural network (GNN) focus on a single use case --- to Preserve the prediction results through identifying important edges and nodes. In this paper, we develop a multi-purpose interpretation framework by acquiring a mask that indicates topology perturbations of the input graphs. We pack the framework into an interactive visualization system (GNNViz) which can fulfill multiple purposes: Preserve, Promote, or Attack GNN’s predictions. We illustrate our approach's novelty and effectiveness with three case studies: First, GNNViz can assist non-expert users to easily explore the relationship between graph topology and GNN's decision (Preserve), or to manipulate the prediction (Promote or Attack) for an image classification task on MS--COCO; Second, on the Pokec social network dataset, our framework can uncover unfairness and demographic biases; Lastly, it compares with state-of-the-art GNN explainer baseline on a synthetic dataset.

## Hierarchical Embedding for Hyper-Networks

### TL;DR

Introducing a novel, multi-level approach to find the  embedding of nodes and hyperedges of a hypergraph in an unsupervised manner.

### Abstract

Many problems such as vertex classification and link prediction in network data
can be solved using graph embeddings, and a number of algorithms are known for constructing
such embeddings. However, it is difficult to use graphs to capture 
non-binary relations such as communities of vertices. These kinds of 
complex relations are expressed more naturally as hypergraphs.
While hypergraphs are a generalization of graphs, state-of-the-art graph
embedding techniques are not adequate for solving prediction and
classification tasks on large hypergraphs accurately in reasonable time.
In this paper, we introduce NetVec, a novel hierarchical framework for scalable unsupervised hypergraph embedding, that can be coupled with any existing network
embedding algorithm to reduce the amount of compute time and energy required to learn embeddings of hypergraphs. NetVec generates embedding of a hypergraph with 
millions of nodes and hyperedges orders of magnitude faster than the state-of-the-art.

## STR-GODEs: Spatial–Temporal-Ridership Graph ODEs for Metro Ridership Prediction

### TL;DR

We extended Neural ODE algorithms to the graph network and proposed the STR-GDE network for metro ridership prediction

### Abstract

The metro ridership prediction has always received extensive attention from governments and researchers. Recent works focus on designing complicated graph convolutional recurrent network architectures to capture spatial and temporal patterns. These works extract the information of spatial dimension well, but the limitation of temporal dimension still exists. We extended Neural ODE algorithms to the graph network and proposed the STR-GODE network, which can effectively learn spatial, temporal, and ridership correlations without the limitation of dividing data into equal-sized intervals on the timeline. While learning the spatial relations and the temporal correlations, we modify the GODE-RNN cell to obtain the ridership feature and hidden states. Ridership information and its hidden states are added to the GODESolve to reduce the error accumulation caused by long time series in prediction. Extensive experiments on two large-scale datasets demonstrate the efficacy and robustness of our model.

## Hypergraph Transformer for Compositional Question Answering

### TL;DR

we argue the importance of the core level of abstraction of inputs, and suggest the hypergraph-based representation to enhance compositional reasoning ability of the transformer.

### Abstract

Despite the fact that machine learning methods have achieved remarkable development on various problems, a considerable gap exists between machine-level recognition tasks and human-level reasoning. Here, we focus on compositional question answering which requires complex reasoning over the external knowledge graphs. The core of this problem is how to construct a query-aware knowledge graph (KG) from the massive number and incomplete nature of facts, and reason over the knowledge graphs to find the answer. Here, we construct the query-aware KG in a form of hypergraph, consisting of a set of facts extracted from multi-hop graph walk. We then introduce a transformer-based co-attention method to consider compositional relationships between the hypergraphs and given queries. Extensive experiments over two tasks, which are knowledge-based question answering (QA) and knowledge-aware visual QA, demonstrate the accuracy and efficiency of our model for compositional reasoning tasks.

## Neural Relational Inference with Node-Specific Information 

### TL;DR

We propose a method to infer the interaction among entities in a multi-agent system, where each entity can potentially have access to some individualized information

### Abstract

Inferring interactions among entities is an important problem in studying dynamical systems, which greatly impacts the performance of downstream tasks, such as prediction. In this paper, we tackle this problem in a setting where each entity can potentially have a set of individualized information that other entities cannot have access to. Specifically, we represent the system using a graph in which the individualized information become node-specific information (NSI). We build our model in the framework of Neural Relation Inference (NRI), where the interaction among entities are interpretably uncovered using variational inference. We adopt NRI model to incorporate the individualized information by introducing private nodes in the graph that represent NSI. Such representation enables us to uncover more accurate relations among the agents and therefore leads to better performance on the downstream tasks. Our experiment results over real-world datasets validate the merit of our proposed algorithm. 

## Second-Order Unsupervised Feature Selection via Knowledge Contrastive Distillation

### TL;DR

None

### Abstract

Unsupervised feature selection aims to select a subset from the original features that are most useful for the downstream tasks without external guidance information. While most unsupervised feature selection methods focus on ranking features based on the intrinsic properties of data, they do not pay much attention to the relationships between features, which often leads to redundancy among the selected features. In this paper, we propose a two-stage Second-Order unsupervised Feature selection via knowledge contrastive disTillation (SOFT) model that incorporates the second-order covariance matrix with the first-order data matrix for unsupervised feature selection. In the first stage, we learn a sparse attention matrix that can represent second-order relations between features. In the second stage, we build a relational graph based on the learned attention matrix and perform graph segmentation for feature selection. Experimental results on 12 public datasets demonstrate the effectiveness of our proposed method.

## Learning to Extend Program Graphs to Work-in-Progress Code

### TL;DR

We extend the notion of program graphs to work-in-progress code by learning to predict edge relations and show improved performance on fixing variable misuse bugs and code completion. 

### Abstract

Source code spends most of its time in a broken or incomplete state during software development. This presents a challenge to machine learning for code, since high-performing models typically rely on graph structured representations of programs derived from traditional program analyses. Such analyses may be undefined for broken or incomplete code. We extend the notion of program graphs to work-in-progress code by learning to predict edge relations between tokens, training on well-formed code before transferring to work-in-progress code. We consider the tasks of code completion and localizing and repairing variable misuse in a work-in-process scenario. We demonstrate that training relation-aware models with fine-tuned edges consistently leads to improved performance on both tasks.

## Coupled Segmentation and Edge Learning via Dynamic Graph Propagation

### TL;DR

A coupled learning framework for joint semantic segmentation and semantic edge detection

### Abstract

Image segmentation and edge detection are both central problems in perceptual grouping. While segmentation can be easily transformed into contour edges, the converse is nontrivial since general edges may not always form closed contours. In this paper, we propose a principled end-to-end framework for coupled edge and segmentation learning. At the core of our framework is a recurrent module termed as dynamic graph propagation (DGP) layer that performs message passing on dynamically constructed graphs. The layer uses learned gating to dynamically select neighbors for message passing using max-pooling. The output from message passing is further gated with an edge signal to refine segmentation. Experiments demonstrate that the proposed framework is able to let both tasks mutually improve each other. Our method achieves 82.4%/83.3% mIoU on the Cityscapes validation set with a ResNet-101/ResNet-38 backbone. The best model also achieves 78.7% maximum F-score in semantic edge detection, achieving state-of-the-art performance on multiple tasks.

## DenseGAP: Graph-Structured Dense Correspondence Learning with Anchor Points

### TL;DR

None

### Abstract

Establishing dense correspondence between two images is a fundamental problem in computer vision, typically tackled by matching local feature descriptors. However, such local features are insufficient for disambiguating similar regions due to the lack of global awareness. It is also highly expensive and memory-intensive to compute the correlation between every pair of points across images. To efficiently make the local features aware of the global context and improve their matching accuracy for building dense correspondence, we introduce DenseGAP, a novel and efficient solution for Dense correspondence learning with a Graph-structured neural network, conditioned on Anchor Points. Specifically, we first propose a graph structure that utilizes anchor points to provide sparse but strong prior for inter- and intra-image context and connects them to all image points via directed edges. Then we design a novel network to efficiently generate high-resolution feature maps at a low memory cost by using multiple lightweight message-passing layers to broadcast information across all the edges. Finally, based on the predicted feature maps, we present a coarse-to-fine framework of producing accurate correspondence using cycle consistency. Our learned feature descriptors capture both local and global information, enabling a continuous feature field for querying in arbitrary resolution. By analyzing our network with comprehensive ablative experiments and evaluating its performance on large-scale public indoor and outdoor datasets, we demonstrate its significant advantages in solving correspondence problems.

## Slimmable Aggregation for Graph Neural Networks via Reinforcement Learning

### TL;DR

None

### Abstract

Prior multi-hop aggregation schemes in graph neural networks (GNNs) typically treat high-order neighbors independently in message passing, thereby ignoring the topological correlations among the nodes at various distances. In this paper, we explicitly account for the dependencies among multi-hop neighbors, by developing a path-based multi-hop aggregation framework that aggregates information across high-order paths at every GNN layer. Despite the improved utilization of long-range information, the proposed path-based method is, unfortunately, prone to heavy computational cost. This discovery further motivates us to devise slimmable aggregation schemes to enhance the propagation efficiency, where only a subset of critical paths is sampled and leveraged for the subsequent feature aggregation. Towards this end, we propose a reinforcement-learning~(RL) based slimmable aggregator that learns to slim the redundant structural elements and perform high-order message passing in one stage. This is achieved by modeling the graph slimming problem as a sequential decision making process, where an elaborated RL agent is trained to generate discrete decisions via policy gradient. To improve the convergence performance of the RL agent, we also devise a couple of reward attribution and negative sampling techniques. Furthermore, the proposed RL-based slimmable method also triggers novel aggregation functionality, enabling the control over the tradeoff between speed and performance at inference. Experiments on several benchmarks demonstrate that the proposed slimmable aggregator yields results superior to the state of the art. Our code will be made publicly available.

## Approximate Frank-Wolfe Algorithms over Graph-structured Support Sets

### TL;DR

We propose approximated Frank-Wolfe algorithms to solve convex optimization problems over graph-structured support sets.

### Abstract


In this paper, we propose approximate Frank-Wolfe (FW) algorithms to solve convex optimization problems over graph-structured support sets where the \textit{linear minimization oracle} (LMO) cannot be efficiently obtained in general. We first demonstrate that two popular approximation assumptions (\textit{additive} and \textit{multiplicative gap errors)}, are not valid for our problem, in that no cheap gap-approximate LMO oracle exists in general. Instead, a new \textit{approximate dual maximization oracle} (DMO) is proposed, which approximates the inner product rather than the gap. When the objective is $L$-smooth, we prove that the standard FW method using a $\delta$-approximate DMO converges as $\mathcal{O}(L / \delta t + (1-\delta)(\delta^{-1} + \delta^{-2}))$ in general, and as $\mathcal{O}(L/(\delta^2(t+2)))$ over a $\delta$-relaxation of the constraint set. Additionally, when the objective is $\mu$-strongly convex and the solution is unique, a variant of FW converges to $\mathcal{O}(L^2\log(t)/(\mu \delta^6 t^2))$ with the same per-iteration complexity. Our empirical results suggest that even these improved bounds are pessimistic, with significant improvement in recovering real-world images with graph-structured sparsity.

## Dirichlet Energy Constrained Learning for Deep Graph Neural Networks

### TL;DR

None

### Abstract

Graph neural networks (GNNs) integrate deep architectures and topological structure modeling in an effective way. However, the performance of existing GNNs would decrease significantly when they stack many layers, because of the over-smoothing issue. Node embeddings tend to converge to similar vectors when GNNs keep recursively aggregating the representations of neighbors. To enable deep GNNs, several methods have been explored recently. But they are developed from either techniques in convolutional neural networks or heuristic strategies. There is no generalizable and theoretical principle to guide the design of deep GNNs. To this end, we analyze the bottleneck of deep GNNs by leveraging the Dirichlet energy of node embeddings, and propose a generalizable principle to guide the training of deep GNNs. Based on it, a novel deep GNN framework -- EGNN is designed. It could provide lower and upper constraints in terms of Dirichlet energy at each layer to avoid over-smoothing. Experimental results demonstrate that EGNN achieves state-of-the-art performance by using deep layers.

## SalKG: Learning From Knowledge Graph Explanations for Commonsense Reasoning

### TL;DR

Train KG-augmented models using extra supervision from KG saliency explanations.

### Abstract

Augmenting pre-trained language models with knowledge graphs (KGs) has achieved success on various commonsense reasoning tasks. However, for a given task instance, the KG, or certain parts of the KG, may not be useful.
Although KG-augmented models often use attention to focus on specific KG components, the KG is still always used, and the attention mechanism is never explicitly taught which KG components should be used. Meanwhile, saliency methods \cite{bastings2020elephant} can measure how much a KG feature (\textit{e.g.}, graph, node, path) influences the model to make the correct prediction, thus explaining which KG features are useful. This paper explores how saliency explanations can be used to improve KG-augmented models' performance. First, we propose to create coarse (\textit{Is the KG useful?}) and fine (\textit{Which nodes/paths in the KG are useful?}) saliency explanations. Second, to motivate saliency-based supervision, we analyze oracle KG-augmented models which directly use saliency explanations as extra inputs for guiding their attention. Third, we propose \textsc{SalKG}, a framework for KG-augmented models to learn from coarse and/or fine saliency explanations. Given saliency explanations created from a task's training set, \textsc{SalKG} jointly trains the model to predict the explanations, then solve the task by attending to KG features highlighted by the predicted explanations. On three popular commonsense QA benchmarks (CSQA, OBQA, CODAH), we show that \textsc{SalKG} models can yield large performance gains --- up to 3.27\% on CSQA.

## Learning ground states of quantum Hamiltonians with graph networks

### TL;DR

None

### Abstract

Solving for the lowest energy eigenstate of the many-body Schrodinger equation is a cornerstone problem that hinders understanding of a variety of quantum phenomena. The difficulty arises from the exponential nature of the Hilbert space which casts the governing equations as an eigenvalue problem of exponentially large, structured matrices. Variational methods approach this problem by searching for the best approximation within a lower-dimensional variational manifold. In this work we use graph neural networks to define a structured variational manifold and optimize its parameters to find high quality approximations of the lowest energy solutions on a diverse set of Heisenberg Hamiltonians. Using graph networks we learn distributed representations that by construction respect underlying physical symmetries of the problem and generalize to problems of larger size. Our approach achieves state-of-the-art results on a set of quantum many-body benchmark problems and works well on problems whose solutions are not positive-definite. The discussed techniques hold promise of being a useful tool for studying quantum many-body systems and providing insights into optimization and implicit modeling of exponentially-sized objects.

## SkipNode: On Alleviating Over-smoothing for Graph Convolutional Networks

### TL;DR

None

### Abstract

Over-smoothing is one severe problem of Deep Graph Convolutional Networks (GCNs). However, most existing researches on alleviating over-smoothing still lack generality or efficiency. In this paper, we comprehensively analyze the over-smoothing problem and accordingly propose the SkipNode, a simple and effective manner to alleviate over-smoothing for deep GCNs. Our SkipNode randomly selects nodes to skip their updating messages by backtracking the nodes' input information at the end of each layer of GCN. Theoretically, backtracking node information reduces the degeneration of feature diversity caused by deep GCN layers. Skip manner enables gradient to be directly passed back, thus stopping gradient vanishing and weight over-decaying. Experimentally, the nodes that are not selected will maintain the graph updating process, therefore reducing the probable structural information loss. We conducted extensive experiments on seven datasets to demonstrate the superiority of SkipNode. In specific, the SkipNode has strong generality that could improve most GCN-based methods, including GCN/ResGCN/JKNet/GCNII. Besides, the SkipNode has better efficiency by outperforming the commonly used DropEdge and DropNode. All codes are provided in the supplemental materials and will be released on GitHub.

## Few-Shot Molecular Graph Property Prediction using Molecular Rationales

### TL;DR

 

### Abstract

The primary difficulty of predicting molecular properties usually lies in the scarcity of labeled training samples for new tasks. Meta learning algorithm is an efficient solution to tackle this barrier while its usage on molecular property prediction is still under-explored. Here, we propose Proto-MGNN, a novel meta learning algorithm to solve the few-shot molecular property prediction problem. Proto-MGNN follows the general framework of model-agnostic meta learning and further introduces the prototypical networks module to learn prototype cluster representations of each class. We construct pre-defined molecular rationales by extracting vocabularies of structural motifs from dataset so that Proto-MGNN could leverage these rationales to augment data samples when learning prototype cluster representations. Empirical results for five popular benchmark datasets demonstrate that Proto-MGNN has obtained significant performance gains. Unlike previous methods, Proto-MGNN is more interpretable since human experts can easily manipulate molecular rationales with domain knowledge. 

## Structured Sparse R-CNN for Direct Scene Graph Generation

### TL;DR

None

### Abstract

Scene graph generation (SGG) is to detect entity pairs with their relations in an image.  Existing SGG approaches often use multi-stage pipelines to decompose this task into object detection, relation graph construction, and dense or dense-to-sparse relation prediction. Instead, from a perspective on SGG as a direct set prediction, this paper presents a simple, sparse, and unified framework for relation detection, termed as Structured Sparse R-CNN. The key to our method is a set of learnable triplet queries and structured triplet detectors which could be jointly optimized from the training set in an end-to-end manner. Specifically, the triplet queries encode the general prior for entity pair locations, categories, and their relations, and provide an initial guess of relation detection for subsequent refinement. The triplet detector presents a cascaded dynamic head design to progressively refine the results of relation detection. In addition, to relieve the training difficulty of Structured Sparse R-CNN, we propose a relaxed and enhanced training strategy based on knowledge distillation from a Siamese Sparse R-CNN. We also propose adaptive focusing parameter and average logit approach for imbalance data distribution. We perform experiments on two benchmarks:  Visual Genome and Open Images, and the results demonstrate that our method achieves the state-of-the-art performance. Meanwhile, we perform in-depth ablation studies to provide insights on our structured modeling in triplet detector design and training strategies.

## Dual Hierarchical Attention Networks for Binary Heterogeneous Graph Learning

### TL;DR

None

### Abstract

Heterogeneous graphs, each of which consists of different types and edges, are pervasive in real-world scenarios, among which the binary graphs (i.e. only include two different types of nodes, e.g. author/paper, user/item.) are significant kind of heterogeneous graphs.  Heterogeneous graph learning aims to learn low-dimension node representations that could preserve both node attributes and their relation information. However, most of the existing graph neural networks were designed for homogeneous graphs, which thus are infeasible for heterogeneous graphs. Some recent methods that were proposed to learn on heterogeneous graphs also face several limitations, such as the insufficient utilization of heterogeneous information and insufficient representation ability.  In this paper, we propose DHAN, a novel \textbf{D}ual \textbf{H}ierarchical \textbf{A}ttention \textbf{N}etworks, to learn comprehensive node representations on binary heterogeneous graphs with intra-class and inter-class hierarchical attention networks. Different from existing methods, DHAN conducts hierarchical attention on the intrinsic structure of both node intra-class and inter-class graphs. The intra-class attention aims to learn the node representations from its same type of neighbors, while inter-class attention is able to aggregate node representations from its different types of neighbors. The dual attention operations enable DHAN to sufficiently leverage not only the node intra-class neighboring information but also the inter-class neighboring information in binary heterogeneous graph.  Moreover, both of the intra/inter-class attention networks take advantage of hierarchical operation, which first performs node aggregation under a specific relation and then performs different relations aggregation. The hierarchical architecture design makes DHAN could provide multi-level representations with sufficient representation ability and proper interpretability. Experimental results on various tasks against the state-of-the-arts sufficiently confirm the capability of DHAN in learning node comprehensive representations on binary heterogeneous graphs.

## CSC-GCN: Contrastive Semantic Calibration for Graph Convolution Network

### TL;DR

None

### Abstract

Graph Convolutional Networks (GCNs) have been successfully applied to node representation learning in various real-world applications. However, the performance of GCNs drops rapidly when the labeled data are severely scarce, and the node features are prone to being indistinguishable with stacking more layers, causing over-fitting and over-smoothing problems. In this paper, we propose a simple yet effective Contrastive Semantic Calibration for Graph Convolution Network (CSC-GCN), which integrates stochastic identity aggregation and semantic calibration to overcome these weaknesses. The basic idea is the node features obtained from different aggregation operations should be similar. Toward that end, identity aggregation is utilized to extract semantic features from labeled nodes, while stochastic label noise is adopted to alleviate the over-fitting problem. Then, contrastive learning is employed to improve the discriminative ability of the node features, and the features from different aggregation operations are calibrated according to the class center similarity. In this way, the similarity between unlabeled features and labeled ones from the same class is enhanced while effectively reducing the over-smoothing problem. Experimental results on the three popular datasets show that the proposed CSC-GCN outperforms state-of-the-art methods on various classification tasks.


## Molecule Generation from Input-Attributions over Graph Convolutional Networks

### TL;DR

Automatically generate new molecules using Graph Convolutional Network and input-attribution methods

### Abstract

It is well known that Drug Design is often a costly process both in terms of time and economic effort. While good Quantitative Structure-Activity Relationship models (QSAR) can help to predict molecular properties without the need to synthesize them, it is still required to come up with new molecules to be tested. This is mostly done in lack of tools to determine which modifications are more promising or which aspects of a molecule are more influential for the final activity/property. Here we present an automatic process which involves Graph Convolutional Network models and input-attribution methods to generate new molecules. We also explore the problems of over-optimization and applicability, recognizing them as two important aspects in the practical use of such automatic tools.

## FedGNN: Federated Graph Neural Network for Privacy-Preserving Recommendation

### TL;DR

None

### Abstract

Graph neural network (GNN) is widely used for recommendation to model high-order interactions between users and items. Existing GNN-based recommendation methods rely on centralized storage of user-item graphs for model learning. However, user data is privacy-sensitive, and the centralized storage of user-item graphs may arouse privacy concerns and risks. In this paper, we propose a privacy-preserving GNN-based recommendation method based on federated learning, which is named FedGNN. It can collaboratively train GNN models from decentralized user data and meanwhile exploit high-order user-item interaction information with privacy well protected. In our method, we locally train GNN model in each user client based on the local user-item graph inferred from the local user-item interaction data, where the gradients of GNN are communicated between clients and a server for aggregation and local GNN model update. Since local gradients may contain private information, we apply local differential privacy techniques to the local gradients to protect user privacy. In addition, in order to protect the items that users have interactions with, we propose to incorporate randomly sampled items as pseudo interacted items for anonymity. Since local user-item interaction data only contain first-order interaction information, we propose a user-item graph expansion method to find neighboring users with co-interacted items and exchange their embeddings periodically, which can expand the local user-item graphs and propagate high-order information in a privacy-preserving way. Extensive experiments on six benchmark datasets validate that FedGNN can achieve competitive results with existing centralized GNN-based recommendation methods and meanwhile effectively protect user privacy.

## Training Graph Completion Models by Asking Inductive Logical Questions

### TL;DR

A framework that first learns logic rules from a small boostrap set then generates triples from the rules to train a graph completion model.

### Abstract

Knowledge graphs (KG) are widely used in many applications; however, they are incomplete and many methods are proposed to predict the missing triples. Recent KG completion models can achieve state-of-the-art performance when trained on a sufficiently large dataset. However, obtaining a large number of triples is difficult and many KGs suffer from the long tail distributions where relations have only a few triples. This raises a challenging question: "Can we train a completion model with a fraction of the dataset that achieves a similar performance?". To this end, we propose LogicQA, a weakly supervised framework that automatically mines human readable data labeling rules that can be evaluated and then applied to the KG to generate more triples for training. The framework consists of three components: (i) an inductive logic programming module that learns logic rules from a small bootstrap set; (ii) a question answering module that evaluates the rules with an oracle; and (iii) a knowledge distillation module that integrates the generated triples and trains the completion model. Experiments show that LogicQA outperforms strong weakly supervised methods in two knowledge graph datasets and one scene graph dataset.

## A Convergence Analysis of Gradient Descent on Graph Neural Networks

### TL;DR

We provide a convergence analysis of gradient descent for graph neural networks.

### Abstract

Graph Neural Networks~(GNNs) are a powerful class of architectures for solving learning problems on graphs. While many variants of GNNs have been proposed in the literature and have achieved strong empirical performance, their theoretical properties are less well understood. In this work we study the convergence properties of the gradient descent algorithm when used to train GNNs. In particular, we consider the realizable setting where the data is generated from a network with unknown weights and our goal is to study conditions under which gradient descent on a GNN architecture can recover near optimal solutions. While such analysis has been performed in recent years for other architectures such as fully connected feed-forward networks, the message passing nature of the updates in a GNN poses a new challenge in understanding the nature of the gradient descent updates. We take a step towards overcoming this by proving that for the case of deep linear GNNs gradient descent provably recovers solutions up to error $\epsilon$ in $O(\text{log}(1/\epsilon))$ iterations, under natural assumptions on the data distribution. Furthermore, for the case of one-round GNNs with ReLU activations, we show that gradient descent provably recovers solutions up to error $\epsilon$ in $O(\frac{1}{\epsilon^2} \log(\frac{1}{\epsilon}))$ iterations. 


## Search without Training or Data: Graph Properties for Performance Estimation in NAS

### TL;DR

Data-free and training-free performance estimation using graph properties for NAS 

### Abstract

Neural Architecture Search (NAS) has a huge environmental footprint and its computational cost is prohibitive for all but the largest institutions. As such, reducing the cost of estimating architecture performance (the main bottleneck) is a crucial task to make NAS greener and more accessible. 
In this work, we analyse the relation between the test performance of architectures and their topological features under various graph-based search spaces. By leveraging useful graphs properties we can effectively estimate the performance, and rank architectures in a zero-cost and data-independent manner, that is, without the need for any data or training.
We empirically demonstrate that our graph-properties-based performance estimators can achieve superior correlation with true rankings by test accuracy. Additionally, we evaluate this estimator for efficient NAS on two NAS settings and show that:
(i) in the zero-cost setting, it reduces the search costs to near zero but still manages to find architectures with competitive performance and generalising well on different image datasets; and 
(ii) in the hybrid setting, it is integrated into existing NAS strategies and help improve the search efficiency and performance by constraining the search space to a good subregion using the zero-cost evaluations.

## GNisi: A graph network for reconstructing Ising models from multivariate binarized data

### TL;DR

We build a graph neural network approach for solving inverse Ising problems.

### Abstract

Ising models are a simple generative approach to describing interacting binary variables. They have proven useful in a number of biological settings because they enable one to represent observed many-body correlations as the separable consequence of many direct, pairwise statistical interactions. The inference of Ising models from data can be computationally very challenging and often one must be satisfied with numerical approximations or limited precision. In this paper we present a novel method for the determination of Ising parameters from data, called GNisi, which uses a Graph Neural network trained on known Ising models in order to construct the parameters for unseen data. We show that GNisi is more accurate than the existing state of the art software, and we illustrate our method by applying GNisi to gene expression data.

## Distilling Meta Knowledge on Heterogeneous Graph for Illicit Drug Trafficker Detection on Social Media

### TL;DR

create a novel dataset and develop a novel heterogeneous graph model for illicit drug trafficker detection.

### Abstract

Driven by the considerable profits, the crime of drug trafficking (a.k.a. illicit drug trading) has co-evolved with modern technologies, e.g., social media such as Instagram has become important platform for marketing and selling illicit drugs. The activities of online drug trafficking are nimble and resilient, which call for novel techniques to effectively detect, disrupt, and dismantle illicit drug trades. In this paper, we propose a holistic framework named MetaHG to automatically detect illicit drug traffickers on social media (i.e., Instagram), by tackling following two new challenges: (1) different from existing works which merely focus on analyzing post content, MetaHG is capable of jointly modeling post content and relational structured information on social media for illicit drug trafficker detection; (2) in addition, through the proposed meta-learning technique, MetaHG addresses the issue of requiring sufficient data for model training. More specifically, in our proposed MetaHG, we first build a heterogeneous graph (HG) to comprehensively characterize the complex ecosystem of drug trafficking on social media.  Then, we employ relation-based graph convolutional neural network to learn node (i.e., user) representations over the built HG, in which we introduce graph structure learning to compensate sparse relational information among entities in the HG for more robust node representation learning. Afterwards, we propose a meta-learning algorithm to optimize model parameters. A self-supervised module and a knowledge distillation module are further designed to exploit unlabeled data for improving model. Extensive experiments based on the real-world data collected from Instagram demonstrate that the proposed MetaHG outperforms state-of-the-art methods.

## Multi-dimensional Shared Representation Learning for Session-based Recommendation

### TL;DR

None

### Abstract

Session-based recommendation system (SBR) is designed to predict the short-term decisions of anonymous users. Although user's next click behavior in the real world is often uncontrollable, there are many potential factors affecting users' decisions. Many previous studies have shown that applying Recurrent Neural Network (RNN) and Graph Neural Network (GNN) to SBR tasks can achieve outstanding performances. However, the existing SBR models only use single feature extraction method to represent an item in the recommendation process. These models either use the GNN methods which ignore the sequence location features in the training process or use RNN methods which cannot obtain the weight features of every item in the sequence. The common problem with the above methods is that none of them can extract the features of different dimensions at the same time, which leads to the low quality item representations. To this end, a new method is proposed in this paper, which is called Multi-dimensional Shared Representation Learning (MSR). (i) To get multi-dimensional features, we use the double-flow feature extraction based on transformer layers and graph attention network (GAT) layers. (ii) Through the MSR module, we first use multiple gate graph neural network (GGNN) to learn the joint representation of items in the session, then fuse the multi-dimensional features by attention mechanism. To verify the efficiency of the proposed method, a variety of experiments have been conducted. The simulation results demonstrate that the proposed method largely surpass the state-of-the-art method in recommendation accuracy.

## Robust Adversarial Learning on Graphs Using Deep Manifold Transformation

### TL;DR

Our paper investigates the relationship between the node embedding of surrogate model and the performance in black-box attacks and proposes RALoG to improve the quality of the generated adversarial attacks by optimizing the surrogate embedding.

### Abstract

In adversarial learning on graphs, gradient-based attack strategies acquire meta-gradients of either node features or graph structures through a trained GNN classifier known as the surrogate model. It is noted that the gradient information provided by the surrogate is an essential factor that affects the performance of the attack model. Adversarial samples generated via common surrogates, such as GCN, tend to work adequately in white-box attacks but have shown less transferability in black-box attacks. However, existing works use common GNNs as surrogate models without discussing the factors influencing the transferability of attacks in black-box setups or whether their surrogate models can provide reliable gradient information. This paper investigates the impact of surrogate node embedding optimization from an adversarial perspective on poisoning attacks to the downstream tasks of node classification. To optimize the meta-gradients provided by the surrogate, we propose robust adversarial learning on graphs (RALoG), an optimized surrogate model embedding, which uses inter-node similarities to preserve the original data information and ease the over-density of nodes mapping in the embedding layer. Experiments show that our method effectively improves the performance of the adversarial attacks generated by the gradient-based attacker in black-box setups. The result indicates that the improvement of surrogate models learning low-dimensional node embedding can provide more robust meta-gradient information to the gradient-based attackers.

## Learning Temporal Point Process with Side Information

### TL;DR

We develop a systematic framework MixNet for event sequences that captures the temporal and structural dependence of  event time, labels and associate attributes, and achieve state-of-the-art results on prediction tasks.

### Abstract

Discrete event sequences are commonly modeled by temporal point processes. Real event datasets such as e-commerce transactions and electronic health records, however involve more complex dependencies among different event types and have additional side information that may provide insight for certain prediction tasks. We propose a systematic framework, called MixNet, which leverages neural density estimation and probabilistic graphical models to capture the complex temporal and structure dependence among event time, correlated labels, and associated attributes for such data. Our models incorporate the learning of such attributes by maximizing either the joint log-likelihood or conditional log-likelihood of the event sequences. For non-prediction tasks, our flexible framework can easily extend to be fully generative, and is thus capable of simulating event sequences with attributes. Furthermore, our approach achieves superior performance compared to previous baselines on four benchmark datasets for prediction tasks. We also demonstrate that the use of additional features reduces the negative log-likelihood per time for event time prediction by roughly 1/3 in addition to the significant improvement in label prediction accuracy in some cases.

## DIGRAC: Digraph Clustering with Flow Imbalance

### TL;DR

We introduce a graph neural network framework to obtain node embeddings for clustering directed networks in a self-supervised manner, including a novel probabilistic imbalance loss.

### Abstract

Node clustering is a powerful tool in the analysis of networks. Here, we introduce a graph neural network framework with a novel scalable Directed Mixed Path Aggregation(DIMPA) scheme to obtain node embeddings for directed networks in a self-supervised manner, including a novel probabilistic imbalance loss. The method is end-to-end in combining embedding generation and clustering without an intermediate step. In contrast to standard approaches in the literature, in this paper, directionality is not treated as a nuisance, but rather contains the main signal. In particular, we leverage the recently introduced cut flow imbalance measure, which is tightly related to directionality; cut flow imbalance is optimized without resorting to spectral methods or cluster labels. Experimental results on synthetic data, in the form of directed stochastic block models and real-world data at different scales, demonstrate that our method attains state-of-the-art results on directed clustering, for a wide range of noise and sparsity levels, as well as graph structures.

## Piper: Multidimensional Planner for DNN Parallelization

### TL;DR

None

### Abstract

The rapid increase in sizes of state-of-the-art DNN models and in the compute and memory requirements of training them has led to the development of many execution schemes such as data parallelism, (pipelined) model parallelism, tensor (intra-layer) model parallelism, and various memory-saving optimizations. However, no prior work has tackled the highly complex problem of finding the optimal partitioning of the DNN computation graph across many accelerators, especially when combining the modes of parallelism and optimizations above.

In this work we introduce Piper, an efficient optimization algorithm for this problem that is based on dynamic programming and a two-level approach. Our two-level approach is driven by the insight that being given tensor-parallelization techniques for individual layers (e.g. Megatron-LM) allows for a significant reduction of the search space, as opposed to considering arbitrary tensor-parallel configurations of the entire DNN operator graph, and makes the global problem tractable.

## Stability of Manifold Neural Networks to Deformations

### TL;DR

We prove the stability to deformations of convolutional neural networks on Riemannian manifolds. 

### Abstract

Stability is an important property of graph neural networks (GNNs) which explains their success in many problems of practical interest. Existing GNN stability results depend on the size of the graph, restricting applicability to graphs of moderate size. To understand the stability properties of GNNs on large graphs, we consider neural networks supported on manifolds. These are defined in terms of manifold diffusions mediated by the Laplace-Beltrami (LB) operator and are interpreted as limits of GNNs running on graphs of growing size. We define manifold deformations and show that they lead to perturbations of the manifold's LB operator that consist of an absolute and a relative perturbation term. We then define filters that split the infinite dimensional spectrum of the LB operator in finite partitions, and prove that manifold neural networks (MNNs) with these filters are stable to both, absolute and relative perturbations of the LB operator. Stability results are illustrated numerically in resource allocation problems in wireless networks.

## Efficient and Local Parallel Random Walks

### TL;DR

Random walk simulation and clustering in the MPC model.

### Abstract

Random walks are a fundamental primitive used in many machine learning algorithms with several applications in clustering and semi-supervised learning. Despite their relevance, the first efficient parallel algorithm to compute random walks has been introduced very recently (Łącki et al.). Unfortunately their method has a fundamental shortcoming: their algorithm is non-local in that it heavily relies on computing random walks out of all nodes in the input graph, even though in many practical applications one is interested in computing random walks only from a small subset of nodes in the graph. In this paper, we present a new algorithm that overcomes  this limitation by building random walks efficiently and locally at the same time. We show that our technique is both memory and round efficient, and in particular yields an efficient parallel local clustering algorithm. Finally, we complement our theoretical analysis with experimental results showing that our algorithm is significantly more scalable than previous approaches.

## Score-based Causal Discovery from Heterogeneous Data

### TL;DR

The first score-based causal discovery algorithm for heterogeneous data, which is able to identify more causal directions.

### Abstract

Causal discovery has witnessed significant progress over the past decades. Most algorithms in causal discovery consider a single domain with a fixed distribution. However, it is commonplace to encounter heterogeneous data (data from different domains with distribution shifts). Applying existing methods on such heterogeneous data may lead to spurious edges or incorrect directions in the learned graph. In this paper, we develop a novel score-based approach for causal discovery from heterogeneous data. Specifically, we propose a Multiple-Domain Score Search (MDSS) algorithm, which is guaranteed to find the correct graph skeleton asymptotically. Furthermore, benefiting from distribution shifts, MDSS enables the detection of more causal directions than previous algorithms designed for single domain data. The proposed MDSS can be readily incorporated into off-the-shelf search strategies, such as the greedy search and the policy-gradient-based search. Theoretical analyses and extensive experiments on both synthetic and real data demonstrate the efficacy of our method. 

## Modeling Hierarchical Logical Reasoning Chains 

### TL;DR

None

### Abstract

Machine reading comprehension poses new challenges over logical reasoning, which aims to understand the implicit logical relationships entailed in the given contexts and perform inference over them. Due to the complexity of logic, logical relationships exist at different granularity levels. However, most existing methods of logical reasoning individually focus on either entity-aware or discourse-based information but ignore the hierarchical relationships that may even have mutual effects. In this paper, we propose a holistic graph network (HGN) which deals with context at both discourse level and word level, as the basis for logical reasoning, to provide a more fine-grained relationship extraction. Specifically, a dual-level attention mechanism, including node-level and type-level attention, is leveraged in our graph attention model to interact with different nodes, which can be interpreted as bridges in the reasoning process. Our proposed model is evaluated on the ReClor benchmark, showing its improvement over baseline as well as its capability to understand more complex logical relationships.

## Roto-translated Local Coordinate Frames For Interacting Dynamical Systems

### TL;DR

None

### Abstract

Modelling interactions is critical in learning complex dynamical systems, namely systems of interacting objects with highly non-linear and time-dependent behaviour. A large class of such systems can be formalized as \emph{geometric graphs}, \emph{i.e.} graphs with nodes positioned in the Euclidean space given an \emph{arbitrarily} chosen global coordinate system, for instance vehicles in a traffic scene. Notwithstanding the arbitrary global coordinate system, the governing dynamics of the respective dynamical systems are invariant to rotations and translations, also known as  \emph{Galilean invariance}. As ignoring these invariances leads to worse generalization, in this work we propose local coordinate systems per node-object to induce roto-translation invariance to the geometric graph of the interacting dynamical system. Further, the local coordinate systems allow for a natural definition of anisotropic filtering in graph neural networks. Experiments in traffic scenes, 3D motion capture, and colliding particles demonstrate the proposed approach comfortably outperforms the recent state-of-the-art.

## Federated Myopic Community Detection with One-shot Communication

### TL;DR

We propose a new problem (federated community detection) and provide formal guarantees.

### Abstract

In this paper, we study the problem of recovering the community structure of a network under federated myopic learning. 
Under this paradigm, we have several clients, each of them having a myopic view, i.e., observing a small subgraph of the network. 
Each client sends a censored evidence graph to a central server. 
We provide an efficient algorithm, which computes a consensus signed weighted graph from clients evidence, and recovers the underlying network structure in the central server. 
We analyze the topological structure conditions of the network, as well as the signal and noise levels of the clients that allow for recovery of the network structure. 
Our analysis shows that exact recovery is possible and can be achieved in polynomial time. 
We also provide information-theoretic limits for the central server to recover the network structure from any single client evidence.
Finally, as a byproduct of our analysis, we provide a novel Cheeger-type inequality for general signed weighted graphs.

## Towards understanding retrosynthesis by energy-based models

### TL;DR

We explore various EBM designs for retrosynthesis, and show some designs outperform the state of the art. 

### Abstract

Retrosynthesis is the process of identifying a set of reactants to synthesize a target molecule. It is of vital importance to material design and drug discovery. Existing machine learning approaches based on language models and graph neural networks have achieved encouraging results. However, the inner connections of these models are rarely discussed, and rigorous evaluations of these models are largely in need. In this paper, we propose a framework that unifies sequence- and graph-based methods as energy-based models (EBMs) with different energy functions. This unified view establishes connections and reveals the differences between models, thereby enhancing our understanding of model design. We also provide a comprehensive assessment of performance to the community. Moreover, we present a novel dual variant within the framework that performs consistent training to induce the agreement between forward- and backward-prediction. This model improves the state-of-the-art of template-free methods with or without reaction types.  

## A Little Truth Injection but a Big Reward: Label Aggregation with Graph Neural Networks

### TL;DR

 This paper proposes a novel graph neural network model to improve label aggregation with a little truth injection.

### Abstract

Various correlations hidden in crowdsourcing annotation tasks bring opportunities to further improve the accuracy of label aggregation. However, these relationships are usually extremely difficult to be modeled. And, most existing methods can merely make use of one or two correlations. In this paper, we propose a novel graph neural network model, namely LAGNN, which models five different correlations in crowdsourced annotation tasks by utilizing deep graph neural networks with convolution operations and derives a high label aggregation performance. Moreover, by injecting a little ground truth in its training stage, the label aggregation performance of LAGNN can be further significantly improved. We evaluate LAGNN on a large number of simulated datasets generated through varying six degrees of freedom and on eight real-world crowdsourcing datasets in both supervised and unsupervised (agnostic) modes. Experimental results consistently show that the proposed LAGNN significantly outperforms six state-of-the-art models in terms of label aggregation accuracy.

## Structure-Aware Random Fourier Kernel for Graphs

### TL;DR

We propose a novel structure-aware random Fourier kernel to improve GP's performance on graph-structured data.

### Abstract

Gaussian Processes (GPs) define distributions over functions and their generalization capabilities depend heavily on the choice of kernels. In this paper, we propose a novel structure-aware random Fourier (SRF) kernel for GPs that brings several benefits when modeling graph-structured data. First, SRF kernel is defined with a spectral distribution based on the Fourier duality given by the Bochner's theorem, transforming the kernel learning problem to a distribution inference problem. Second, SRF kernel admits a random Fourier feature formulation that makes the kernel scalable for optimization. Third, SRF kernel enables to leverage geometric structures by taking subgraphs as inputs. To effectively optimize GPs with SRF kernel, we develop a variational EM algorithm, which alternates between an inference procedure (E-step) and a learning procedure (M-step). Experimental results on five real-world datasets show that our model can achieve state-of-the-art performance in two typical graph learning tasks, i.e., object classification and link prediction. 

## Time-Aware Neighbor Sampling on Temporal Graphs

### TL;DR

None

### Abstract

We present a new neighbor sampling method on temporal graphs. In a temporal graph, predicting different nodes' time-varying properties can require the receptive neighborhood of various temporal scales. In this work, we propose the TNS (Time-aware Neighbor Sampling) method: TNS learns from temporal information to provide an adaptive receptive neighborhood for every node at any time. Learning how to sample neighbors is non-trivial, since the neighbor indices in time order are discrete and not differentiable. To address this challenge, we transform neighbor indices from discrete values to continuous ones by interpolating the neighbors' messages. TNS can be flexibly incorporated into popular temporal graph networks to improve their effectiveness without increasing their time complexity. TNS can be trained in an end-to-end manner. It needs no extra supervision and is automatically and implicitly guided to sample the neighbors that are most beneficial for prediction. Empirical results on multiple standard datasets show that TNS yields significant gains on edge prediction and node classification.

## Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks

### TL;DR

This work reveals the connection of heterophily and oversmoothing problems in the graph convolutional neural networks under a unified theoretical framework.

### Abstract

Most graph convolutional neural networks (GCNs) perform poorly in graphs where neighbors typically have different features/classes (heterophily) and when stacking multiple layers (oversmoothing). These two seemingly unrelated problems have been studied independently, but there is recent empirical evidence that solving one problem may benefit the other. In this work, going beyond empirical observations, we aim to: (1) propose a new perspective to analyze the heterophily and oversmoothing problems under a unified theoretical framework, (2) identify the common causes of the two problems based on the proposed framework, and (3) propose simple yet effective strategies that address the common causes.
Focusing on the node classification task, we use linear separability of node representations as an indicator to reflect the performance of GCNs and we propose to study the linear separability by analyzing the statistical change of the node representations in the graph convolution. We find that the relative degree of a node (compared to its neighbors) and the heterophily level of a node's neighborhood are the root causes that influence the separability of node representations. Our analysis suggests that: (1) Nodes with high heterophily always produce less separable representations after graph convolution; (2) Even with low heterophily, degree disparity between nodes can influence the network dynamics and result in a pseudo-heterophily situation, which helps to explain oversmoothing. Based on our insights, we propose simple modifications to the GCN architecture---i.e., degree corrections and signed messages---which alleviate the root causes of these issues, and also show this empirically on 9 real networks. Compared to other approaches, which tend to work well in one regime but fail in others, our modified GCN model consistently performs well across all settings.

## Efficient Inference of Interventional Distributions

### TL;DR

Learning interventional distributions in Pearl's model.

### Abstract

We consider the problem of efficiently inferring interventional distributions in a causal Bayesian network from a finite number of observations. Let $\mathcal{P}$ be a causal model on a set $\mathbf{V}$ of  observable variables on a given causal graph $G$. For sets $\mathbf{X},\mathbf{Y}\subseteq \mathbf{V}$, and setting $\mathbf{x}$ to $\mathbf{X}$, let $P_{\mathbf{x}}(\mathbf{Y})$ denote the interventional distribution on $\mathbf{Y}$ with respect to an intervention $\mathbf{x}$ to  variables $\mathbf{X}$. Shpitser and Pearl (AAAI 2006), building on the work of Tian and Pearl (AAAI 2001), gave an exact characterization of the class of causal graphs for which the interventional distribution $P_{\mathbf{x}}({\mathbf{Y}})$ can be uniquely determined. 

We give the first efficient version of the Shpitser-Pearl algorithm. In particular, under natural assumptions, we give a polynomial-time algorithm that on input a causal graph $G$  on observable variables $\mathbf{V}$, a setting $\mathbf{x}$ of a set $\mathbf{X} \subseteq \mathbf{V}$, outputs succinct descriptions of both an evaluator and a generator for a distribution $\hat{P}$ that is $\varepsilon$-close (in total variation distance) to $P_{\bf x}({\mathbf{Y}})$ where $Y=\mathbf{V}\setminus \mathbf{X}$, if $P_{\bf x}(\mathbf{Y})$ is identifiable. 

We also show that when $\mathbf{Y}$ is an arbitrary set, there is no efficient algorithm that outputs both an evaluator and a generator of a distribution that is $\varepsilon$-close to $P_{\bf x}({\mathbf{Y}})$ unless all problems that have statistical zero-knowledge proofs, including the Graph Isomorphism problem,  have efficient randomized algorithms. 

## Multi-Armed Bandit Random Walk for Decentralized Learning in Networks with Heterogeneous Data

### TL;DR

None

### Abstract

We consider the problem of learning a global model that fits data distributed on the nodes of a graph. We study decentralized algorithms based on random walks on the graph. In a random walk, the global model is updated using a sample of the visited node's dataset before being passed to a neighboring node chosen at random. We focus on the challenge posed by the statistical heterogeneity of the nodes' datasets, which are sampled from different unknown distributions. This has the drawback of causing a high variance in the gradient estimates, which hurts the convergence rate. We devise a random walk learning algorithm that minimizes the local drift from the global objective in the model updates by visiting more frequently more informative nodes. Designing such an algorithm is confronted at an early stage by the uncertainty about the distribution of each node dataset, of which we only have access to a limited number of samples each visit. At the core of our algorithm is a multi-armed bandit random walk that performs noisy importance estimates in an exploration/exploitation process. The reward is a reduction in the gap between the expected local gradient estimate under uncertainty versus this estimate under the optimal random walk design that requires full information on the distributed data. We prove a sub-linear decaying of the variance-cost and give guarantees on the convergence to the optimal model of the decentralized learning algorithm. Moreover, we present numerical results for different graph settings to illustrate our theoretical findings and show that our algorithm outperforms state-of-art random walk design for speeding up the convergence.

## Transaction Relational Modeling for Fraud Detection by Graph Convolutional Networks

### TL;DR

None

### Abstract

With the popularity of e-commerce and e-payment, detecting real-time transaction fraud has become increasingly important. Users' behavior sequence has been proven to provide rich information for differentiating fraud transactions. The state-of-the-art methods usually map the user's behavior sequence into a vector space where the representation distributions of normal sequence and fraud sequence are distinguishable. However, for future unseen sequences, these methods hardly give the prediction since the decision boundary is learned from the past sequences. To tackle the above problem, we propose a novel framework to associate the unseen sequences with past known sequences whose representation may not close to the unseen sequence in the vector space. However, the relationship still denotes their intrinsic correlation, resulting in similar prediction labels (e.g. the sequence with the same device ID may denote the behaviors by the same user). With this relationship, the unseen sequence could be classified by their associated historical sequences and consequently enhance the prediction accuracy of the unseen sequences. More specifically, we construct a similarity graph and high-risk graph whose edges denote the relationship between transactions to incorporate diverse domain knowledge. Then, a graph neural network is designed to effectively extract useful information from the associated sequences and a framework is proposed to make full use of hierarchical information. To demonstrate the effectiveness of the proposed model, we conduct extensive fraud detection experiments by utilizing data from a world-leading e-commerce platform. Our method outperforms state-of-the-art models by 1.25\% on average and consistently outperforms baseline methods across all the datasets, reaching up to 0.9815 in terms of AUC.

## Planning Spatial Networks

### TL;DR

We devise a method for goal-directed spatial graph construction by formulating this task as an MDP and propose a variant of Monte Carlo Tree Search which outperforms previous approaches in terms of optimality and scalability.

### Abstract

We tackle the problem of goal-directed graph construction: given a starting graph, a global objective function (e.g., communication efficiency), and a budget of modifications, the aim is to find a set of edges whose addition to the graph maximally improves the objective. This problem emerges in many networks of great importance for society such as transportation and critical infrastructure networks. We identify two significant shortcomings with present methods. Firstly, they focus exclusively on network topology while ignoring spatial information; however, in many real-world networks, nodes are embedded in space, which yields different global objectives and governs the range and density of realizable connections. Secondly, existing RL methods scale poorly to large networks due to the high cost of training a model and the scaling factors of the action space and global objectives.

In this work, we formulate the problem of goal-directed construction of spatial networks as a deterministic MDP. We adopt the Monte Carlo Tree Search framework for planning in this domain, prioritizing the optimality of final solutions over the speed of policy evaluation. We propose several improvements over the standard UCT algorithm for this family of problems, addressing their single-agent nature, the trade-off between the costs of edges and their contribution to the objective, and an action space linear in the number of nodes. We demonstrate the suitability of this approach for improving the global efficiency and attack resilience of a variety of synthetic and real-world networks, including Internet backbone networks and metro systems. We obtain 24% better solutions on average compared to UCT on the largest networks tested, and scalability superior to previous methods.

## Successor Feature Landmarks for Long-Horizon Goal-Conditioned Reinforcement Learning

### TL;DR

Graph-based framework exploiting successor features to achieve long-horizon goal-conditioned RL

### Abstract

Operating in the real-world often requires agents to learn about a complex environment and apply this understanding to achieve a breadth of goals. This problem, known as goal-conditioned reinforcement learning (GCRL), becomes especially challenging for long-horizon goals. Current methods have tackled this problem by augmenting goal-conditioned policies with graph-based planning algorithms. However, they struggle to scale to large, high-dimensional state spaces and assume access to exploration mechanisms for efficiently collecting training data. In this work, we introduce Successor Feature Landmarks (SFL), a framework for exploring large, high-dimensional environments so as to obtain a policy that is proficient for any goal. SFL leverages the ability of successor features (SF) to capture transition dynamics, using it to drive exploration by estimating state-novelty and to enable high-level planning by abstracting the state-space as a non-parametric landmark-based graph. We further exploit SF to directly compute a goal-conditioned policy for inter-landmark traversal, which we use to execute plans to "frontier" landmarks at the edge of the explored state space. We show in our experiments on MiniGrid and ViZDoom that SFL enables efficient exploration of large, high-dimensional state spaces and outperforms state-of-the-art baselines on long-horizon GCRL tasks. A video demo of SFL can be found at https://bit.ly/3hJ2t1i.

## DAG-GPs: Learning Directed Acyclic Graph Structure For Multi-Output Gaussian Processes

### TL;DR

Multi-output Gaussian processes with a directed acyclic graph structure imposed between outputs.

### Abstract

Multi-output Gaussian processes (MOGPs) encode correlations between output dimensions. Inter-output correlations allow observed patterns in data to be extrapolated to unobserved outputs. However, existing MOGP methods either require a hand-coded structure, or may learn correlations between outputs that are actually unrelated, leading to significantly reduced predictive accuracy. We present two technical innovations to avoid these issues. First, we introduce the DAG-GP model, which restricts correlations between outputs by composing latent GPs using a directed acyclic graph (DAG) structure. Our method learns the DAG structure using the data, eliminating the need to encode a known structure. The algorithm prevents spurious correlations by only introducing inter-output correlations when improvement in likelihood justifies the increase in structure complexity. Second, we address the  run time complexity of structural learning for GPs and the myopia of greedy search methods using A* with bounding conflicts (A*BC). A*BC combines optimal A* search with conflict-directed search, enabling the optimal DAG to be found without evaluating the likelihoods of all graphs. Our evaluation of the DAG-GP model on MOGP benchmarks demonstrates state of the art performance for prediction and shows that the solved structures have physically meaningful interpretations. Timing experiments show that use of A*BC reduces training time for the DAG-GP model by approximately 50\%, with no loss of structure optimality. 

## Deep Transferable Metric Learning for Clustering

### TL;DR

A method to learn a metric allowing o score the quality of a clustering, usable as a reward for a reinforcement learning clustering agent

### Abstract

Due to the curse of dimensionality, clustering in high dimension spaces remains a hard task mainly because distance-based algorithms like k-means are no longer tractable or effective. Moreover, the choice of the metric is crucial as it is highly dependent on the dataset characteristics; Euclidean and other standard distance metrics may not be appropriate. We propose a framework for
learning a transferable metric. Using a graph auto-encoder, we show that it is possible to build dataset independent features characterising the geometric properties of a given clustering. These features are used to train a critic that serves as a metric which measures the quality of a clustering. We learn and test the metric on several datasets of variable complexity (synthetic, MNIST, SVHN, omniglot) and achieve close to state of the art results while using only a fraction of these datasets and shallow networks. We show that the learned metric is transferable from a dataset to another even when changing domain or task. 


## Nonlocal Kernel Network (NKN): a stable and resolution independent deep neural network

### TL;DR

None

### Abstract

Deep neural networks, often required in complex learning tasks such as image classification, are hard to train. We propose a novel formulation, inspired by the graph kernel network (GKN), that allows for deep layers. Our nonlocal kernel network (NKN) stems from the interpretation of the neural network as a discrete nonlocal diffusion reaction problem that, in the limit of infinite layers, is equivalent to a parabolic nonlocal equation, whose stability is analyzed via nonlocal vector calculus. The resemblance with graph neural networks allows NKNs to capture long-range dependencies in the feature space, while the continuous treatment of node-to-node interactions makes NKNs resolution independent. Furthermore, the resemblance with neural ODEs, reinterpreted in a nonlocal sense, and the stable network dynamics between layers allows for generalization of NKN's optimal parameters from shallow to deep networks. This fact enables the use of shallow-to-deep initialization techniques.  Our tests show that NKNs outperform baseline methods in both PDE learning tasks (Poisson and multi-dimensional Darcy's equations) and image classification tasks and generalize well to different resolutions and depths. 

## MIP-GNN: A data-driven framework for guiding combinatorial solvers

### TL;DR

 We propose a general data-driven framework, based on graph neural networks, replacing heuristic components of state-of-the-art mixed-integer linear solvers.

### Abstract

Mixed-Integer Linear Programs (MIPs) offer a generic way of formulating and solving combinatorial optimization problems. While generally reliable, state-of-the-art MIP solvers base many essential decisions on hand-crafted heuristics, largely ignoring common patterns within a given instance distribution of the problem of interest. Here, we propose MIP-GNN, a general data-driven tool for enhancing such solvers with data-driven insights. By encoding the variable-constraint interactions of a given MIP as a bipartite graph, we leverage state-of-the-art graph neural network architectures to predict variable biases, i.e., component-wise averages of (near) optimal solutions, indicating how likely a variable will be set to 0 or 1 in (near) optimal solutions of binary MIPs. In turn, these biases are used to guide the solver, replacing heuristic components. To demonstrate the generality of our framework, we integrate MIP-GNN into a state-of-the-art MIP solver, applying it to tasks such as node selection, branching variable selection, and warm-starting, showing significant improvements compared to the default setting of the solver on a class of challenging MIPs.  

## Property-aware Adaptive Relation Networks for Molecular Property Prediction

### TL;DR

Considering molecule property prediction problem where few labeled data is available, we propose a property-aware adaptive relation networks which models molecule structure and pairwise molecule relationship with respect to the target property. 

### Abstract

Molecular property prediction plays a fundamental role in drug discovery to discover candidate molecules with target properties. However, molecular property prediction is essentially a few-shot problem which makes it hard to obtain regular models. In this paper, we propose property-aware adaptive relation networks for the few-shot molecular property prediction problem. We first obtain molecular embeddings via a graph-based molecular encoder learned across different tasks where each of them corresponds to the prediction of one property. To capture the specific structure property/activity relationship, we particularly design property-aware embedding projection to co-adapt the embedding of each molecule w.r.t. other molecules in the current task and further select relevant substructures. The resultant embeddings are used to construct a relation graph modeling the similarity between molecules which facilitates effective information propagation and is dynamically optimized during learning. Extensive experiments on many benchmark drug property prediction datasets show that our method outperforms state-of-the-art methods under both standard few-shot learning settings and transfer learning across different datasets setting. 

## Generalized Few-Shot Node Classification on Graphs

### TL;DR

None

### Abstract

For real-world graph structured data, the node class distribution is inherently imbalanced and long-tailed, which naturally leads to a few-shot learning scenario with limited nodes labelled in newly emerged classes. Existing efforts are carefully designed to solve this few-shot learning problem via data augmentation, learning transferable initialization, to name a few. However, most, if not all, of them are based on a strong assumption that all the test nodes must exclusively come from novel classes, which is impractical in real-world applications. In this paper, we study a broader and more realistic problem named 'generalized few-shot node classification', where the test samples can be from either novel classes or base classes. Compared with the standard few-shot node classification, this new problem imposes several unique challenges, including 'asymmetric classification' and 'inconsistent preference'. In response, we propose a shot-aware graph neural network (STAGER) equipped with a new training paradigm named imbalanced episodic training. Experiment results on four real-world datasets demonstrate the efficacy of our model, with up to 14% accuracy improvement over baselines. 

## GRIN: Generative Relation and Intention Network for Multi-agent Trajectory Prediction

### TL;DR

We build a model that disentangles social relations and human intents to generate multimodal future trajectories of multi-agent systems.

### Abstract

Learning the distribution of future trajectories conditioned on the past is a crucial problem for understanding multi-agent systems. This is challenging because humans make decisions based on complex social relations and personal intents, resulting in highly complex uncertainties over trajectories. To address this problem, we propose a conditional deep generative model that combines advances in graph neural networks. The prior and recognition model encodes two types of latent codes for each agent: an inter-agent latent code to represent social relations and an intra-agent latent code to represent agent intentions. The decoder is carefully devised to leverage the codes in a disentangled way to predict multi-modal future trajectory distribution. Specifically, a graph attention network built upon inter-agent latent code is used to learn continuous pair-wise relations, and an agent's motion is controlled by its latent intents and its observations of all other agents. Through experiments on both synthetic and real-world datasets, we show that our model outperforms previous work in multiple performance metrics. We also show that our model generates realistic multi-modal trajectories.

## Learning to Extend Molecular Scaffolds with Structural Motifs

### TL;DR

We propose a new fragment-based generative model of molecules that can be constrained to include an arbitrary subgraph.

### Abstract

Recent advancements in deep learning-based modeling of molecules promise to accelerate in silico drug discovery. A plethora of generative models is available, building molecules either atom-by-atom and bond-by-bond or fragment-by-fragment. However, many drug discovery projects require a fixed scaffold to be present in the generated molecule, and incorporating that constraint has only recently been explored. In this work, we propose a new graph-based model that naturally supports scaffolds as initial seed of the generative procedure, which is possible because our model is not conditioned on the generation history. At the same time, our generation procedure can flexibly choose between adding individual atoms and entire fragments. We show that training using a randomized generation order is necessary for good performance when extending scaffolds, and that the results are further improved by increasing the fragment vocabulary size. Our model pushes the state-of-the-art of graph-based molecule generation, while being an order of magnitude faster to train and sample from than existing approaches.

## Task-Oriented Hypergraph Neural Networks for Semi-supervised Classification

### TL;DR

None

### Abstract

Most current hypergraph learning schemes transform a hypergraph into a graph for graph convolution, which causes information loss. In addition, existing hypergraph learning schemes mostly focus on tasks for nodes and overlook tasks for hyperedges although hyperedges can represent independent objects. Motivated by this property of hyperedges, we define two new semi-supervised classification tasks on hypergraphs: (1) node classification from hyperedge features and (2) hyperedge classification from node features. To tackle various hypergraph-based semi-supervised classification tasks, we suggest task-oriented hypergraph neural network (ToHNN) of which the architecture changes depending on the type of task. We further propose edge-to-edge ($E2E$) aggregation, a novel aggregation scheme that fully exploits hyperedge features, which is an essential component of ToHNN. The effectiveness of ToHNN is demonstrated through extensive experiments on various benchmark datasets. ToHNN shows validity on both of the new tasks and significantly outperforms current state-of-the-art methods on two existing tasks: (3) node classification from node features and (4) hyperedge classification from hyperedge features.

## Revisiting Latent-Space Interpolation via a Quantitative Evaluation Framework

### TL;DR

We design a quantitative evaluation framework for latent-space interpolation algorithms, and discuss how we adapt the model or training for better interpolation performance.

### Abstract

Latent-space interpolation is commonly used to demonstrate the generalization ability of deep latent variable models. Various algorithms have been proposed to calculate the best trajectory between two encodings in the latent space. In this work, we show how data labeled with semantically continuous attributes can be utilized to conduct a quantitative evaluation of latent-space interpolation algorithms, for variational autoencoders. Our framework can be used to complement the standard qualitative comparison, and also enables evaluation for domains (such as graph) in which the visualization is difficult. Interestingly, our experiments reveal that the superiority of interpolation algorithms could be domain-dependent. While normalised interpolation works best in the image domain, spherical linear interpolation achieves the best performance in the graph domain. Next, we propose a simple-yet-effective method to restrict the latent space via a bottleneck structure in the encoder. We report that all interpolation algorithms evaluated in this work can benefit from this restriction. Finally, we conduct interpolation-aware training with the labeled attributes, and show that this explicit supervision can boost the interpolation performance.

## Scalable Variational Approaches for Bayesian Causal Discovery

### TL;DR

We design a variational method to learn distributions over linear structural equation parameters from data, unlike previous point estimate methods. 

### Abstract

A structural equation model (SEM) is an effective framework to reason over causal relationships represented via a directed acyclic graph (DAG).
Recent advances have enabled effective maximum-likelihood point estimation of DAGs from observational data. 
However, a point estimate may not accurately capture the uncertainty in inferring the underlying graph in practical scenarios, wherein the true DAG is non-identifiable and/or the observed dataset is limited.
We propose LiGA-VI, a variational inference (VI) framework for estimating a distribution over DAGs characterizing a linear-Gaussian (LiGa) SEM.
Developing a full Bayesian posterior over DAGs is challenging due to the the discrete and combinatorial nature of graphs.
We analyse key design choices for scalable VI over DAGs, such as 1) the parametrization of DAGs via an expressive variational family, 2) a continuous relaxation that enables low-variance stochastic optimization, and 3) suitable priors over the latent variables.
We provide a series of experiments on real and synthetic data showing that our distributional approach outperforms maximum-likelihood methods on standard causal discovery metrics such as structural Hamming distance. 

## Train on Small, Play the Large: Scaling Up Board Games with AlphaZero and GNN

### TL;DR

We propose a GNN-based AlphaZero model for playing board games, which is trained on small-scaled boards and is capable of advancing to successfully to play large-scaled ones.

### Abstract

Playing board games is considered a major challenge for both humans and AI researchers. Because some complicated board games are quite hard to learn, humans usually begin with playing on smaller boards and incrementally advance to master larger board strategies. Most neural network frameworks that are currently tasked with playing board games neither perform such incremental learning nor possess capabilities to automatically scale up. In this work, we look at the board as a graph and combine a graph neural network architecture inside the AlphaZero framework, along with some other innovative improvements. Our ScalableAlphaZero is capable of learning to play incrementally on small boards, and advancing to play on large ones. Our model can be trained quickly to play different challenging board games on multiple board sizes, without using any domain knowledge. We demonstrate the effectiveness of ScalableAlphaZero and show, for example, that by training it for only three days on small Othello boards, it can defeat the AlphaZero model on a large board, which was trained to play the large board for $30$ days.

## Relational Generative Model for Scalable Multi-modal Learning

### TL;DR

None

### Abstract

The study of complex  systems requires the integration of heterogeneous, high-dimensional data (e.g. multi-omics). Previous generative approaches for multi-modal inputs either do not learn a joint representation in a supervised framework, which makes it difficult to capture complementary and task-relevant information, or their predictive functions are deterministic.  In this paper, we propose a novel Bayesian relational generative model that learns a graph of dependencies between samples across multi-modal data types through adopting priors over the relational structure of the given data modalities. The dependency graph in our method, multi-modal  Relational Neural Process (mRNP), not only posits distributions over the functions and naturally enables rapid adaptation to new observations by its predictive distribution, but also serves as a more interpretable and semantically meaningful shared representation of different modalities, driving better insights into their underlying mechanisms. Experiments on both toy regression and classification of real-world datasets demonstrate the potential of mRNP for better predictions as well as more robust uncertainty estimates compared to existing baselines and state-of-the-art methods while inferring a shared structured representation.

## Spectral embedding for dynamic networks with stability guarantees

### TL;DR

None

### Abstract

We consider the problem of embedding a dynamic network, to obtain time-evolving vector representations of each node, which can then be used to describe the changes in behaviour of a single node, one or more communities, or the entire graph. Given this open-ended remit, we wish to guarantee stability in the spatio-temporal positioning of the nodes: assigning the same position, up to noise, to nodes behaving similarly at a given time (cross-sectional stability) and a constant position, up to noise, to a single node behaving similarly across different times (longitudinal stability). These properties are defined formally within a generic dynamic latent position model. By showing how this model can be recast as a multilayer random dot product graph, we demonstrate that unfolded adjacency spectral embedding satisfies both stability conditions, allowing, for example, spatio-temporal clustering under the dynamic stochastic block model. We also show how alternative methods, such as omnibus, independent or time-averaged spectral embedding, lack one or the other form of stability.

## Connect to the Past: Graph-based Latent Estimation for Blind Super-Resolution

### TL;DR

None

### Abstract

Single image super-resolution (SISR) aims at recovering high-resolution (HR) images from degraded low-resolution (LR) images.  In this paper, we focus on blind super-resolution (blind SR), which is a complicated and challenging problem due to the unknown degradation process.  The previous methods assume a unified degradation model and fail to generalize to multiple real-world scenarios. Besides, the existing methods often neglect the direct relationship between a test LR image and LR-HR pairs in the training set. In view of these above issues, we reformulate the degradation with a general assumption and estimate a degradation latent vector instead of a blur kernel. We introduce a Graph-bAsed Latent Estimation (GALE) network to predict the latent vector based on selected LR-HR image pairs. This brings valuable information for the test LR image from graph aggregation, which is referred to a connection to the past. Extensive experiments on synthetic and real-world images show that the proposed GALE network can provide visually favorable SR results and the state-of-the-art performance in blind SR problem.

## Adaptive Data Augmentation on Temporal Graphs

### TL;DR

None

### Abstract

Temporal Graph Networks (TGNs) are powerful on modeling temporal graph data based on their increased complexity. Higher complexity carries with it a higher risk of overfitting, which makes TGNs capture random noise instead of essential semantic information. To address this issue, our idea is to transform the temporal graphs using data augmentation (DA) with adaptive magnitudes, so as to effectively augment the input features and preserve the essential semantic information. Based on this idea, we present the \textbf{MeTA} (\textit{Memory Tower Augmentation}) module: a multi-level module that processes the augmented graphs of different magnitudes on separate levels, and performs message passing across levels to provide adaptively augmented inputs for every prediction. MeTA can be flexibly applied to the training of popular TGNs to improve their effectiveness without increasing their time complexity. To complement MeTA, we propose three DA strategies to realistically model noise by modifying both the temporal and topological features. Empirical results on standard datasets show that MeTA yields significant gains for the popular TGN models on edge prediction and node classification in an efficient manner.

## Beyond Error: Knowledge Testing of Spatiotemporal Model Performance  In  Crowd Flow Prediction

### TL;DR

None

### Abstract

One of the major challenges in crowd flow forecast nowadays is to provide predictions with not only high accuracy but also interpretability, which is critical to scientific understanding, AI safety, and debugging. Even though most researches evaluating this ability by measuring forecast error, these are not enough to demonstrate the spatio-temporal knowledge and solid cognition have been charged. In this paper, we define four based metrics from four aspects: spatiality, temporality, cor- relation and stability to comprehensively evaluate the three class models structure: convolution, sequence and graph. Further, we proposed the novel attributed technology named attributed road graph, to serve as a refined, simplified, interpretable  algorithm on coarse explanation generated by attributed model. We conduct experiments on two wide range of real-world taxi dataset with different levels of difficulty  from Chengdu and Xi’an to evaluate the baseline models performance. Rigorous  and integrated recommendations based on above metrics have been introduced for  which ML architecture to use and, provide the suggestion about the direction for  designing next generation model.

## Deep Reinforcement Learning with Counterfactual Reasoning for Dynamic Vehicle Dispatching

### TL;DR

None

### Abstract

Improving the drivers' income and alleviating supply-demand mismatch are two important tasks in the ride-sharing platform. To address these challenging tasks, we propose a Causal Counterfactual Advantage Actor-Critic (CCA2C) algorithm to dispatch vehicles efficiently, which consists of three modules: A2C module, Action Influence Graph (AIG) module and Causal Counterfactual Reasoning (CCR) module. Firstly, the A2C module adapts to the complex dynamic traffic environment of supply and demand based on the mutual supervision and control of the actor and critic network. And then, the AIG module provides an interpretability and intervention causal graph for agents to explore counterfactual policy space. Finally, the CCR module measures the return and regret between actual policy and other unselected policies by counterfactual reasoning to further update dispatching policy. We also design a simulator with two large-scale datasets to evaluate the algorithm extensively. The experimental results of practical datasets show that our proposed algorithm outperforms state-of-the-art methods in terms of order response rate by 2.08% and 3.33%, and revenue efficiency of inferior drivers by 7.61% and 14.81%, respectively.

## Multi-human 3D Mesh Reconstruction with Multi-layer Relational Graph Neural Network

### TL;DR

We propose a multi-layer RGCN network to reconstruct multi-human mesh from one single image.. 

### Abstract

Many computer vision and robotics applications, including activity recognition and bodily expressed emotion understanding, rely on the ability to generate multi-person 3D human mesh reconstructions from a single monocular image. The main challenge for this task is to estimate the 3D relationships between human bodies so as to avoid issues such as interpenetration and inconsistent depth ordering to obtain a coherent representation of the scene. Current methods utilize neural network frameworks to directly estimate parameters for each human mesh such as Skinned Multi-Person Linear Model (SMPL) parameters, while adding additional loss constraints for depth ordering or interpenetration. Because SMPL parameters just represent the rotation of joints relative to one single human body, this makes it difficult to accurately estimate connections between subjects. To overcome this, instead of using human mesh parameters, we propose leveraging 3D world coordinates of human mesh vertices in conjunction with a novel multi-layer heterogeneous relation graph network for multi-person human mesh estimation. To achieve this, we first use FeatureNet to predict 2D and 3D human joint coordinates. Using these coordinates, we then construct a graph to represent the holistic multi-person environment. Here, human skeleton joints and human mesh vertices are represented as two different kinds of nodes while each person is represented by their own layer. Vertex and joint nodes within one person are then connected to draw the human skeleton and mesh structure, while joint nodes from persons close in proximity are also connected to capture across-human interactions. From here, graph convolutional layers are then used to estimate the human mesh vertices. Extensive experiments on standard 3D human benchmarks Panoptic, MuPoTS-3D, and Human3.6M illustrate that our method surpasses previous multi-person human mesh SOTA method by 5.3%, 6.2% , 5.1% separately.

## CE-BART: Cause-and-Effect BART for Visual Commonsense Generation

### TL;DR

We present a network referred to as Cause-and-Effect BART (CE-BART) for visual commonsense generation, which entails generating cause-and-effect inferences grounded in a given image and event description.

### Abstract

"A Picture is worth a thousand words". Given an image, humans are able to deduce various cause-and-effect descriptions of past, current, and future events beyond the image. The task of visual commonsense generation aims at generating three cause-and-effect descriptions (1) what needed to happen before, (2) what is the intent and (3) what will happen after, for a given image and corresponding textual event and place. However, such a task is challenging for machines owing to two limitations: (1) existing approaches directly utilize conventional vision-language transformers to learn relationships between input modalities, (2) existing approaches does not capture the relationships between training samples, but considers each sample independently. To this end, we propose Cause-and-Effect BART (CE-BART) which is based on (1) Structured Graph Reasoner that captures intra- and inter-modality relationships among visual and textual representations, and (2) Cause-and-Effect Generator that generates cause-and-effect descriptions by considering the causal relations among inferences. Structured graph reasoner builds semantic graphs for individual modalities and strengthens their representations via capturing intra- and inter-modality relations among graph structures.  Cause-and-effect generator is an extension of BART transformer architecture with three decoders, each for generating before, intent, and after descriptions. Through causal connections between three decoders, it can attend to hidden states of the former decoder which take the role of cause to generate effect description. We demonstrate the validity of CE-BART on VisualCOMET dataset, a challenging visual commonsense generation benchmark. Our empirical results show that CE-BART achieves state-of-the-art performance in VisualCOMET, while extensive ablation study and qualitative analysis demonstrate the performance gain and improved interpretability. The code will be made publicly available.


## Neuroevolution-Enhanced Multi-Objective Optimization for Mixed-Precision Quantization

### TL;DR

A flexible and scalable multi-objective framework for mixed-precision quantization leveraging graph neural networks and NEMO, a novel search method leveraging both classical search and neuroevolution techniques.

### Abstract

Mixed-precision quantization is a powerful tool to enable memory and compute savings of neural network workloads by deploying different sets of bit-width precisions on separate compute operations. Recent research has shown significant progress in applying mixed-precision quantization techniques to reduce the memory footprint of various workloads, while also preserving task performance. Prior work, however, has often ignored additional objectives, such as bit-operations, that are important for deployment of workloads on hardware. Here we present a flexible and scalable framework for automated mixed-precision quantization that optimizes multiple objectives. Our framework relies on Neuroevolution-Enhanced Multi-Objective Optimization (NEMO), a novel search method, to find Pareto optimal mixed-precision configurations for memory and bit-operations objectives. Within NEMO, a population is divided into structurally distinct sub-populations (species) which jointly form the Pareto frontier of solutions for the multi-objective problem. At each generation, species are re-sized in proportion to the goodness of their contribution to the Pareto frontier. This allows NEMO to leverage established search techniques and neuroevolution methods to continually improve the goodness of the Pareto frontier. In our experiments we apply a graph-based representation to describe the underlying workload, enabling us to deploy graph neural networks trained by NEMO to find Pareto optimal configurations for various workloads trained on ImageNet. Compared to the state-of-the-art, we achieve competitive results on memory compression and superior results for compute compression for MobileNet-V2, ResNet50 and ResNeXt-101-32x8d, one of the largest ImageNet models amounting to a search space of $\sim10^{146}$. A deeper analysis of the results obtained by NEMO also shows that both the graph representation and the species-based approach are critical in finding effective configurations for all workloads.

## Inferring community characteristics in labelled networks

### TL;DR

None

### Abstract

Labelled networks form a very common and important class of data,
naturally appearing in numerous applications in science and engineering.
A typical inference goal is to determine how the vertex labels
(or {\em features}) affect the network's graph structure. A standard
approach has been to partition the network into blocks grouped
by distinct values of the feature of interest. A block-based random
graph model -- typically a variant of the stochastic block model --
is then used to test for evidence of asymmetric behaviour within these
feature-based communities. Nevertheless, the resulting communities
often do not produce a natural partition of the graph. In this work,
we introduce a new generative model, the feature-first block model (FFBM),
which is more effective at describing vertex-labelled undirected
graphs and also facilitates the use of richer queries on labelled networks.
We develop a Bayesian framework for inference with this model,
and we present a method to efficiently sample from the posterior
distribution of the FFBM parameters. The FFBM's structure is kept
deliberately simple to retain easy interpretability of the parameter
values. We apply the proposed methods to a variety of network data
to extract the most important features along which the vertices
are partitioned. The main advantages of the proposed approach are
that the whole feature-space is used automatically, and features
can be rank-ordered implicitly according to impact. Any features
that do not significantly impact the high-level structure can be
discarded to reduce the problem dimension. In cases where the vertex
features available do not readily explain the community structure
in the resulting network, the approach detects this and is protected
against over-fitting. Results on several real-world datasets
illustrate the performance of the proposed methods.

## Balanced Belief Propagation

### TL;DR

None

### Abstract

For the human brain, everyday life requires performing complex probabilistic inference tasks, many of which are mathematically intractable. A crucial question is how the brain can carry out such approximate inference in an efficient manner. A possibility is that the structure of neural networks mirrors probabilistic graphs and that the network activity corresponds to the Belief Propagation algorithm (BP) in the underlying graph. However, the poor performance of BP in cyclic graphs casts doubt on this proposition, given the recurrent nature of the brain. Here, we introduce Balanced Belief Propagation, which extends previously proposed algorithms like Fractional BP, and is linked to a generalization of BP's Bethe Free Energy on factor graphs. This method allows the network to balance its inputs by compensating for the loops of information arising from recurrence. We show that the proposed approach not only improves the quality of probabilistic inferences but also brings stability to the network, which is a known feature of balanced excitatory-inhibitory (E-I) networks. Finally, we relate E-I balance to performance in probabilistic tasks, with applications to E-I imbalance and brain disorders such as psychosis.

## RED: Overcoming Over-smoothing in Graph Neural Networks via Rough Edge Dropping

### TL;DR

A pre-processing method for the graph data to mitigate over-smoothing in GNNs

### Abstract

Graph Neural Networks (GNNs), especially the GNNs based on message passing framework (MPGNNs), have been successfully applied to solve complex problems in many fields. Using multiple layers in other deep learning models can lead to higher accuracy. However, such a technique leads to over-smoothing issues in MPGNNs because features of adjacent nodes tend to be similar. In this paper, we build a novel framework to study the characteristics of the over-smoothness phenomenon in MPGNNs. In the framework, we first put forward a metric named MPV (Message Passing Velocity) to measure the speed of being smoothing. We first prove that edges with larger MPV contribute more to the smoothness of the graph and thus make MPGNNs easier to be over-smoothing. We name those edges Rough Edges and propose the RED (Rough Edge Dropping) approach to relieve the over-smoothing issues in MPGNNs. Comprehensive experimental results show that RED can not only yield up to 3.50% improvement in two-layer GNNs on testing accuracy for node classification, but also yield up to 12.11% improvement in multi-layer GNNs. In terms of the time cost in training, RED helps speedup the training process by 2.60×.

## On the Effectiveness of Local Search for Graph Adversarial Robustness

### TL;DR

We propose an effective local search scheme to generate adversarial perturbations for graph convolutional neural networks.

### Abstract

Graph neural networks have been shown to be vulnerable to adversarial perturbations. Due to the discrete nature of graphs, constructing adversarial perturbations on graphs is essentially a combinatorial optimization problem. In the past, various methods based on gradient information have been proposed to approximately solve this combinatorial problem, e.g., gradient heuristics and relaxation. However, it is unclear whether these methods provide an adequate solution. In this work, we develop a simple yet efficient discrete local search scheme, which significantly improves the attack strength. Despite its simplicity, our discrete local search algorithm outperforms a wide range of existing baselines in both evasion and poisoning settings. The fact that many existing techniques in the literature are outperformed by our simple method suggests that the previously adopted approximations are an inappropriate match to the discrete structure.

## Ladder-GNN: Hop-Aware Representation Learning for Graph Neural Networks

### TL;DR

None

### Abstract

In Graph Neural Networks (GNNs), the embedding of each node is obtained by aggregating information with its direct and indirect neighbors. As the messages passed among nodes contain both information and noise, the critical issue in GNN representation learning is how to retrieve information effectively while suppressing noise. Generally speaking, interactions with distant nodes usually introduce more noise for a particular node than those with close nodes. However, in most existing works, the messages being passed among nodes are mingled together, which is inefficient from a communication perspective. Mixing the information from clean sources (low-order neighbors) and noisy sources (high-order neighbors) makes discriminative feature extraction challenging.
Motivated by the above, we propose a simple yet effective ladder-style GNN architecture, namely Ladder-GNN. Specifically, we separate messages from different hops and assign different dimensions for them before concatenating them to obtain the node representation. Such disentangled representations facilitate extracting information from messages passed from different hops, and their corresponding dimensions are determined with a reinforcement learning-based neural architecture search strategy. The resulted hop-aware representations generally contain more dimensions for low-order neighbors and fewer dimensions for high-order neighbors, leading to a ladder-style aggregation scheme. We verify the proposed Ladder-GNN on several semi-supervised node classification tasks. Experimental results show that the proposed simple hop-aware representation learning solution can achieve state-of-the-art performance on most datasets. 

## On Provable Benefits of Depth in Training Graph Convolutional Networks

### TL;DR

None

### Abstract

Graph Convolutional Networks (GCNs) are known to suffer from performance degradation as the number of layers increases, which is usually attributed to over-smoothing. Despite the apparent consensus, we observe that there exists a discrepancy between the theoretical understanding of over-smoothing and the practical capabilities of GCNs. 
  Specifically,  we argue that over-smoothing not necessarily happen in practice, a deeper model is provable expressive, can converge to global optimal with linear convergence rate, and achieve very high training accuracy as long as properly trained. 
 Despite being capable of achieving high training accuracy, empirical results shows that the deeper models generalize poorly on the testing set and existing theoretical understanding of such behavior remains elusive. To achieve better understanding, we carefully analyzing the \emph{generalization capability} of GCNs, we show that the training strategies to achieve high training accuracy 
 significantly deteriorate the \emph{generalization capability} of GCNs. Motivated by these findings, we propose a decoupled structure for GCNs that detaches weight matrices from feature propagation to preserve the expressive power and ensure good generalization performance. We conduct empirical evaluations on various synthetic and real-world datasets to validate the correctness of our theory.

## MedGTX: Graph-Text Multi-Modal Encoder for Medical Representation Learning

### TL;DR

None

### Abstract

As the volume of Electronic Health Records (EHR) sharply grows, there has been emerging interest in learning the representation of EHR for healthcare applications. Representation learning of EHR requires appropriate modeling of the two dominant modalities in EHR: structured data and unstructured text. In this paper, we present MedGTX, a pre-trained model for multi-modal representation learning of the structured and textual EHR data. MedGTX uses a novel graph encoder to exploit the graphical nature of structured EHR data, and a text encoder to handle unstructured text, and a cross-modal encoder to learn a joint representation space. We pre-train the model with four pre-training tasks, where masked language modeling, masked literal prediction, and relation classification are designed to learn the representation of each modality, and cross-modal alignment prediction guides the model to align the representation space of the two modalities. We propose four novel downstream tasks to tackle a real-world problem in EHR data, and extensive evaluation results consistently show the effectiveness of pre-training for all downstream tasks. Given the promising performance of MedGTX, we believe this work opens a new door to jointly understanding the two fundamental modalities of EHR data.

## Rot-Pro: Modeling Transitivity by Projection in Knowledge Graph Embedding

### TL;DR

None

### Abstract

Knowledge graph embedding models learn the representations of entities and relations in the knowledge graphs for predicting missing links (relations) between entities. Their effectiveness are deeply affected by the ability of modeling and inferring different relation patterns such as symmetry, asymmetry, inversion, composition and transitivity. Although existing models are already able to model many of these relations patterns, transitivity, a very common relation pattern, is still not been fully supported. In this paper, we first theoretically show that the transitive relations can be modeled with projections. We then propose the Rot-Pro model which combines the projection and relational rotation together. We theoretically prove that Rot-Pro can infer all the above relation patterns. Experimental results show that the proposed Rot-Pro model  effectively learns the transitivity pattern and achieves the state-of-the-art results on the link prediction task in the datasets containing transitive relations.

## MorphoGNN: Morphological Embedding for Single Neuron with Graph Neural Networks

### TL;DR

This paper presents a graph neural network based morphological embedding for neuron classification, indexing, and representation.

### Abstract

With the development of optical microscoic imaging systems, neuroscientists can now obtain large datasets of morphological structure at a single neuron scale positioned across the whole mouse brain.  However, the enormous amount of morphological data challenge the classic approach of neuron classification, indexing and other analysis tasks. In this paper, we propose MorphoGNN, a single neuron morphological embedding based on the graph neural networks~(GNN). This method learns the spatial structure information between the nodes of reconstructed neuron fibers by its nearest neighbors on each layer and captures the lower-dimensional representation of a single neuron through an end-to-end model. This model is composed of densely connected edge convolution~(EdgeConv) layers and double pooling operator, regularized with joint cross-entropy loss and triplet loss. An increasing population of the neighbor nodes meets the need of learning more information with features expanding at deep layer. We tested the proposed embeddings on the neuron classification, retrieval, and neuron subtyping tasks. Our method achieves competitive performance both on the general point cloud dataset and the neuron morphology dataset.

## Be Confident! Towards Trustworthy Graph Neural Networks via Confidence Calibration

### TL;DR

None

### Abstract

Despite Graph Neural Networks (GNNs) have achieved remarkable accuracy, whether the results are trustworthy is still unexplored. Previous studies suggest that many modern neural networks are over-confident on the predictions, however, surprisingly, we discover that GNNs are primarily in the opposite direction, i.e., GNNs are under-confident. Therefore, the confidence calibration for GNNs is highly desired. In this paper, we propose a novel trustworthy GNN model by designing a topology-aware post-hoc calibration function. Specifically, we first verify that the confidence distribution in a graph has homophily property, and this finding inspires us to design a calibration GNN model (CaGCN) to learn the calibration function. CaGCN is able to obtain a unique transformation from logits of GNNs to the calibrated confidence for each node, meanwhile, such transformation is able to preserve the order between classes, satisfying the accuracy-preserving property. Moreover, we apply the calibration GNN to self-training framework, showing that more trustworthy pseudo labels can be obtained with the calibrated confidence and further improve the performance. Extensive experiments demonstrate the effectiveness of our proposed model in terms of both calibration and accuracy.

## Graph Neural Networks for High-Level Synthesis Design-Space Exploration

### TL;DR

We use graph neural networks to automate the design-space exploration of hardware accelerators with High-level Synthesis 

### Abstract

The design of efficient hardware accelerators for high-throughput data-processing applications, e.g., deep neural networks, is a challenging task in computer architecture design. In this regard, High-Level Synthesis (HLS) emerges as a solution for fast prototyping application-specific hardware starting from a C/C++ behavioural description of the application computational flow. In the accelerator synthesis phase, designers apply HLS directives to optimize the hardware implementation, by trading-off cost and performance. This Design-Space Exploration (DSE) aims at identifying Pareto optimal synthesis configurations whose exhaustive search is often unfeasible due to the design-space dimensionality and the prohibitive computational cost of the synthesis process. Within this framework, we effectively and efficiently address the design problem by proposing, for the first time in the literature, graph neural networks that jointly predict acceleration performance and hardware costs of a synthesized behavioral specification given optimization directives. The learned model can be used to rapidly approach the Pareto curve by guiding the DSE, taking into account performance and cost estimates. The proposed method outperforms traditional HLS-driven DSE approaches, by accounting for arbitrary length of computer programs and the invariant properties of the input. We propose a novel hybrid control and data flow graph representation that enables training the graph neural network on specifications of different hardware accelerators; the methodology naturally transfers to unseen data-processing applications too. Moreover, we show that our approach achieves prediction accuracy comparable with that of commonly used simulators without having access to analytical models of the HLS compiler and the target FPGA, while being orders of magnitude faster. Finally,  the learned representation can be exploited for DSE in unexplored configuration spaces by fine-tuning on a small number of samples from the new target domain. The outcome of the empirical evaluation of this transfer learning shows strong results against state-of-the-art baselines in relevant benchmarks including neural processing.

## New Insights into Graph Convolutional Networks using Neural Tangent Kernels

### TL;DR

None

### Abstract

Graph Convolutional Networks (GCNs) have emerged as powerful tools for learning on network structured data. Although empirically successful, GCNs exhibit certain behaviour that has no rigorous explanation---for instance, the performance of GCNs significantly degrades with increasing network depth, whereas it improves marginally with depth using skip connections.

This paper focuses on semi-supervised learning on graphs, and explains the above observations through the lens of Neural Tangent Kernels (NTKs). We derive NTKs corresponding to infinitely wide GCNs (with and without skip connections).  Subsequently, we use the derived NTKs to identify that, with suitable normalisation, network depth does not always drastically reduce the performance of GCNs---a fact that we also validate through extensive simulation. Furthermore, we propose NTK as an efficient 'surrogate model' for GCNs that does not suffer from performance fluctuations due to hyper-parameter tuning since it is a hyper-parameter free deterministic kernel. The efficacy of this idea is demonstrated through a comparison of different skip connections for GCNs using the surrogate NTKs.

## DiskE: A Fully Expressive Geometric Model for Knowledge Graph Embedding

### TL;DR

We propose a fully expressive geometric model for knowledge graph embedding that is capable of representing symmetry, inversion, and composition.

### Abstract

Knowledge graphs (KGs) can describe various types of information in a unified manner, which is useful for incorporating human knowledge.
Although these structured datasets have various applications, there are numerous missing links to be complemented.
Embedding models have been proven to be practical methods to predict missing links; however, few models combine to ensure expressive power theoretically, and represent inference patterns explicitly.
Our proposed model, DiskE, combines both of these properties.
We theoretically prove that DiskE is fully expressive; that is, DiskE can represent any KG accurately.
In practice, the fully expressive models, including DiskE, perform better than other models in the link prediction task. In particular, DiskE achieves the best results with the WN18RR dataset.
We also show that DiskE can represent multiple inference patterns, including composition patterns, which means that DiskE can infer a missing link from multiple sources.
We design DiskE to be constructed only by geometric operations, which enables us to check the capability of inference patterns; 
DiskE successfully captures inference patterns inherent in KGs in numerical experiments on the FB15k-237 dataset.
To the best of our knowledge, there are no other models that combine full expressiveness and the capability of representing composition patterns.

## Spatially Invariant Unsupervised 3D Object Segmentation with Graph Neural Networks

### TL;DR

We defined a spatial mixture model over point cloud which allows us to achieve unsupervised point cloud segmentation without RGB information.

### Abstract

 In this paper, we tackle the problem of unsupervised 3D object segmentation from a point cloud without RGB information. In particular, we propose a framework, SPAIR3D, to model a point cloud as a spatial mixture model and jointly learn the multiple-object representation and segmentation in 3D via Variational Autoencoders (VAE). Inspired by SPAIR, we adopt an object-specification scheme that describes each object's location relative to its local voxel grid cell rather than the point cloud as a whole. To model the spatial mixture model on point clouds, we derive the Chamfer Likelihood, which fits naturally into the variational training pipeline. We further design a new spatially invariant graph neural network to generate a varying number of 3D points as a decoder within our VAE. Experimental results demonstrate that SPAIR3D is capable of detecting and segmenting variable number of objects without appearance information across diverse scenes.

## Learning to Walk with Dual Agents for Knowledge Graph Reasoning

### TL;DR

None

### Abstract

Reinforcement learning (RL) based graph walking has shown great success in navigating an agent to automatically complete various reasoning tasks over an incomplete knowledge graph (KG) by exploring multi-hop relational paths. This is undesirable for many reasoning tasks in real-world scenarios, where short paths connecting the source and target entities are not available in incomplete KGs, and thus the reasoning performances drop drastically unless the agent is able to seek out more clues from longer paths. To address the challenge, in this paper, we propose a dual-agent reinforcement learning framework, which trains two agents (\textsc{giant} and \textsc{dwarf}) to walk over a KG \textit{jointly} and search for the answer \textit{collaboratively}. Our approach tackles the reasoning challenge in long paths by assigning one of the agents (\textsc{giant}) searching on cluster-level paths quickly and providing stage-wise hints for another agent (\textsc{dwarf}). Finally, experimental results on several KG reasoning benchmarks show that our approach can search answers more accurately and efficiently, and outperforms \prev{other} existing RL-based methods for long path queries by a large margin.

## Model-based Meta Reinforcement Learning using Graph Structured Surrogate Models

### TL;DR

None

### Abstract

Reinforcement learning is a promising paradigm for solving sequential decision-making problems, but low data efficiency and weak generalization across tasks are bottlenecks in real-world applications. Model-based meta reinforcement learning addresses these issues by learning dynamics and leveraging knowledge from prior experience. In this paper, we take a closer look at this framework, and propose a new posterior sampling based approach that consists of a new model to identify task dynamics together with an amortized policy optimization step. We show that our model, called a graph structured surrogate model (GSSM), outperforms state-of-the-art methods in predicting environment dynamics. Additionally, our approach is able to obtain high returns, while allowing fast execution during deployment by avoiding test-time policy gradient optimization.

## Revisiting Transformation Invariance of Geometric Data using Graph Neural Networks

### TL;DR

None

### Abstract

Deep neural networks have achieved great successes in many domains in the last decade. When designing neural networks to handle the ubiquitous geometric data such as point clouds and graphs, it is critical that the model can maintain invariance towards various transformations such as translation, rotation, and scaling. The existing graph neural network (GNN) approaches can only maintain permutation-invariance, failing to guarantee invariance with respect to other transformations. Besides GNNs, other works design sophisticated transformation-invariant layers, which are computationally expensive and difficult to be extended. To solve this problem, we revisit why the existing GNNs cannot maintain transformation invariance when handling geometric data. Our findings show that transformation-invariant and distance-preserving initial node representations are sufficient to achieve transformation invariance. Motivated by these findings, we propose Transformation Invariant GNN (TinvGNN), a straightforward and general framework for geometric data. Specifically, we realize transformation-invariant and distance-preserving initial node representations by modifying multi-dimensional scaling (MDS) before feeding the representations into GNNs. We theoretically prove that the proposed TinvGNN can strictly guarantee transformation invariance, being general and flexible enough to be combined with the existing neural networks. Extensive experimental results on point cloud analysis and combinatorial optimization demonstrate the effectiveness and general applicability of our proposed method. 

## A Graph Fusion Approach to Cross-Lingual Machine Reading Comprehension

### TL;DR

Cross-lingual Machine Reading Comprehension

### Abstract

Although great progress has been made for Machine Reading Comprehension (MRC) in English, scaling out to a large number of languages remains a huge challenge due to the lack of large amounts of annotated training data in non-English languages. To address this challenge, some recent efforts of cross-lingual MRC employ machine translation to transfer knowledge from English to other languages, through either explicit alignment or implicit attention. For effective knowledge transition, it is beneficial to leverage both semantic and syntactic information. However, the existing methods fail to explicitly incorporate syntax information in model learning. Consequently, the models are not robust to errors in alignment and noises in attention. In this paper, we propose a novel method, GraFusionMRC, which jointly models the cross-lingual alignment information and the mono-lingual syntax information using a graph. We develop a series of algorithms including graph construction, learning, and pre-training. The experiments on two benchmark datasets for cross-lingual MRC shows that our approach improves the state-of-the-art methods with a large margin, and verify the effectiveness of syntax information for cross-lingual MRC. The code will be made open sourced on github.

## A GNN Based Approach to LTL Model Checking

### TL;DR

None

### Abstract

Model Checking is widely applied in verifying complicated and especially concurrent systems. Despite of its popularity, model checking suffers from the state space explosion problem that restricts it from being applied to certain systems, or specifications. Many works have been proposed in the past to address the state space explosion problem, and they have achieved some success, but the inherent complexity still remains an obstacle for purely symbolic approaches. In this paper, we propose a Graph Neural Network (GNN) based approach for model checking, where the model is expressed using a B{\"u}chi automaton and the property to be verified is expressed using Linear Temporal Logic (LTL). We express the model as a GNN, and propose a novel node embedding framework that encodes the LTL property and characteristics of the model. We reduce the LTL model checking problem to a graph classification problem, where there are two classes, 1 (if the model satisfies the specification) and 0 (if the model does not satisfy the specification). The experimental results show that our framework predicts results with very high accuracy and is up to 17 times faster than state-of-the-art tools. Our approach is particularly useful when dealing with very large LTL formulae and small to moderate sized models.

## A Variational Approach for Combinatorial Optimization Problems on Graphs

### TL;DR

None

### Abstract

Combinatorial Optimization (CO) problems are often NP-hard, thereby hindering the collection of labeled data for supervised learning.
Graphical models can provide an unsupervised framework, however, inference on it is intractable in general.
Combining those two problems, our work proposes an unsupervised method to learn a variational distribution, from which we can efficiently sample good solutions for CO problems. 
We show that our designed graphical model is guaranteed to concentrate on good solutions in both constrained and unconstrained scenarios.
Additionally, the graphical model naturally introduces a temperature with which we are able to ease the learning by avoiding or encouraging local optima during training.
We demonstrate our unsupervised learning framework on three CO problems on both synthetic and real-world graphs. 
The results show that our method achieves performance significantly better than other unsupervised neural methods as well as classical methods and integer solvers.

## LPRules: Rule Induction in Knowledge Graphs Using Linear Programming

### TL;DR

None

### Abstract

Knowledge graph (KG) completion is a well-studied problem in AI. Rule-based methods and embedding-based methods form two of the solution techniques.  Rule-based methods learn first-order logic rules that capture existing facts in an input graph and then use these rules for reasoning about missing facts. A major drawback of such methods is the lack of scalability to large datasets.  In this paper, we present a simple linear programming (LP) model to choose rules and assign weights to them. To deal with exponentially many rules, we start with a small initial subset of rules, and then use standard column generation ideas to add more rules in order to improve the LP model objective value. To foster interpretability and generalizability, we limit the complexity of the set of chosen rules via explicit constraints, and tune the complexity hyperparameter for individual datasets. We show that our method can obtain state-of-the-art results for three out of four widely used KG datasets, while taking significantly less computing time than other popular rule learners including some based on neuro-symbolic methods. The improved scalability of our method allows us to tackle large datasets such as YAGO3-10.

## Principled Hyperedge Prediction with Structural Spectral Features and Neural Networks

### TL;DR

We propose SNALS a framework for hyperedge prediction in tackling the principle ambiguity issues in hypergraph structural learning with graph neural network.

### Abstract

Hypergraph offers a framework to depict the multilateral relationships in real-world complex data. Predicting higher-order relationships, i.e hyperedge, becomes a fundamental problem for the full understanding of complicated interactions. The development of graph neural network (GNN) has greatly advanced the analysis of ordinary graphs with pair-wise relations. However, these methods could not be easily extended to the case of hypergraph. In this paper, we generalize the challenges of GNN in representing higher-order data in principle, which are edge- and node-level ambiguities. To overcome the challenges, we present \textbf{SNALS} that utilizes bipartite graph neural network with structural features to collectively tackle the two ambiguity issues. SNALS captures the joint interactions of a hyperedge by its local environment, which is retrieved by collecting the spectrum information of their connections. As a result, SNALS achieves nearly 30\% performance increase compared with most recent GNN-based models. In addition, we applied SNALS to predict genetic higher-order interactions on 3D genome organization data. SNALS showed consistently high prediction accuracy across different chromosomes, and generated novel findings on 4-way gene interaction, which is further validated by existing literature. 

## TAMP-S2GCNets: When Time-Aware Multipersistence Meets Spatio-Supra Graph Convolutional Nets while Forecasting Time Series

### TL;DR

We take a step forward to integrate the two emerging directions in time-aware machine learning: time-conditioned Graph Neural Networks and time-aware topological multidimensional representations, with a focus on multivariate time series forecasting. 

### Abstract

In the last few of years, Graph Convolutional Networks (GCNs) gain increasing popularity as a powerful alternative for learning complex dependencies in multivariate spatio-temporal processes. However, most existing GCNs have inherently static architectures and as a result, do not explicitly account for time dependencies of the encoded knowledge and are limited in simultaneously inferring latent temporal relations among entities. In turn, some important hidden time-conditioned interrelations may be captured by the machinery of multipersistence which allows us to quantify dynamics of the data shape along multiple geometric dimensions. We make the first step toward integrating the two emerging directions and propose a new model, Time-Aware Multipersistence Spatio-Supra Graph Convolutional Networks (TAMP-S2GCNets). We summarize inherent time-conditioned topological properties of the data as time-aware multipersistence Euler-Poincar\'e surface and prove its stability. We then construct a supragraph convolution module which simultaneously accounts for the extracted intra- and inter- spatio-temporal dependencies in the data. Our extensive experiments on highway traffic flow, Ethereum token prices, and COVID-19 hospitalizations demonstrate that TAMP-S2GCNets outperforms the state-of-the-art tools in multivariate time series forecasting tasks.

## Self-supervised Incremental Deep Graph Learning for Ethereum Phishing Scam Detection

### TL;DR

None

### Abstract

In recent years, phishing scams have become the crime type with largest money involved on Ethereum, the second largest blockchain platform. Meanwhile, graph neural network (GNN) has shown promising performance in various node classification tasks. However, for Ethereum transaction data, which could be naturally abstracted to a real world complex graph, the scarcity of labels and the huge volume of transaction data make it difficult to take advantage of GNN methods. Here in this paper, to address the two challenges, we propose a \textbf{S}elf-supervised \textbf{I}ncr\textbf{E}mental deep \textbf{G}raph l\textbf{E}arning model (\textbf{SIEGE}), for the phishing scam detection problem on Ethereum. In our model, two pretext tasks designed from spatial and temporal perspectives help us effectively learn useful node embedding from the huge amount of unlabelled transaction data. And the incremental paradigm allows us to efficiently handle large scale transaction data and help the model maintain good performance when the data distribution is drastically changing. We collect transaction records about half an year from Ethereum and our extensive experiments show that our model consistently outperforms strong baselines in both transductive and inductive settings.

## TNCC: Tackling Oversmoothing in Spatio-Temporal Model with Topology-based Regularizer

### TL;DR

None

### Abstract

Graph-based neural networks have demonstrated great success in a wide range of spatio-temporal prediction tasks, including traffic modeling and disease prediction. However, existing methods often suffer from the oversmoothing issue which is recognized as the major reason limiting the depth and expressive power of GNN models in spatio-temporal modeling. This paper summarizes the topology-based oversmoothing regularizers in a general form and proposes a new temporal normalized cross-correlation regularizer TNCC. Compared to previous topology-based regularizers, TNCC encourages stronger decorrelation between node embeddings. We further adapt TNCC with spatio-temporal prediction models by reducing the temporal variance and space complexity. We evaluate TNCC on real-world spatio-temporal datasets and node classification benchmark datasets. We also evaluate TNCC under different settings such as large node distribution shift and scaling as well as semi-supervised tasks. The results show that TNCC outperforms baseline models under most settings and significantly reduces oversmoothing, especially for dynamic graph data. For spatio-temporal prediction tasks, TNCC achieves up to 34% lower MSE than naive GAT model. For node classification tasks, TNCC achieves up to 10% higher accuracy compared to the best baseline regularizer.

## Towards a Rigorous Theoretical Analysis and Evaluation of GNN Explanations

### TL;DR

An axiomatic framework to theoretically analyze, empirically evaluate, and compare GNN explanation methods.

### Abstract

As Graph Neural Networks (GNNs) are increasingly employed in real-world applications, it becomes critical to ensure that the stakeholders understand the rationale behind their predictions.   While several GNN explanation methods have been proposed recently, there has been little to no work on theoretically analyzing the behavior of these methods or systematically evaluating their effectiveness. Here, we introduce the first axiomatic framework for theoretically analyzing, evaluating, and comparing state-of-the-art GNN explanation methods. We outline and formalize the key desirable properties that all GNN explanation methods should satisfy in order to generate reliable explanations, namely, faithfulness, stability, and fairness. We leverage these properties to present the first ever theoretical analysis of the effectiveness of state-of-the-art GNN explanation methods. Our analysis establishes upper bounds on all the aforementioned properties for popular GNN explanation methods. We also leverage our framework to empirically evaluate these methods on multiple real-world datasets from diverse domains. Our empirical results demonstrate that some popular GNN explanation methods (e.g., gradient-based methods) perform no better than a random baseline and that methods which leverage the graph structure are more effective than those that solely rely on the node features.

## Successive POI Recommendation via Brain-inspired Spatiotemporal Aware Representation

### TL;DR

None

### Abstract

POI vector representation (embedding) is the core of successive POI recommendation. However, existing approaches only rely on basic discretization and interval analyses and fail to fully exploit complicated spatiotemporal attributes of POIs. Neuroscience research has shown that the mammalian brain entorhinal-hippocampal system provides efficient graph representations for general knowledge. Moreover, entorhinal grid cells present concise spatial representations, while hippocampal place cells represent perception compilations effectively. Thus, the entorhinal-hippocampal system provides a novel angle for spatiotemporal aware representation, which inspires us to propose the SpatioTemporal aware Embedding model of POIs (STEP). STEP considers two types of POI-specific representations: sequential representation and spatiotemporal conjunctive representation. Both representations are learned using sparse unlabeled data based on the proposed graph-building policies. Notably, the spatiotemporal conjunctive representation represents POIs from spatial and temporal aspects jointly and precisely. Furthermore, we introduce a successive POI recommendation method using STEP and simple recurrent networks. Experimental results on two datasets demonstrate that STEP captures POI-specific spatiotemporal information more accurately and achieves the state-of-the-art successive POI recommendation performance. Therefore, STEP provides a novel solution to spatiotemporal aware representation and paves a new way for spatiotemporal modeling-related tasks.

## Video Compressive Sensing via Dynamic Spatial and Temporal Correlations Modeling

### TL;DR

None

### Abstract

Video snapshot compressive imaging (SCI) utilizes a 2D detector to capture sequential video frames and compresses them into a single measurement. Various reconstruction methods have been developed to recover the high-speed video frames from the snapshot measurement. However, most existing reconstruction methods are incapable of capturing long-range spatial and temporal dependencies, which are critical for video processing. In this paper, we propose a flexible and robust module based on dynamic graph to improve existing state-of-the-art models for video SCI reconstruction by modeling long-range interactions between pixels in space as well as time regardless of distance. Specifically, we employ a spatial transformer network (STN) to estimate global transformation and develop a cross-scale dynamic graph neural network for efficiently capturing locally varying motions for compensatory, which consists of position-specific dynamic walks, cross-scale node sampling and node-conditioned dynamic aggregation. Extensive results on both simulation and real data demonstrate the effectiveness and efficiency of the proposed module appended to state-of-the-art methods, and the visualization clearly illustrates the intrinsic dynamic operations of our proposed model for boosting the video SCI reconstruction results.  

## COMPASS: Contrastive Multimodal Pretraining for Autonomous Systems

### TL;DR

We introduce the first demonstration of a `"general-purpose" pretraining approach that learns multimodal representations for various downstream tasks in autonomous systems.

### Abstract

Learning representations that generalize across tasks and domains is challenging yet necessary for autonomous systems. Although task-driven approaches are appealing, designing models specific to each application can be limiting with insufficient data, especially when dealing with highly variable multimodal input spaces arising from different tasks in different environments. We introduce the first general-purpose pretraining pipeline, COntrastive Multimodal Pretraining for AutonomouS Systems (COMPASS), to overcome such limitations arising from task-specific models. COMPASS constructs a multimodal graph by considering the essential information for autonomous systems and the properties of different modalities. In the graph network, multimodal signals are connected and mapped into two factorized spatio-temporal latent spaces: a ``motion pattern space'' and a ``current state space.'' By learning from multimodal correspondence in each latent space, it creates state representations of the environment, allowing it to infer necessary information such as temporal dynamics, geometry, and semantics. We train COMPASS on a large-scale multimodal simulation dataset TartanAIR~\cite{tartanair2020iros} and evaluate it on drone navigation, vehicle racing, and visual odometry tasks. On each, COMPASS proves effective for improving perception of autonomous agents, generalizing to unseen environments while being robust to noisy visual scenes.

## Metapath-guided Attribute Network Filtering for Accurate and Efficient GNN-based Recommendation

### TL;DR

We propose a metapath-guided attribute network filtering for accurate and efficient GNN-based recommendation, called MFGRec.

### Abstract

Graph neural networks (GNNs) have exhibited a strong capacity for learning graph structure data, which holds great promise for recommender systems. Some GNN-based methods incorporate side information (such as attributes and social information) to enhance the user/item representations. The adding of side information helps to address the problem of data sparsity and improve the accuracy of recommender systems, but GNNs usually become less efficient as the size and noise of networks increases. In this work, we propose a metapath-guided attribute network filtering for accurate and efficient GNN-based recommendation, called MFGRec. Our method can maintain high accuracy and efficiency when there is a massive amount of diverse network data by using metapath-guided attribute network filtering to select more valuable neighbors. We also design a semantic attention fusion method to distinguish the semantic differences from the diverse metapaths and to improve the expressive ability of user/item representations. Experimental results on three publicly accessible datasets and nine baselines demonstrate that our model achieves more accuracy in terms of recommendation and lowers runtime costs compared with existing state-of-the-art methods, as well as shows good interpretability.

## Address the Unseen Relationships: Attribute-based Person Search with Graph Neural Networks and Pseudo-Label

### TL;DR

None

### Abstract

Attribute-based person search aims to identify the particular pedestrian by textual attribute information. Compared to person re-identification which requires imagery samples as its query, attribute-based person search is more useful under the circumstance that only witness is available. Most existing attribute-based person search methods focus on improving the matching correlation and alignments by learning better representations of person-attribute instance pairs, with no consideration of the latent relationships between attributes. In this work, we propose a graph neural network(GNN) and pseudo-label based person search method. Concretely, the model directly constructs the graph based on label co-occurrence probability, in which the nodes are represented by attribute embeddings and edges are by the filtered correlation matrix of attribute labels. In order to obtain better representations, we combine the cross-attention module and the GNN. Further, to address the unseen attribute relationships, we update the edge information through the instances in testing set with high predicted probability thus to better adapt the attribute distribution. Extensive experiments illustrate that our model outperforms the existing state-of-the-art methods on two publicly available person search benchmarks: Market-1501 and PETA.

## Decentralized Optimization with Heterogeneous Delays: a Continuous-Time Approach

### TL;DR

This paper studies decentralized optimization using a novel time-continuous approach, in order to derive sharp stability conditions and convergence guarantees under heterogneneous communication and computation delays.

### Abstract

In decentralized optimization, nodes of a communication network privately possess a local objective function, and communicate using gossip-based methods in order to minimize the average of these per-node objectives. While synchronous algorithms can be heavily slowed down by a few nodes and edges in the graph (the \emph{straggler problem}), their asynchronous counterparts lack from a sharp analysis taking into account heterogeneous delays in the communication network. In this paper, we propose a novel continuous time framework to analyze asynchronous algorithms which does not require to define a global ordering of the events, and allows to finely characterize the time complexity in the presence of (heterogeneous) delays. Using this framework, we describe a fully asynchronous decentralized algorithm to minimize the sum of smooth and strongly convex functions. Our algorithm (\emph{DCDM}, Delayed Coordinate Dual Method), based on delayed randomized gossip communications and local computational updates, is to our knowledge the first to achieve an asynchronous speed-up: the rate of convergence is tightly characterized in terms of the eigengap of the graph weighted by local delays only, instead of the global worst-case delays as in previous analyses. 

## A Unifying View of Recursions for Path Planning Using Probabilistic Inference 

### TL;DR

None

### Abstract

Path planning problems can be solved using standard techniques from dynamic programming and control and/or by viewing planning as probabilistic inference.  The idea of using estimation on stochastic models to solve control problems falls under the rubric of Active Inference (AI) and Control as Inference (CAI).  In this work, we take an algorithmic approach by focusing on the specific recursions that arise in various planning scenarios, that, although they appear similar, show noticeable differences, at least when applied to the typical path planning problem. We start by posing the path planning problem on a probabilistic factor graph, and show how the various algorithms translate into specific message composition rules. We then show how this unified approach, presented both in probability space and in log-space, provides a very general framework that includes the Sum-product, the Max-product, Dynamic programming and mixed Reward/Entropy criteria-based algorithms. The framework also expands algorithmic design options for smoother or sharper policy distributions, including generalized Sum/Max-product algorithm, a Smooth Dynamic programming algorithm and modified versions of the Reward/Entropy recursions. We provide a comparison of the expanded suite of algorithms via simulations on a grid extrapolated form a real-world scene for a single agent with multiple goals, enriched with a semantic map. 

## Revisit Layer-wise Sampling in Fast Training for Graph Convolutional Networks

### TL;DR

None

### Abstract

Many sampling-based methods have been developed for approximating embedding aggregation in graph convolutional network (GCN) training, inspired by variance reduction. Among them, layer-wise sampling recursively samples some columns in the Laplacian matrix; node-wise sampling instead samples some neighbors of a certain node, which may become computationally inefficient when the adjacency matrix is large. This paper revisits the two approaches from a perspective of approximate matrix multiplication (AMM), and evaluates the approximation accuracy and computational efficiency as two metrics characterizing the sampling quality. Based on the metrics, our analysis shows that the layer-wise sampling incurs inaccurate inference when they cannot guarantee vertex cover of the nodes from the previous layer. We thus propose a new sampling-based method towards an enhanced balance between training efficiency and generalization accuracy. Extensive analysis and experiments demonstrate the proposed method attains accuracy close to cutting edge node-wise sampling methods, while is as efficient as layer-wise sampling methods.

## Hierarchically Regularized Deep Forecasting

### TL;DR

We propose a model for hierarchical time series forecasting that incorporates the hierarchy structure into the model.

### Abstract

Hierarchical forecasting is a key problem in many practical multivariate forecasting applications - the goal is to simultaneously predict a large number of correlated time series that are arranged in a pre-specified aggregation hierarchy. The challenge is to exploit the hierarchical correlations to simultaneously obtain good prediction accuracy for time series at different levels of the hierarchy. In this paper, we propose a new approach for hierarchical forecasting based on decomposing the time series along a global set of basis time series and modeling hierarchical constraints using the coefficients of the basis decomposition for each time series. Unlike past methods, our approach is scalable at inference-time (forecasting for a specific time series only needs access to its own data) while (approximately) preserving coherence among the time series forecasts. We experiment on several publicly available datasets and demonstrate significantly improved overall performance on forecasts at different levels of the hierarchy, compared to existing state-of-the-art hierarchical reconciliation methods.

## KID-REVIEW: Knowledge-Guided Scientific Review Generation with Oracle Pre-training

### TL;DR

A knowledge guided approach to generate scientific peer reviews.

### Abstract

The surge in the number of scientific submissions has brought challenges to the work of peer review. In this paper, as a first step, we explore the possibility of designing an automated system, which is not meant to replace humans, but rather providing a first-pass draft for a machine-assisted human review process. Specifically, we present an end-to-end knowledge-guided review generation framework for scientific papers grounded in cognitive psychology research that a better understanding of text requires different types of knowledge. In practice, we found that this seemingly intuitive idea suffered from training difficulties. In order to solve this problem, we put forward an oracle pre-training strategy, which can not only make the Kid-Review better educated but also make the generated review cover more aspects. Experimentally, we perform a comprehensive evaluation (human and automatic) from different perspectives. Empirical results have shown the effectiveness of different types of knowledge as well as oracle pre-training. We make all code, relevant dataset available: https://github.com/Anonymous4nlp233/KIDReview as well as the Kid-Review system: http://nlpeer.reviews.

## The Expressive Power of GNNs in the Context of Node Features

### TL;DR

The paper studies the expressive power of GNNs when they use extra (random) features 

### Abstract

Previous studies indicate that most GNNs are no more powerful than the 1-WL algorithm, but the limitation is only obvious when there are no node features. This work considers the nested structure of GNN's function spaces in terms of the amount of (random) node features. We constructively show that extra node features increase GNN's expressive power even if these features contain no information about node or graph labels. The analysis deepens our understanding of how a GNN can exploit node features. We further enhance a GNN's learning performance by providing random features and propose measures to reduce the variance of model outputs. Our analysis is well supported by extensive experiments.

## Neural Bellman-Ford Networks: A General Graph Neural Network Framework for Link Prediction

### TL;DR

A scalable and interpretable framework for link prediction inspired by the generalized Bellman-Ford algorithm.

### Abstract

Link prediction is a very fundamental task on graphs. Inspired by traditional path-based methods, in this paper we propose a general and flexible representation learning framework based on paths for link prediction. Specifically, we define the representation of a pair of nodes as the generalized sum of all path representations, with each path representation as the generalized product of the edge representations in the path. Motivated by the Bellman-Ford algorithm for solving the shortest path problem, we show that the proposed path formulation can be efficiently solved by the generalized Bellman-Ford algorithm. To further improve the capacity of the path formulation, we propose the Neural Bellman-Ford Network (NBFNet), a general graph neural network framework that solves the path formulation with learned operators in the generalized Bellman-Ford algorithm. The NBFNet parameterizes the generalized Bellman-Ford algorithm with 3 neural components, namely Indicator, Message and Aggregate functions, which corresponds to the boundary condition, multiplication operator, and summation operator respectively. The NBFNet is very general, covers many traditional path-based methods, and can be applied to both homogeneous graphs and multi-relational graphs (e.g., knowledge graphs) in both transductive and inductive settings. Experiments on both homogeneous graphs and knowledge graphs show that the proposed NBFNet outperforms existing methods by a large margin in both transductive and inductive settings, achieving new state-of-the-art results.

## Active Learning Convex Halfspaces on Graphs

### TL;DR

We systematically study the query complexity of learning geodesically convex halfspaces on a vertex-labelled graph. 

### Abstract

We systematically study the query complexity of learning geodesically convex halfspaces on vertex-labelled graphs. Geodesic convexity is a natural generalisation of Euclidean convexity and allows the definition of convex sets and halfspaces on graphs. We prove upper bounds on the query complexity linear in the treewidth and the minimum hull set size but only logarithmic in the diameter. We show tight lower bounds corresponding to well-established separation axioms. Additionally, we identify the Radon number as a central parameter bounding the query complexity and the VC dimension. While previous bounds typically depend on the cut size of the labelling, all parameters in our bounds can be computed from the unlabelled graph. We empirically compare our proposed approach with other active learning algorithms and provide evidence that ground-truth communities in real-world graphs are often convex.

## GeoMol: Torsional Geometric Generation of Molecular 3D Conformer Ensembles

### TL;DR

We generate ensembles of 3D conformers from the input molecular graph in an end-to-end fashion by explicitly modeling local atomic 3D structures, torsion angles, chirality, and other geometric elements. 

### Abstract

Prediction of a molecule's 3D conformer ensemble from the molecular graph holds a key role in areas of cheminformatics and drug discovery. Existing generative models have several drawbacks including lack of modeling important molecular geometry elements (e.g. torsion angles), separate optimization stages prone to error accumulation, and the need for structure fine-tuning based on approximate classical force-fields or computationally expensive methods such as metadynamics with approximate quantum mechanics calculations at each geometry. We propose GeoMol -- an end-to-end, non-autoregressive and SE(3)-invariant machine learning approach to generate distributions of low-energy molecular 3D conformers. Leveraging the power of message passing neural networks (MPNNs) to capture local and global graph information, we predict local atomic 3D structures and torsion angles, avoiding unnecessary over-parameterization of the geometric degrees of freedom (e.g. one angle per non-terminal bond). Such local predictions suffice both for the training loss computation, as well as for the full deterministic conformer assembly (at test time). We devise a non-adversarial optimal transport based loss function to promote diverse conformer generation. GeoMol predominantly outperforms popular open-source, commercial, or state-of-the-art machine learning (ML) models, while achieving significant speed-ups. We expect such differentiable 3D structure generators to significantly impact molecular modeling and related applications.

## Learning Symbolic Program Representations for Food Images and Cooking Recipes

### TL;DR

We propose to learn cooking programs as a meaningful and rich semi-structured high-level representation for food images and cooking recipes.

### Abstract

In this paper, we are interested in modeling a how-to instructional procedure, such as a cooking recipe into a meaningful and rich high-level representation. We propose to represent cooking recipes as programs. Programs provide a non-ambiguous symbolic representation of the task, are easily manipulated and can be potentially executed by agents. Also, programs capture cooking semantics and sequential relationships of actions leading to a corresponding graph. We build a model that is trained to learn a joint embedding between cooking recipes and food images via self-supervision and jointly generate a program from this embedding as a sequence. Since the sequence of some actions can be permuted without violating the recipe graph, we train the sequence model with the set of all valid programs and introduce an objective function that operates on this set. To validate our idea, we crowdsource programs for cooking recipes and show that: (a) projecting the image-recipe embeddings into programs leads to better cross-modal retrieval results; (a) generating programs from images leads to better recognition results compared to predicting raw cooking instructions; (c) using our model, we show how to generate food images by manipulating programs via optimizing the latent code of a GAN.

## Learning to Maximize Influence

### TL;DR

None

### Abstract

As the field of machine learning for combinatorial optimization advances, traditional problems are resurfaced and readdressed through this new perspective. The overwhelming majority of the literature focuses on small graph problems, while several real-world problems are devoted to large graphs. Here we focus on two such problems that are related: influence estimation, a \#P-hard counting problem, and influence maximization, an NP-hard problem. We develop GLIE, a Graph Neural Network (GNN) that inherently parameterizes an upper bound of influence estimation and train it on small simulated graphs.
Experiments show that GLIE can provide accurate predictions faster than the alternatives for graphs 10 times larger than the train set.  More importantly, it can be used on arbitrary large graphs for influence maximization, as the predictions can rank effectively seed sets even when the accuracy deteriorates. To showcase this we propose a version of a standard Influence Maximization (IM) algorithm where we substitute traditional influence estimation with the predictions of GLIE. We also transfer GLIE into a reinforcement learning model that learns how to choose seeds to maximize influence sequentially using GLIE's hidden representations and predictions. The final results show that the proposed methods surpass a previous GNN-RL approach and perform on par with a SOTA IM algorithm.

## Generalized Results for the Existence and Consistency of the MLE in the Bradley-Terry Model

### TL;DR

None

### Abstract

Ranking problems based on pairwise comparisons, such as those arising in online gaming, often involve a large pool of items to order.  In these situations, the gap in performance between any two items can be significant, and the smallest and largest winning probabilities can be very close to zero or one. Furthermore, each item may be compared only to a subset of all the items, so that not all pairwise comparisons are observed. In this paper, we study the performance of the Bradley-Terry-Luce model for ranking from pairwise comparison data under more realistic settings than those considered in the literature so far. In particular, we allow for near-degenerate winning probabilities and arbitrary comparison designs. We obtain novel results about the existence and $\ell_2$ convergence rates of the maximum likelihood estimator of the model parameters without the bounded winning probability assumption commonly used in the literature and for arbitrary comparison graph topologies. Central to our approach is the reliance on the Fisher information matrix to express the dependence on the graph topologies and the impact of the values of the winning probabilities on the estimation risk and on the conditions for the existence of the MLE. Our bounds recover existing results as special cases but are more broadly applicable.

## Towards Fundamental Limits of Multi-armed Bandits with Random Walk Feedback

### TL;DR

None

### Abstract

Despite the ubiquitous applications of bandit learning algorithms in recommendation systems, social network, or online advertisement, where user behaviors can be modeled as a random walk over a network, few studies have utilized the network structure to improve learning efficiency. In this paper, we address this issue by providing a novel bandit learning formulation, where each arm is the starting node of a random walk in a network and the reward is the length of walk. This formulation not only captures a large number of applications in practice but also provides a framework to actively reduce learning complexity by utilizing graph structure in the random walk feedback. We provide a comprehensive understanding of this formulation by studying both the stochastic and the adversarial setting. In the stochastic setting, we show that this problem is not easier than a standard MAB, although additional information is available through random walk trajectories. In the adversarial setting, we establish a novel algorithm that achieves regret bound of order $\widetilde{\mathcal{O}} \left( \sqrt{ \kappa T}\right) $, where $\kappa$ is a constant that depends on the structure of the graph, instead of the number of arms (nodes). This bounds significantly improves regular bandit algorithms, whose complexity depends on the number of arms (nodes). 

## Domain-Adaptive Text Classification with Structured Knowledge from Unlabeled Data

### TL;DR

None

### Abstract

Language models often require training with large-scale data, and they require additional labeled data in order to adapt to new domains. Previous works for domain adaptation typically do not leverage the implicit relationship between words across domains, which contain rich semantic information. In this paper, we propose a novel method, called Domain Adaptation with Structured Knowledge (DASK), that exploits this cross-domain relationship. DASK operates by first building a knowledge graph to capture the relationship between pivot terms (domain-independent words) and non-pivot terms in the target domain. During training, DASK injects pivot-related facts from the knowledge graph into source domain texts. For each downstream task, these knowledge-injected texts are then fed into a BERT variant capable of learning knowledge-injected textual data. Through knowledge injection, our model learns domain-invariant features for non-pivots according to their relationships with pivots. Like any pivot-based method, DASK requires the pivots to have domain-invariant behaviors. DASK ensures this by dynamically inferring via the polarity scores of candidate pivots during training with pseudo-labels. We extensively evaluate DASK on a wide range of cross-domain sentiment classification tasks and observe up to 2.9% performance improvement over baselines for 20 different domain pairs.

## On the Importance of Sampling in Training GCNs: Tighter Analysis and Variance Reduction

### TL;DR

None

### Abstract

Graph Convolutional Networks (GCNs) have achieved impressive empirical advancement across a wide variety of graph-related applications. Despite their great success, training GCNs on large graphs suffers from computational and memory issues. A potential path to circumvent these obstacles is sampling-based methods, where at each layer a subset of nodes is sampled. Although recent studies have empirically demonstrated the effectiveness of sampling-based methods, these works lack theoretical convergence guarantees under realistic settings and cannot fully leverage the information of evolving parameters during optimization. In this paper, we describe and analyze a general \textbf{\textit{doubly variance reduction}} schema that can accelerate any sampling method under the memory budget. The motivating impetus for the proposed schema is a careful analysis for the variance of sampling methods where it is shown that the induced variance can be decomposed into node embedding approximation variance (\emph{zeroth-order variance}) during forward propagation and layerwise-gradient variance (\emph{first-order variance}) during backward propagation. We theoretically analyze the convergence of the proposed schema and show that it enjoys an $\mathcal{O}(1/T)$ convergence rate.  We complement our theoretical results by integrating the proposed schema in different sampling methods and applying them to different large real-world graphs.

## Learning to Collaborate

### TL;DR

We propose a learn to collaborate framework for achieving a collaboration equilibrium in a collaborative network.

### Abstract

In this paper, we focus on effective learning over a collaborative research network involving multiple clients. Each client has its own sample population which may not be shared with other clients due to privacy concerns. The goal is to learn a model for each client, which behaves better than the one learned from its own data, through secure collaborations with other clients in the network. Due to the discrepancies of the sample distributions across different clients, it is not necessarily that collaborating with everyone will lead to the best local models. We propose a learning to collaborate framework, where each client can choose to collaborate with certain members in the network to achieve a ``collaboration equilibrium", where smaller collaboration coalitions are formed within the network so that each client can obtain the model with the best utility. We propose the concept of benefit graph which describes how each client can benefit from collaborating with other clients and develop a Pareto optimization approach to obtain it. Finally the collaboration coalitions can be derived from it based on graph operations. Our framework provides a new way of setting up collaborations in a research network. Experiments on both synthetic and real world data sets are provided to demonstrate the effectiveness of our method.

## Automated Self-Supervised Learning for Graphs

### TL;DR

We propose an automated framework to search self-supervised tasks in an unsupervised manner.

### Abstract

Graph self-supervised learning has gained increasing attention due to its capacity to learn expressive node representations. Many pretext tasks, or loss functions have been designed from distinct perspectives. However, we observe that different pretext tasks affect downstream tasks differently cross datasets, which suggests that searching pretext tasks is crucial for graph self-supervised learning.  Different from existing works focusing on designing single pretext tasks, this work aims to investigate how to automatically leverage multiple pretext tasks effectively. Nevertheless, evaluating representations derived from multiple pretext tasks without direct access to ground truth labels makes this problem challenging. To address this obstacle, we make use of a key principle of many real-world graphs, i.e., homophily, or the principle that ``like attracts like,'' as the guidance to effectively search various self-supervised pretext tasks. We provide theoretical understanding and empirical evidence to justify the flexibility of homophily in this  search task. Then we propose the AutoSSL framework which can automatically search over combinations of various self-supervised tasks. By evaluating the framework on 7 real-world datasets, our experimental results show that AutoSSL can significantly boost the performance on downstream tasks including node clustering and node classification compared with training under individual tasks. 

## Learning the Markov order of paths in graphs

### TL;DR

We found that the Bayesian model selection of Multi order network models is much better than the MLE techniques which were previously considered.

### Abstract

We address the problem of learning the Markov order in categorical sequences that represent paths in a network, i.e., sequences of variable lengths where transitions between states are constrained to a known graph. Such data pose challenges for standard Markov order detection methods and demand modeling techniques that explicitly account for the graph constraint. Adopting a multi-order modeling framework for paths, we develop a Bayesian learning technique that (i) detects the correct Markov order more reliably than a competing method based on the likelihood ratio test, (ii) requires considerably less data than methods using AIC or BIC, and (iii) is robust against partial knowledge of the underlying constraints. We further show that a recently published method that uses a likelihood ratio test exhibits a tendency to overfit the true Markov order of paths, which is not the case for our Bayesian technique. Our method is important for data scientists analyzing patterns in categorical sequence data that are subject to (partially) known constraints, e.g. sequences with forbidden words, mobility trajectories, click stream data, or sequence data in bioinformatics. Addressing the key challenge of model selection, our work is also relevant for the growing body of research that emphasizes the need for higher-order models in network analysis.

## Learning Multi-Level Consistency for Noisy Labels

### TL;DR

None

### Abstract

Recent methods performing well on Learning with Noisy Label (LNL) problem generally are based on consistency regularization. It usually consists of three stages: warm-up, noisy/clean data division, and semi-supervised learning. However, these methods trained purely with classification consistency suffer from the confirmation bias problem and tend to memorize the noisy labels, resulting in accumulated error and degraded performance. Leveraging the compositional and relational peculiarities of the noisy data, we propose a graph-based Multi-Level Consistency (MLC) framework that explicitly encodes the relations of samples within a batch at class distribution-level, embedding-level, and feature map-level respectively. We also design a pseudo label graph based on smoothness constraint with memory bank to impose consistency regularization on multi-level, which better pulls away false-labeled features from true-labeled features.  Moreover, we propose a Dynamic Filter Module (DFM) which effectively improves the reliability of divided data by re-filtering noisy data despite its simplicity. Our method achieves the state-of-the-art performance on multiple benchmark datasets. On Cifar-100 with 90\% noisy labels, MLC achieves a top-1 accuracy of 49.4\%, outperforming DivideMix by 17.9\%. Source code is available at \url{https://github.com/Zey97/MLC}.

## Overlapping Community Detection Based on Federated Multi-label Propagation

### TL;DR

We propose a federated graph learning model and and a federated multi-label propagation based on it for overlapping community detection.

### Abstract

Community detection aims to find out closely related vertex groups in a complex network, which has many practical applications such as customer recommendation, protein molecule discovery, and criminal tracking. As more and more people are concerned about the leakage of their sensitive information, detecting communities without disclosing personal privacy has become a hot topic in complex network analysis. The existing anonymization-based community detection methods have to modify network structure to protect sensitive vertices and links, which incur inevitable accuracy loss that may be too great to obtain meaningful communities. In this paper, we first propose a novel federated graph learning model (FGLM) for distributed privacy-preserving graph learning. Second, a federated multi-label propagation algorithm (FMLPA) based on the model is developed to detect overlapping communities on distributed networks with attributes. We develop a new label perturbation strategy to conceal vertex degrees in distributed label update and employ a homomorphic encryption system to protect the label information exchanged between participants. The experiments on real-world and artificial datasets demonstrate that the new algorithm achieves identical results to those of the standalone multi-label propagation algorithm and more than 200% higher accuracy than the simple distributed multi-label propagation method without federated learning.

## On Batch Size Selection for Stochastic Training for Graph Neural Networks

### TL;DR

None

### Abstract

Batch size is an important hyper-parameter for training deep learning models with the stochastic gradient descent (SGD) method, and it has a great influence on the training time and model performance. We study the batch size selection problem for training graph neural network (GNN) with SGD method. 
To reduce the training time while keeping a decent model performance, we provide theoretical analysis for the batch size selection based on both the variance of gradients and computation time for each mini-batch. 
In GNN, gradients evaluated on samples in a mini-batch are not independent and it is challenging to evaluate the exact variance of gradients. To address the dependency, we analyze an estimator for gradients that considers the randomness arising from two consecutive layers in GNN, and suggest a guideline for picking the appropriate scale of the batch size. 
We complement our theoretical results with extensive empirical experiments for ClusterGCN, FastGCN, and GraphSAINT on 4 datasets: Ogbn-products, Ogbn-arxiv, Reddit, and Pubmed. We demonstrate that in contrast to conventional deep learning models, GNNs benefit from large batch sizes.

## Forward-reverse Adaptive Graph Convolutional Network for Skeleton-Based Action Recognition

### TL;DR

By learning the forward and reverse deep information, our proposed method can significantly improve the recognition accuracy.

### Abstract

In the latest research based on skeleton data, the GCN-based methods have achieved excellent performance on action recognition tasks. Existing GCN-based methods often adopt the strategy of fusing the information flow of joints and bones to obtain better results. However, this multi-stream approach ignores the reversibility of the skeleton data in the temporal dimension. Reverse skeleton data, which has excellent discrimination and richer information for action recognition tasks, is rarely investigated in existing methods. In this work, we propose a novel forward-reverse adaptive graph convolutional network (FR-AGCN) for action recognition based on skeleton data. The information of joints and bones, as well as their reverse information, are modeled in a multi-stream network at the same time. By learning the forward and reverse deep information, this strategy can significantly improve the recognition accuracy. Extensive experiments on two large-scale datasets NTU-RGB+D and NTU-RGB+D 120 show that the performance of our model has exciting advantages.

## Federated Inference through Aligning Local Representations and Learning a Consensus Graph

### TL;DR

We study a distributed setting, where a datum is split across owners, and propose a local-global framework to perform inference.

### Abstract

Machine learning is faced with many data challenges when applied in practice. Among them, a notable barrier is that data are distributed and sharing is unrealistic for volume and privacy reasons. Federated learning is a recent formalism to tackle this challenge, so that data owners can develop a common model jointly but use it separately. In this work, we consider a less addressed scenario where a datum consists of multiple parts, each of which belongs to a separate owner. In this scenario, joint efforts are required not only in learning but also in inference. We formalize the concept of \emph{federated inference}, which allows each data owner to learn its own model that captures local data characteristics and copes with data heterogeneity. On the top is a federation of the local models, performing global inference that incorporates all distributed parts collectively. To settle this scenario, we propose aligning the ambiguous data representations caused by arbitrary arrangement of neurons in local neural network models, as well as learning a consensus graph among data owners in the global model to improve performance. We demonstrate effectiveness of the proposed framework on four real-life data sets from power grid systems and traffic networks.

## DIMIX: DIminishing MIXing for Sloppy Agents

### TL;DR

None

### Abstract

We study non-convex distributed optimization problems where a set of agents collaboratively solve a separable optimization problem that is distributed over a time-varying network. The existing methods to solve these problems rely on (at most) one time-scale algorithms, where each agent  performs a diminishing or constant step-size gradient descent at the average estimate of the agents in the network. However, if possible at all, exchanging exact information, that is required to evaluate these average estimates, potentially introduces a massive communication overhead. Therefore, a reasonable practical assumption to be made is that agents only receive a rough approximation of the neighboring agents' information. To address this, we introduce and study a \textit{two time-scale} decentralized algorithm with a broad class of \textit{lossy} information sharing methods (that includes noisy, quantized, and/or compressed information sharing) over \textit{time-varying} networks. In our method, one time-scale suppresses the (imperfect) incoming information from the neighboring agents, and one time-scale operates on local cost functions' gradients. We show that with a proper choices for the step-sizes' parameters, the algorithm achieves a convergence rate of $\mathcal{O}({T}^{-1/3 + \epsilon})$ for non-convex distributed optimization problems over time-varying networks, for any $\epsilon>0$. Our  simulation results support the theoretical results of the paper.

## Scalable Hierarchical Embeddings of Complex Networks

### TL;DR

A novel graph representation learning method relying on the Latent Space Model approach for large scale networks.

### Abstract

Graph representation learning has become important in order to understand and predict intrinsic structures in complex networks. A variety of embedding methods has in recent years been developed including the Latent Distance Modeling (LDM) approach directly optimizing the likelihood of the network adjacency matrix while accounting for degree heterogeneity and network characteristics such as transitivity and homophily. A major challenge is scaling network embedding approaches to very large networks and a drawback of LDM is the computational cost invoked evaluating the full likelihood of having O(N^2) complexity, making such analysis of large networks infeasible. We propose a novel multiscale hierarchical approximation of the full likelihood of LDMs providing high details where the likelihood approximation is most important while scaling in complexity at O(NlogN). The approach relies on a clustering procedure approximating the Euclidean norm of every node pair according to the multiscale hierarchical structure imposed. We demonstrate the accuracy of our approximation and for the first time embed very large networks in the order of a million nodes using LDM and contrast the predictive performance to prominent scalable graph embedding approaches. We find that our approach significantly outperforms these existing scalable approaches in the ability to predict links utilizing a surprisingly low embedding dimensionality of two to three dimensions whereas the extracted hierarchical structure facilitates network visualization and interpretation. The developed scalable hierarchical embedding approach enables accurate low dimensional representations of very large networks providing visualizations at multiple scales that can further our understanding of their properties and structure.

## Dynamic Causal Bayesian Optimization

### TL;DR

We develop a probabilistic framework to find a sequence of optimal interventions in a dynamic causal graph.

### Abstract

We study the problem of performing a sequence of optimal interventions in a dynamic causal system where both the target variable of interest, and the inputs, evolve over time. This problem arises in a variety of domains including healthcare, operational research and policy design. Our approach, which we call Dynamic Causal Bayesian Optimisation (DCBO), brings together ideas from decision making, causal inference and Gaussian process (GP) emulation. DCBO is useful in scenarios where the causal effects are changing over time. Indeed, at every time step, DCBO identifies a local optimal intervention by integrating both observational and past interventional data collected from the system. We give theoretical results detailing how one can transfer interventional information across time steps and define a dynamic causal GP model which can be used to find optimal interventions in practice. Finally, we demonstrate how DCBO identifies optimal interventions faster than competing approaches in multiple settings and applications.

## MOC-GAN: Mixing Objects and Captions to Generate Realistic Images

### TL;DR

None

### Abstract

Generating images with conditional descriptions gains increasing interests in recent years. However, existing conditional inputs are suffering from either unstructured forms (captions) or limited information and expensive labeling (scene graphs). For a targeted scene, the core items, objects, are usually definite while their interactions are flexible and hard to clearly define. Thus, we introduce a more rational setting, generating a realistic image from the objects and captions. Under this setting, objects explicitly define the critical roles in the targeted images and captions implicitly describe their rich attributes and connections. Correspondingly, a MOC-GAN is proposed to mix the inputs of two modalities to generate realistic images. It firstly infers the implicit relations between object pairs from the captions to build a hidden-state scene graph. So a multi-layer representation containing objects, relations and captions is constructed, where the scene graph provides the structures of the scene and the caption provides the image-level guidance. Then a cascaded attentive generative network is designed to coarse-to-fine generate phrase patch by paying attention to the most relevant words in the caption. In addition, a phrase-wise DAMSM is proposed to better supervise the fine-grained phrase patch consistency. On COCO dataset, our method outperforms the state-of-the-art methods on both Inception Score and FID while maintaining high visual quality. Extensive experiments demonstrate the unique features of our proposed method.

## A Unified Random Walk, Its Induced Laplacians and Spectral Convolutions for Deep Hypergraph Learning

### TL;DR

None

### Abstract

Deep hypergraph learning with hypergraph convolutional neural networks receives a surge of interest due to its great flexibility in modeling high-order interactions for irregular data. Many hypergraph convolutional models are straightforward generalizations of graph convolutional networks by incorporating simple hypergraph Laplacians, which lack expressiveness for encoding the hypergraph information or are short of spectral guarantees. In this work, we first propose a unified framework for hypergraph random walks and derive new random walk-based hypergraph Laplacians. The new Laplacians are more expressive than existing ones and admit regular spectral properties based on our theoretical analysis. As a fundamental property, we propose and prove the equivalency conditions between our hypergraph random walks and graph random walks, under which graphs can be viewed as low-order encoders of hypergraphs. Guided by the equivalency results, we propose a Generalized Hypergraph Convolutional Network (GHCN) for deep hypergraph learning. As a convolution-based model, GHCN inevitably suffers from the over-smoothing issue, which is also verified by our convergence analysis. To improve GHCN, we further present a Simple Hypergraph Spectral Convolution (SHSC) model, which could aggregate the long-range information via a constructed discounted Markov diffusion process. Extensive experiments from various domains including social network analysis, visual objective classification, and protein fold classification demonstrate that the proposed approaches outperform state-of-the-art spectral methods with a large margin.

## Discrete-Valued Neural Communication

### TL;DR

Discretizing communication among components of a structured architecture improves systematic generalization, both in theory and practice.

### Abstract

Deep learning has advanced from fully connected architectures to structured models organized into components, e.g., the transformer composed of positional elements, modular architectures divided into slots, and graph neural nets made up of nodes. In structured models, an interesting question is how to conduct dynamic and possibly sparse communication among the separate components. Here, we explore the hypothesis that restricting the transmitted information among components to discrete representations is a beneficial bottleneck. The motivating intuition is human language in which communication occurs through discrete symbols. Even though individuals have different understandings of what a ``"cat" is based on their specific experiences, the shared discrete token makes it possible for communication among individuals to be unimpeded by individual differences in internal representation.  To discretize the values of concepts dynamically communicated among specialist components, we extend the quantization mechanism from the Vector-Quantized Variational Autoencoder to multi-headed discretization with shared codebooks and use it for discrete-valued neural communication (DVNC). Our experiments show that DVNC substantially improves systematic generalization in a variety of architectures---transformers, modular architectures, and graph neural networks. We also show that the DVNC is robust to the choice of hyperparameters, making the method very useful in practice. Moreover, we establish a theoretical justification of our discretization process, proving that it has the ability to increase noise robustness and reduce the underlying dimensionality of the model.

## Circular Social Dilemmas with Reinforcement Learning

### TL;DR

We deal with Reinforcement Learning policies in social dilemmas, we study a more generic case with more than 2 players with a possible circular structure of cooperation 

### Abstract

In many societal and industrial interactions, participants generally prefer their pure self-interest at the expense of the global welfare. Known as social dilemmas, this category of non-cooperative games offers situations where multiple actors should all cooperate to achieve the best outcome but greed and fear lead to a worst self-interested issue. Recently, the emergence of Deep Reinforcement Learning (RL) has generated revived interest in social dilemmas with the introduction of Sequential Social Dilemmas (SSD). Cooperative agents mixing RL policies and Tit-for-tat (TFT) strategies have successfully addressed some non-optimal Nash equilibrium issues. However, this kind of paradigm requires symmetrical and direct cooperation between actors, conditions that are not met when mutual cooperation is possible only with at least a third actor in a circular way. To tackle this issue, this paper extends SSD with Circular Sequential Social Dilemmas (CSSD), a new kind of Markov games that better generalizes the diversity of cooperation between agents. Secondly, to address such circular and asymmetric cooperation, we propose a candidate solution based on RL policies and a graph-based TFT. We conducted some experiments on a simple multi-player grid world which offers adaptable cooperation structures. Our work confirmed that our graph-based approach is beneficial to address circular situations by encouraging self-interested agents to reach mutual cooperation.

## Inferring Granger Causality from Irregularly Sampled Time Series

### TL;DR

We leverage a recently developed stochastic variational inequality-based approach to infer the non-linear Granger causal graph over sepsis-associated derangements from both continuous- and discrete-valued data with different sampling frequency.

### Abstract

Continuous, automated surveillance systems that incorporate machine learning models are becoming increasingly more common in healthcare environments. These models can capture temporally dependent changes across multiple patient variables and can enhance a clinician's situational awareness by providing an early warning alarm of an impending adverse event such as sepsis. However, most commonly used methods, e.g., XGBoost, fail to provide an interpretable mechanism for understanding why a model produced a sepsis alarm at a given time. The ``black box'' nature of many models is a severe limitation as it prevents clinicians from independently corroborating those physiologic features that have contributed to the sepsis alarm. To overcome this limitation, we propose a generalized linear model (GLM) approach to fit a Granger causal graph based on the physiology of several major sepsis-associated derangements (SADs). We adopt a recently developed stochastic monotone variational inequality-based estimator coupled with forwarding feature selection to learn the graph structure from both continuous and discrete-valued as well as regularly and irregularly sampled time series. Most importantly, we develop a non-asymptotic upper bound on the estimation error for any monotone link function in the GLM. We conduct real-data experiments and demonstrate that our proposed method can achieve comparable performance to popular and powerful prediction methods such as XGBoost while simultaneously maintaining a high level of interpretability.

## Independent Prototype Propagation for Zero-Shot Compositionality

### TL;DR

Compositional zero-shot learning by propagating conditionally independent prototypical attribute and object representations through a knowledge graph to learn compositional prototypes that can classify novel compositional classes.

### Abstract

Humans are good at compositional zero-shot reasoning; someone who has never seen a zebra before could nevertheless recognize one when we tell them it looks like a horse with black and white stripes. Machine learning systems, on the other hand, usually leverage spurious correlations in the training data, and while such correlations can help recognize objects in context, they hurt generalization. To be able to deal with underspecified datasets while still leveraging contextual clues during classification, we propose ProtoProp, a novel prototype propagation graph method. First we learn prototypical representations of objects (e.g., zebra) that are conditionally independent w.r.t. their attribute labels (e.g., stripes) and vice versa. Next we propagate the independent prototypes through a compositional graph, to learn compositional prototypes of novel attribute-object combinations that reflect the dependencies of the target distribution. The method does not rely on any external data, such as class hierarchy graphs or pretrained word embeddings. We evaluate our approach on AO-Clever, a synthetic and strongly visual dataset with clean labels, and UT-Zappos, a noisy real-world dataset of fine-grained shoe types. We show that in the generalized compositional zero-shot setting we outperform state-of-the-art results, and through ablations we show the importance of each part of the method and their contribution to the final results.

## Domain Knowledge-based Automated Analog Circuit Design with Deep Reinforcement Learning

### TL;DR

This paper presents a deep reinforcement learning (RL) framework with domain-knowledge to assist the design of analog circuits, achieving state-of-the-art design accuracy and efficiency.

### Abstract

The automated design of analog circuits is a longstanding challenge in the integrated circuit field. 
This paper presents a deep reinforcement learning (RL) framework to assist the design of analog circuits at the pre-layout level, where the goal is to find device parameters to fulfill desired specifications.
Unlike all prior methods, our approach incorporates important domain knowledge that determines the relations between device parameters and circuit specifications,
achieving state-of-the-art design accuracy and efficiency.
It is applicable for designing various analog circuits (e.g., radio-frequency circuits) with different semiconductor technologies, breaking the limitations of prior arts in designing a narrow scope of low-frequency analog circuits with conventional semiconductor technology.
To enable such abilities, we tailor a policy network for our RL agent by properly combining a circuit-topology-based graph neural network (GNN) and a fully connected neural network (FCNN).
The proposed policy network can effectively capture the common physical features (e.g., device's parameters and interactions) present in a circuit graph with the GNN and extract the couplings (e.g., design trade-offs) of specifications with the FCNN, thereby best modeling the relations between circuit parameters and design targets.
Experimental results show that our method achieves 98$\%$ accuracy for the design of exemplary circuits with 1.5$\times$ efficiency of existing best-performing methods.
Our method also demonstrates superior generalization and transferability.

## Time-series Imputation of Temporally-occluded Multiagent Trajectories

### TL;DR

None

### Abstract

In multiagent environments, several decision-making individuals interact while adhering to the dynamics constraints imposed by the environment. These interactions, combined with the potential stochasticity of the agents' decision-making processes, make such systems complex and interesting to study from a dynamical perspective. Significant research has been conducted on learning models for forward-direction estimation of agent behaviors, for example, pedestrian predictions used for collision-avoidance in self-driving cars. However, in many settings, only sporadic observations of agents may be available in a given trajectory sequence. For instance, in football, subsets of players may come in and out of view of broadcast video footage, while unobserved players continue to interact off-screen. In this paper, we study the problem of multiagent time-series imputation, where available past and future observations of subsets of agents are used to estimate missing observations for other agents. Our approach, called the Graph Imputer, uses forward- and backward-information in combination with graph networks and variational autoencoders to enable learning of a distribution of imputed trajectories. We evaluate our approach on a dataset of football matches, using a projective camera module to train and evaluate our model for the off-screen player state estimation setting. We illustrate that our method outperforms several state-of-the-art approaches, including those hand-crafted for football.

## A Hypergraph Convolutional Neural Network for Molecular Properties Prediction using Functional Groups

### TL;DR

None

### Abstract

We propose a Molecular Hypergraph Convolutional Network (MolHGCN) that predicts the molecular properties of a molecule using the atom and functional group information as inputs. Molecules can contain many types of functional groups, which will affect the properties the molecules. For example, the toxicity of a molecule is associated with toxicophores, such as nitroaromatic groups and thiourea.  Conventional graph-based methods that consider the pair-wise interactions between nodes are inefficient in expressing the complex relationship between multiple nodes in a graph flexibly, and applying multi-hops may result in oversmoothing and overfitting problems. Hence, we propose MolHGCN to capture the substructural difference between molecules using the atom and functional group information. MolHGCN constructs a hypergraph representation of the molecule using functional group information in the input SMILES strings, extracts hidden representation using a two-stage message passing process (atom and functional group message passing), and predicts the properties of the molecules using the extracted hidden representation. We evaluate the performance of our model using Tox21, ClinTox, SIDER, BBBP, BACE, ESOL, FreeSolv and Lipophilicity datasets. We show that our model is able to outperform other baseline methods for most of the datasets. We particularly show that incorporating functional group information along with node information results in better separability in the latent space, thus increasing the prediction accuracy of the molecule property prediction. 

## SHINE: Subgraph inference on Hypergraph Inductive Neural nEtwork

### TL;DR

None

### Abstract

Hypergraph neural networks have recently emerged as a series of successful methods to model multi-way connections that are beyond pairwise associations among nodes of the graphs. Multi-way connections are common in many real-world applications and in particular, biomedical applications. For example, genetic pathways or broadly speaking gene sets encode relationship among multiple genes that collectively correspond to a molecular function, which account for the mechanisms of pathogenesis more intuitively and accurately than individual genes. These complex relationships can be modeled using hyperedge connecting all involved nodes (e.g., genes). Existing hypergraph neural network models often focus on node-level or graph-level inference. There is unmet need in learning powerful representations of subgraphs in hypergraphs. Such capabilities are important in real-world applications. For example, in genetic medicine, subjects can be viewed as subgraphs of genes that harbor mutations, while the genes are connected by hyperedges that correspond to pathways or gene sets representing specific molecular functions. Addressing this unmet need, we propose Subgraph inference on Hypergraph Inductive Neural nEtwork (SHINE) for accurate inductive subgraph prediction. SHINE uses informative genetic pathways that directly encode molecular functions as hyperedges to connect genes as hypernodes. SHINE jointly optimizes the objectives of end-to-end subgraph classification and hypernodes' similarity regularization. SHINE simultaneously learns representations for both genes and pathways using strongly dual attention message passing. These learned representations are then aggregated via a subgraph attention layer to derive subgraph-level representation, which is used in training a multilayer perceptron for subgraph inferencing. We evaluated SHINE against multiple state-of-the-art hypergraph neural network models, using large scale NGS and curated datasets. SHINE outperforms a wide array of state-of-the-art models significantly.

## Finding Bipartite Components in Hypergraphs

### TL;DR

In this work we study a new heat diffusion process in hypergraphs, and design a polynomial-time algorithm that approximately finds bipartite components in a hypergraph.

### Abstract

Hypergraphs are important objects to model ternary or higher-order relations of objects, and have a number of applications in analysing many complex datasets occurring in practice. In this work we study a new heat diffusion process in hypergraphs, and employ this process to design a polynomial-time algorithm that approximately finds bipartite components in a hypergraph.  We theoretically prove the performance of our proposed algorithm, and compare it against the previous state-of-the-art through extensive experimental analysis.  The significance of our work is further demonstrated on several large-scale datasets (Penn Treebank, DBLP, IMDB, and Wikipedia), in which our unsupervised algorithm clearly separates objects of different types (e.g., verbs vs. adverbs,  and  authors vs. conferences).

## Beyond the Dot-product: Calculating High-order Self-attention in Transformers

### TL;DR

We proposed a novel high-order self-attention to enhance Transformers.

### Abstract

The recent success of Transformer has benefited many real-world applications, with its capability of building long dependency through pairwise dot-products. However, the strong assumption that elements are directly attentive to each other limits the performance on tasks with high-order dependencies such as natural language understanding and Image captioning. To solve such problems, we are the first to define the High-order Self-Attention (HSA) to build Transformers. Inspired by the pieces moving of English Draughts, we introduce the spectral convolutional technique to calculate HSA on the dot-product feature map. It allows HSA's propagation in each self-attention head and is interchangeable with the canonical self-attention. We further develop the higher-order variants under multi-hop assumption to increase the generality. Moreover, the proposed architecture is compatible with the pre-trained models. With extensive experiments, we empirically show that our methods significantly increase the performance on ten different tasks.

## Planning from Pixels in Environments with Combinatorially Hard Search Spaces

### TL;DR

We combine powerful graph planners with a learned world model to solve environments with a hidden combinatorial nature.

### Abstract

The ability to form complex plans based on raw visual input is a litmus test for current capabilities of artificial intelligence, as it requires a seamless combination of visual processing and abstract algorithmic execution, two traditionally separate areas of computer science. A recent surge of interest in this field brought advances that yield good performance in tasks ranging from arcade games to continuous control; these methods however do not come without significant issues, such as limited generalization capabilities and difficulties when dealing with combinatorially hard planning instances. Our contribution is two-fold: (i) we present a method that learns to represent its environment as a latent graph and leverages state reidentification to reduce the complexity of finding a good policy from exponential to linear (ii) we introduce a set of lightweight environments with an underlying discrete combinatorial structure in which planning is challenging even for humans. Moreover, we show that our methods achieves strong empirical generalization to variations in the environment, even across highly disadvantaged regimes, such as “one-shot” planning, or in an offline RL paradigm which only provides low-quality trajectories.

## On the complexity of the optimal transport problem with graph-structured cost

### TL;DR

None

### Abstract

Multi-marginal optimal transport (MOT) is a generalization of optimal transport to multiple marginals. Optimal transport has evolved into an important tool in many machine learning applications, and its multi-marginal extension opens up for addressing new challenges in the machine learning field. However, the usage of MOT has been largely impeded by its computational complexity which scales exponentially in the number of marginals. Fortunately, in many applications, such as barycenter or interpolation problems, the cost function adheres structures, which has recently been exploited for developing efficient computational methods. In this work, we derive computational bounds for these methods. With $m$ marginal distributions supported on $n$ points, we provide a $ \mathcal{\tilde O}(d(G)m n^2\epsilon^{-2})$ bound for a $\epsilon$-accuracy when the problem is associated with a tree with diameter $d(G)$. For the special case of the Wasserstein barycenter problem, which corresponds to a star-shaped tree, our bound is in alignment with the existing complexity bound for it.

## Reducing Collision Checking for Sampling-Based Motion Planning Using Graph Neural Networks

### TL;DR

A GNN approach learns to reduce collision checking steps and smooth paths for motion planning.

### Abstract

Sampling-based motion planning is a popular approach in robotics for finding paths in continuous configuration spaces. Checking collision with obstacles is the major computational bottleneck in this process. We propose new learning-based methods for reducing collision checking to accelerate motion planning by training graph neural networks (GNNs) that perform path exploration and path smoothing. Given random geometric graphs (RGGs) generated from batch sampling, the path exploration component iteratively predicts collision-free edges to prioritize their exploration. The path smoothing component then optimizes paths obtained from the exploration stage. The methods benefit from the ability of GNNs of capturing geometric patterns from RGGs through batch sampling and generalize better to unseen environments. Experimental results show that the learned components can significantly reduce collision checking and improve overall planning efficiency in challenging high-dimensional motion planning tasks.

## Heterogeneous Knowledge Graph Entity Alignment by Aggregating Extensive Structure and Specific Semantics

### TL;DR

None

### Abstract

Entity alignment aims to link entities from different knowledge graphs (KGs) that refer to the same real-world identity. Recently, embedding-based entity alignment approaches that primarily center on topological structure get close attention in this field. Even achieved promising performance, these approaches overlook the vital impact of entity-specific semantics on entity alignment task. In this paper, we propose a novel framework SSEA (Extensive Structure and Specific Semantics for Entity Alignment), which jointly employs extensive structure and specific semantics to boost the performance of entity alignment. This framework embeds the entities in two cross-lingual knowledge graphs into a unified low dimensional vector space and employs graph convolution networks (GCNs) to learn the embeddings of entities and relations. Meanwhile, it utilizes entity-specific attribute category differences learned from entities' attributes to filter incorrect target entities for each source entity to be linked. We verify our proposed approach on three benchmark datasets from real-world KGs, and experimental results demonstrate that our approach has superior performance to other state-of-art KG alignment approaches.

## Solving Graph-based Public Goods Games with Tree Search and Imitation Learning

### TL;DR

We devise a method for efficiently solving a class of graph-based public goods games using tree search and imitation learning.

### Abstract

Public goods games represent insightful settings for studying incentives for individual agents to make contributions that, while costly for each of them, benefit the wider society. In this work, we adopt the perspective of a central planner with a global view of a network of self-interested agents and the goal of maximizing some desired property in the context of a best-shot public goods game. Existing algorithms for this known NP-complete problem find solutions that are sub-optimal and cannot optimize for criteria other than social welfare.

In order to efficiently solve public goods games, our proposed method directly exploits the correspondence between equilibria and the Maximal Independent Set (mIS) structural property of graphs. In particular, we define a Markov Decision Process, which incrementally generates an mIS, and adopt a planning method to search for equilibria, outperforming existing methods. Furthermore, we devise an imitation learning technique that uses demonstrations of the search to obtain a graph neural network parametrized policy which quickly generalizes to unseen game instances. Our evaluation results show that this policy is able to reach 99.5% of the performance of the planning method while being approximately three orders of magnitude faster to evaluate on the largest graphs tested. The methods presented in this work can be applied to a large class of public goods games of potentially high societal impact.

## GenURL: A General Framework for Unsupervised Representation Learning

### TL;DR

We proposed a general framework for unsupervised representation learning, which can adapt to various scenaios, such as self-supervised visual representation, knowledge distillation and graph embedding, and achieves the state-of-the-art performace.

### Abstract

Unsupervised representation learning has recently attracted the attention of the community. To learn intrinsic low-dimensional structures in various high-dimensional data, we propose a general similarity based framework, called GenURL, which preserves essential structures of the input space. Combining with a specific pretext task, we can adapt GenURL to various scenarios in a unified manner and achieve the state-of-the-art performance, including self-supervised visual representation learning, unsupervised knowledge distillation, graph embeddings, etc. Moreover, the ablation studies reflect the relation between the data characters and the hyper-parameter settings in GenURL.

## Vector-valued Distance and Gyrocalculus on the Space of Symmetric Positive Definite Matrices

### TL;DR

We propose a framework to compute vector-valued distances and adapt Euclidean operations into the SPD manifold

### Abstract

We propose the use of the vector-valued distance to compute distances and extract geometric information from the manifold of symmetric positive definite matrices (SPD), and develop gyrovector calculus, constructing analogs of vector space operations in this curved space. We implement these operations and showcase their versatility in the tasks of knowledge graph completion, item recommendation, and question answering. In experiments, the SPD models outperform their equivalents in Euclidean and hyperbolic space. The vector-valued distance allows us to visualize embeddings, showing that the models learn to disentangle representations of positive samples from negative ones.

## Single-neuron convexification for binarized neural networks

### TL;DR

We provide ideal formulations to model activation functions of binarized neural networks, which can be used to verify the robustness of such neural networks to adversarial attacks.

### Abstract

Binarized neural networks are an important class  of neural network in deep learning due to their computational efficiency. This paper contributes towards a better understanding of the structure of binarized neural networks, specifically, ideal convex representations of the activation functions used. We describe the convex hull of the graph of the signum activation function associated with a single neuron, deriving closed forms for the convex and concave envelopes that improve upon those used in the literature. The new formulations lead to improved methods to verify the robustness of a binarized neural network via convex optimization. 

## Do Large Scale Molecular Language Representations Capture Important Structural Information?

### TL;DR

We investigate the representational power of large-scale pre-trained molecular language model embeddings toward understanding structure-property relationship in molecules. 

### Abstract

Predicting chemical  properties from the structure of a molecule is of great importance in many applications including drug discovery and material design. Machine learning based molecular property prediction holds the promise of enabling accurate predictions at much less complexity, when compared to, for example Density Functional Theory (DFT) calculations. Features extracted from molecular graphs,   using graph neural nets in a supervised manner, have emerged as strong baselines for such  tasks. However, the vast chemical space together with the  limited availability of labels makes supervised learning  challenging, calling for learning a general-purpose molecular representation. Recently,   pre-trained   transformer-based language models (PTLMs) on large unlabeled corpus have produced state-of-the-art results in many  downstream natural language processing  tasks. Inspired by this development, here we present molecular embeddings obtained by training  an efficient transformer encoder model, referred to as MoLFormer. This model was employed  with a linear attention mechanism and highly paralleized training on 1D SMILES sequences of 1.1 billion unlabeled molecules from the PubChem and ZINC datasets. Experiments show that the learned molecular representation  performs competitively, when compared   to existing graph-based and fingerprint-based supervised learning baselines,  on the   challenging  tasks of   predicting properties of QM8 and QM9 molecules.  Further task-specific fine-tuning of the MoLFormerr representation improves performance on several of those property prediction benchmarks. These results provide encouraging evidence that large-scale molecular language models can capture sufficient structural information to be able to accurately predict quantum chemical properties and beyond.  

## Uncover Causality from Heterogeneous Datasets via Meta DAG Structural Learning

### TL;DR

Meta Causal Discovery from Heterogeneous Datasets

### Abstract

This paper considers a scenario where a common causal structure is behind multiple datasets obtained via different stimulation, interventions, and the ways of collection. We aim to address the challenge of uncovering causality from the multiple heterogeneous datasets with different data distributions and nonoverlapping variables. 
Existing causal discovery approaches can recover a directed acyclic graph (DAG) from independent and identically distributed (i.i.d.) observational samples to capture linear or non-linear causal relations among random variables. However, in practice, samples are often collected in different sites that may vary in calibration parameters, sensing frequencies, quantity and types of sensors, locations/environments, etc., and the sets of variables across sites may not fully overlap. Using only the overlapping variables, or making up missing values, is often the default option but with perceivable limitations. We propose a meta structural learning framework that uncovers causality from a set of heterogeneous datasets. The proposed method uses a meta-model to learn the universal causal relations and learns a series of sub-models to adapt the heterogeneity and capture the causality information of individual datasets. Unlike the typical meta-learning, to avoid interference among the heterogeneous datasets, we purposely decouple the causal graph from the rest of the meta model while performing the subset-specific learning. Experiments on both synthetic and real heterogeneous datasets demonstrate clear advantages over existing causal discovery approaches and open new opportunities in future research and real-world applications. 




## Differentially Private Nash Equilibrium Computation

### TL;DR

 We propose distributed algorithms to compute Nash equilibrium for linear quadratic network games with differential privacy guarantee.

### Abstract

In this paper, we develop  distributed computation  algorithms  for  Nash equilibriums of linear quadratic network games with proven differential privacy guarantee.  In a network game with the payoff of each player being a quadratic function,  the cross terms of the  decisions in the payoff function naturally encodes a network structure governing  the players' inter-personal influences. Such social influence structure and the individual marginal payoffs of the players indicate economic dependencies and  preferences, and thus they are subject to privacy concerns. For distributed computing of the Nash equilibrium, the players are interconnected by a public communication graph, over which  dynamical states are shared among neighboring nodes. When the influence network structure is considered to be public knowledge, we propose a distributed randomized gradient descent algorithm, in which each player adds a Laplacian random noise to her marginal payoff in the recursive updates.  It is proven that the algorithm can guarantee differential privacy and convergence in expectation to the  Nash equilibrium  of the network game at each player's state.  Moreover, the mean-square error between  the players' states and the   Nash equilibrium  is shown to be bounded by a constant related to the differential privacy level. Next, when both the players' marginal payoffs and the influence graph are private data, we propose two distributed algorithms by randomized communication and randomized projection, respectively, for privacy preservation. The  differential privacy and convergence guarantees    are also established for such algorithms. Numerical experiments  are provided for validation of the theoretical results.

## Hierarchical Clustering: $O(1)$-Approximation for Well-Clustered Graphs

### TL;DR

We present $O(1)$-approximation algorithms for hierarchical clustering when the input graph exhibits a clears structure of clusters.

### Abstract

Hierarchical clustering  studies a recursive partition of a dataset into clusters of successively smaller size, and is a fundamental problem in data analysis. In this work we study the cost function for hierarchical clustering introduced by Dasgupta, and present two polynomial-time approximation algorithms: Our first result is an $O(1)$-approximation algorithm for graphs of high conductance. Our simple construction bypasses complicated recursive routines of finding sparse cuts known in the literature. Our second and main result is an $O(1)$-approximation algorithm for a wide family of graphs that exhibit a well-defined structure of clusters. This result generalises the previous state-of-the-art, which holds only for graphs generated from stochastic models. The significance of our work is demonstrated by the empirical analysis on both synthetic and real-world datasets, on which our presented algorithm outperforms the previous state-of-the-art and several  classical linkage heuristics.  

## Influence Patterns for Explaining Information Flow in BERT

### TL;DR

This paper introduces a new technique for explaining the information flow in BERT's computational graph using gradient-based method. 

### Abstract

While attention is all you need may be proving true, we do not know why: attention-based transformer models such as BERT are superior but how information flows from input tokens to output predictions are unclear.  We introduce influence patterns,  abstractions  of  sets  of  paths  through  a  transformer model.  Patterns quantify  and localize the flow of  information to paths passing through a sequence of model nodes. Experimentally, we find that significant portion of information flow in BERT goes through skip connections instead of attention heads. We further show that consistency of patterns across instances is an indicator of BERT’s performance. Finally, We demonstrate that patterns account for far more model performance than previous attention-based and layer-based methods.

## Transformers à Grande Vitesse

### TL;DR

A novel technique to predict how delays propagate between trains based on their timed positions in the network's graph

### Abstract

Robust travel time predictions are of prime importance in managing any transportation infrastructure, and particularly in rail networks where they have major impacts both on traffic regulation and passenger satisfaction. We aim at predicting the travel time of trains on rail sections at the scale of an entire rail network in real-time, by estimating trains' delays relative to a theoretical circulation plan. Existing implementations within railway companies generally work using the approximation that a train's delay will stay constant for the rest of its trip.

Predicting the evolution of a given train's delay is a uniquely hard problem, distinct from mainstream road traffic forecasting problems, since it involves several hard-to-model phenomena: train spacing, station congestion and heterogeneous rolling stock among others. We first offer empirical evidence of the previously unexplored phenomenon of delay propagation in the French National Railway Network, leading to delays being amplified by interactions between trains. We then contribute a novel technique using the transformer architecture and pre-trained embeddings to make real-time massively parallel predictions for train delays at the scale of the whole rail network (over 3k trains at peak hours, making predictions at an average horizon of 70 minutes). Our approach yields very positive results on real-world data when compared to currently-used and experimental prediction techniques. Our work is in the early stages of implementation for industrial use at the French railway company SNCF for passenger information systems, and a contender as a tool to aid traffic regulation decisions. 

## Mod Shift: A Principled Alternative to Mean Shift Clustering Using Long Range Repulsion

### TL;DR

Introduces a new clustering algorithm, Mod Shift, which relates to both Mean Shift and Multicut.

### Abstract

Motivated by the idea of making the Multicut problem differentiable, we derive a novel clustering algorithm, Mod Shift, and relate it to Mean Shift, thus establishing a new connection between graph partitioning and clustering in Euclidean space. Mod Shift generalizes Mean Shift in that it uses both attractive and repulsive forces to iteratively shift points in Euclidean space. The number of clusters emerges as a result of the optimization process and does not have to be specified by the user. Experiments show that Mod Shift performs comparably to Mean Shift in a challenging neural segmentation task.

## E(n) Equivariant Normalizing Flows

### TL;DR

None

### Abstract

This paper introduces a generative model equivariant to Euclidean symmetries: E(n) Equivariant Normalizing Flows (E-NFs). To construct E-NFs, we take the discriminative E(n) graph neural networks and integrate them as a differential equation to obtain an invertible equivariant function: a continuous-time normalizing flow. We demonstrate that E-NFs considerably outperform baselines and existing methods from the literature on particle systems such as DW4 and LJ13, and on molecules from QM9 in terms of log-likelihood. To the best of our knowledge, this is the first flow that jointly generates molecule features and positions in 3D.

## Flatten & Ascend: A New Approach To Find Low Rank Optima of Semidefinite Programs

### TL;DR

We propose a new Burer-Monteiro algorithm to find global optima of rank-constrained semidefinite programs, i.e., non-convex optimization, using local optimization, and introduced new lifting bounds using structure and not the size of the input graph.

### Abstract

We propose a paradigm for finding the global optima of rank-constrained semidefinite programs, that are non-convex optimization problems and intractable in general. 
Using Burer-Monteiro factorization technique $\boldsymbol{\mathit{X}} = \boldsymbol{\mathit{Y}} \boldsymbol{\mathit{Y}}^T \in \mathbb{S}_+^n$, one can simply bound the rank of feasible solutions by the number of columns of $\boldsymbol{\mathit{Y}}$, improve scalability, dismiss conic optimization, and use local optimization methods for the subsequent quadratic program.
Despite significant performance of this technique in practice, state-of-the-art algorithms require increasing the dimension, i.e., the number of columsn of $\boldsymbol{\mathit{Y}}$, up to the Barvinok-Pataki bound for generic SDP, and in the worst case up to $O(n)$, to find the global optima. 

Our paradigm guarantees global optimization by local optimization in low dimensions, for any SDP, using constructive proofs of \emph{existence} of low rank optima. Moreover, we show the structure of input can be utilized to accomplish non-convex optimization in even lower dimensions. Specifically we show the tree-width of the input graph (or a corresponding sparcity pattern defined by the semidefinite program), can be an upper bound on the required dimension for Burer-Monteiro optimization, for some well-studied classes of semidefinite programs with applications in combinatorial optimization, clustering, dimension reduction, spectral graph theory, Markov chains, community detection, and synchronization.
Finally, we provide new polytime algorithms and computational complexity results that shed further light on the non-convex optimization landscapes under study, accompanied by experimental results that support our theory.

## Finite-Time Error Bounds for Distributed Linear Stochastic Approximation

### TL;DR

None

### Abstract

This paper considers a novel multi-agent linear stochastic approximation algorithm driven by Markovian noise and general consensus-type interaction, in which each agent evolves according to its local stochastic approximation process which depends on the information from its neighbors. The interconnection structure among the agents is described by a time-varying directed graph. While the convergence of consensus-based stochastic approximation algorithms when the interconnection among the agents is described by doubly stochastic matrices (at least in expectation) has been studied, less is known about the case when the interconnection matrix is simply stochastic. For any uniformly strongly connected graph sequences whose associated interaction matrices are stochastic, the paper derives finite-time bounds on the mean-square error, defined as the deviation of the output of the algorithm from the unique equilibrium point of the associated ordinary differential equation. For the case of interconnection matrices being stochastic, the equilibrium point can be any unspecified convex combination of the local equilibria of all the agents in the absence of communication. Both the cases with constant and time-varying step-sizes are considered. In the case when the convex combination is required to be a straight average and interaction between any pair of neighboring agents may be uni-directional, so that doubly stochastic matrices cannot be implemented in a distributed manner, the paper proposes a push-type distributed stochastic approximation algorithm and provides its finite-time bounds for the performance by leveraging the analysis for the consensus-type algorithm with stochastic matrices.

## Attentional Composition Networks for Long-Tailed Human Action Recognition

### TL;DR

We proposed the attention-based compositional learning to decompose and compose action videos for the long-tailed, few-shot, and zero-shot human action recognition.

### Abstract

The problem of long-tailed visual recognition has been receiving increasing research attention. Despite the pervasive existence in real-world applications, the long-tailed distribution problem has rarely been explored in video-based visual recognition. Thus inspired, in this paper we apply compositional learning to alleviate the long-tailed distribution problem in video-based human action recognition. Intuitively, our method, named attentional composition networks (ACN), learns verb-like and preposition-like components, and then shuffles these components to generate samples for the tail classes in the feature space to alleviate the data insufficiency issue. Specifically, given an input video of an action, it is first represented by a graph that captures the spatial-temporal relations (edges) among detected human/object instances (nodes). Then, ACN utilizes the position information to decompose each action into a set of verb and preposition representations using the edge features in the graph. The verb and preposition features are then integrated via an attention structure in order to explore the same semantics in different actions, where the same verb/preposition may have different meanings. This way, we can enrich the data of tail classes, and consequently improve the action recognition for these classes. To evaluate the compositional human action recognition, we further contribute a new human action recognition dataset, namely ISE-Interaction. Experimental results on both Something-Something V2 and the proposed ISE-Interaction demonstrate the effectiveness of the proposed method for long-tailed, few-shot, and zero-shot problems in human action recognition.

## Trainability for Universal GNNs through Surgical Randomness

### TL;DR

Using the individualization-refinement graph-isomorphism technique we develop the first practical, easily-trainable, and universal class of GNN models.

### Abstract

Message passing neural networks (MPNN) have provable limitations, which can be overcome by universal networks. However, universal networks are typically impractical. The only exception is random node initialization (RNI), a data augmentation method that results in provably universal networks. Unfortunately, RNI suffers from severe drawbacks such as slow convergence and high sensitivity to changes in hyperparameters. We transfer powerful techniques from the practical world of graph isomorphism testing to MPNNs, resolving these drawbacks. This culminates in individualization-refinement node initialization (IRNI). We replace the indiscriminate and haphazard randomness used in RNI by a surgical incision of only a few random bits at well-selected nodes. Our novel non-intrusive data-augmentation scheme maintains the networks' universality while resolving the trainability issues. We formally prove the claimed universality and corroborate experimentally---on synthetic benchmarks sets previously explicitly designed for that purpose---that IRNI overcomes the limitations of MPNNs. We also verify the practical efficacy of our approach on the standard benchmark data sets PROTEINS and NCI1.

## Representation Learning on Spatial Networks

### TL;DR

We propose a new message passing neural network model with theoretical guarantee to handle the spatial-graph topology coupled network.

### Abstract

Spatial networks are networks for which the nodes and edges are constrained by geometry and embedded in real space, which has crucial effects on their topological properties. Although tremendous success has been achieved in spatial and network representation separately in recent years, there exist very little works on the representation of spatial networks. Extracting powerful representations from spatial networks requires the development of appropriate tools to uncover the pairing of both spatial and network information in the appearance of node permutation invariant, and rotation and translation invariant. Hence it can not be modeled merely with either spatial or network models individually. To address these challenges, this paper proposes a generic framework for spatial network representation learning. Specifically, a provably information-lossless and roto-translation invariant representation of spatial information on networks is presented. Then a higher-order spatial network convolution operation that adapts to our proposed representation is introduced. To ensure efficiency, we also propose a new approach that relied on sampling random spanning trees to reduce the time and memory complexity from $O(N^3)$ to $O(N)$. We demonstrate the strength of our proposed framework through extensive experiments on both synthetic and real-world datasets.

## Distilling Self-Knowledge From Contrastive Links to Classify Graph Nodes Without Passing Messages

### TL;DR

We contrastively distill structural self-knowledge into an MLP to tackle with Node Classification in comparable accuracy against GNNs.

### Abstract

Nowadays, Graph Neural Networks (GNNs) following the Message Passing paradigm become the dominant way to learn on graphic data. Models in this paradigm have to spend extra space to look up adjacent nodes with adjacency matrices and extra time to aggregate and transform multiple messages from adjacent nodes. To address this issue, we develop a method called LinkDist that distills self-knowledge from links into a Multi-Layer Perceptron (MLP) without the need to aggregate messages. We also distill self-knowledge from arbitrarily sampled node pairs in a contrastive way to further boost the performance of LinkDist. Experiment with 8 real-world datasets shows the MLP derived from LinkDist can predict the label of a node without knowing its adjacencies but achieve comparable accuracy against GNNs in the context of semi-supervised node classification.


## Physics informed graph neural network to estimate potential energies of unstable chemical systems

### TL;DR

None

### Abstract

We explore different strategies to integrate prior domain knowledge into the design of a deep neural network (DNN). We focus on graph neural networks (GNN), with a use case of estimating the potential energy of chemical systems (molecules and crystals) represented as graphs. We integrate two elements of physics knowledge into the design of the GNN to constrain and regularise its learning, towards higher accuracy and generalisation. First, knowledge on the existence of different types of relations (chemical bonds) between atoms is used to modulate the interaction of nodes in the GNN. Second, knowledge of the relevance of some physical quantities is used to constrain the learnt features towards a higher physical relevance using a simple multi-task paradigm. We demonstrate the general applicability of our knowledge integrations by applying them to two architectures that rely on different mechanisms to propagate information between nodes and to update node states.

## VA-GCN: A Vector Attention Graph Convolution Network for learning on Point Clouds

### TL;DR

In this article, we propose an efficient VA-GCN model to focus on the difference in importance of local information, and explicitly model the local edge features attention matrix in the VAConv module.

### Abstract

Owing to the development of research on local aggregation operators, dramatic breakthrough has been made in point cloud analysis models. However, existing local aggregation operators in current literature fails to attach decent importance to the local information of the point cloud, which limits the power of the models. To fit this gap, we propose an efficient Vector Attention Convolution module (VAConv), which utilizes K-Nearest Neighbor (KNN) to extract the neighbor points of each input point, and then uses the elevation and azimuth relationship of the vectors between the center point and its neighbors to construct an attention weight matrix for edge features. Afterwards, the VAConv adopts a dual-channel structure to fuse weighted edge features and global features. In order to verify the efficiency of the VAConv, we connect the VAConvs with different receptive fields in parallel to obtain a Multi-scale graph convolutional network, VA-GCN. The proposed VA-GCN achieves state-of-the-art performance on standard benchmarks including ModelNet40 , S3DIS and ShapeNet. Remarkably, in the ModelNet40 3D classification task, VA-GCN increased by 2.4% compared to the baseline. 

## Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks

### TL;DR

A spatio-temporal forecasting model based on finite element methods

### Abstract

We propose a new method for spatio-temporal forecasting on arbitrarily distributed points. Assuming that the observed system follows an unknown partial differential equation, we apply finite element methods to model the continuous-time dynamics of the data in the form of a message passing graph neural network. The messages represent interactions between points and imposing further structure on the messages allows us to model the convection and diffusion parts of the dynamics explicitly. We show that our model compares favorably to other spatio-temporal models in multi-step forecasting on synthetic and real-world data. Handling convection and diffusion explicitly reduces the error on data with a significant transport component.

## An Impossibility Theorem for Node Embedding

### TL;DR

We propose three reasonable properties of node embedding functions, then show that no method can satisfy all three simultaneously.

### Abstract

With the increasing popularity of graph-based methods for dimensionality reduction and representation learning, node embedding functions have become important objects of study in the literature. In this paper, we take an axiomatic approach to node embedding methods, first stating three properties for embedding dissimilarity networks, then proving that all three cannot be satisfied simultaneously by any node embedding method. Similar to existing results on the impossibility of clustering under certain axiomatic assumptions, this points to fundamental difficulties inherent to node embedding tasks. Once these difficulties are identified, we then relax these axioms to allow for certain node embedding methods to be admissible in our framework.

## Interventional Sum-Product Networks: Causal Inference with Tractable Probabilistic Models

### TL;DR

We consider the problem of learning interventional distributions (i.e., answering causal queries) with tractable probabilistic models (gated SPN).

### Abstract

While probabilistic models are an important tool for studying causality, doing so suffers from the intractability of inference. As a step towards tractable causal models, we consider the problem of learning interventional distributions using sum-product networks (SPNs) that are over-parameterized by gate functions, e.g., neural networks. Providing an arbitrarily intervened causal graph as input, effectively subsuming Pearl's do-operator, the gate function predicts the parameters of the SPN. The resulting interventional SPNs are motivated and illustrated by a structural causal model themed around personal health. Our empirical evaluation against competing methods from both generative and causal modelling demonstrates that interventional SPNs indeed are both expressive and causally adequate.

## Non-Parametric Inference of Relational Dependence

### TL;DR

This paper introduces a definition of marginal and conditional independence for relational data and proposes a consistent, non-parametric, scalable kernel test to operationalize it for non-i.i.d observations under a set of structural assumptions.

### Abstract

Independence testing plays a central role in statistical and causal inference from observational data. Standard independence tests assume that the data samples are independent and identically distributed (i.i.d.) but that assumption is violated in many real-world datasets and applications centered on relational systems. This work examines the problem of estimating independence in data drawn from relational systems by defining sufficient representations for the sets of observations influencing individual instances.
Specifically, we define marginal and conditional independence tests for relational data by considering the kernel mean embedding as a flexible aggregation function for relational variables. We propose a consistent, non-parametric, scalable kernel test to operationalize the relational independence test for non-i.i.d observational data under a set of structural assumptions.
We empirically evaluate our proposed method on a variety of synthetic and semi-synthetic networks and demonstrate its effectiveness compared to state-of-the-art kernel-based independence tests.

## Local Hyper-Flow Diffusion

### TL;DR

We propose the first local diffusion method that achieves edge-size-independent Cheeger-type guarantee for the problem of local hypergraph clustering while applying to a rich class of higher-order relations.

### Abstract

Recently, hypergraphs have attracted a lot of attention due to their ability to capture complex relations among entities. The insurgence of hypergraphs has resulted in data of increasing size and complexity that exhibit interesting small-scale and local structure, e.g., small-scale communities and localized node-ranking around a given set of seed nodes. Popular and principled ways to capture the local structure are the local hypergraph clustering problem and the related seed set expansion problem. In this work, we propose the first local diffusion method that achieves edge-size-independent Cheeger-type guarantee for the problem of local hypergraph clustering while applying to a rich class of higher-order relations that covers a number of previously studied special cases. Our method is based on a primal-dual optimization formulation where the primal problem has a natural network flow interpretation, and the dual problem has a cut-based interpretation using the $\ell_2$-norm penalty on associated cut-costs. We demonstrate the new technique is significantly better than state-of-the-art methods on both synthetic and real-world data.


## Learning to Affiliate: Mutual Centralized Learning for Few-shot Classification

### TL;DR

None

### Abstract

Few-shot learning (FSL) aims to learn a classifier that can be easily adapted to accommodate new tasks not seen during training, given only a few examples. To handle the limited-data problem in few-shot regimes, recent methods tend to collectively use a set of local features to densely represent an image instead of using a mixed global feature. They generally explore a unidirectional query-to-support paradigm in FSL, e.g., find the nearest/optimal support feature for each query feature and aggregate these local matches for a joint classification. In this paper, we propose a new method Mutual Centralized Learning (MCL) to fully affiliate the two disjoint sets of dense features in a bidirectional paradigm. We associate each local feature with a particle that can bidirectionally random walk in a discrete feature space by the affiliations. To estimate the class probability, we propose the features' accessibility that measures the expected number of visits to the support features of that class in a Markov process. We relate our method to learning a centrality on an affiliation network and demonstrate its capability to be plugged in existing methods by highlighting centralized local features. Experiments show that our method achieves the state-of-the-art on both miniImageNet and tieredImageNet.

## Can fMRI reveal the representation of syntactic structure in the brain?

### TL;DR

We show that naturalistic fMRI data can be used to study syntactic representations in the human brain by leveraging our proposed graph embedding-based features that encode syntactic structure.

### Abstract

While studying semantics in the brain, neuroscientists use two approaches. One is to identify areas that are correlated with semantic processing load. Another is to find areas that are predicted by the semantic representation of the stimulus words. However, in the domain of syntax, most studies have focused only on identifying areas correlated with syntactic processing load. One possible reason for this discrepancy is that representing syntactic structure in an embedding space such that it can be used to model brain activity is a non-trivial computational problem. Another possible reason is that it is unclear if the low signal-to-noise ratio of neuroimaging tools such as functional Magnetic Resonance Imaging (fMRI) can allow us to reveal correlates of complex (and perhaps subtle) syntactic representations. In this study, we propose novel multi-dimensional features that encode information about the syntactic structure of sentences. Using these features and fMRI recordings of participants reading a natural text, we model the brain representation of syntax. First, we find that our syntactic structure-based features explain additional variance in the brain activity of various parts of the language system, even after controlling for complexity metrics that capture processing load. At the same time, we see that regions well-predicted by syntactic features are distributed in the language system and are not distinguishable from those processing semantics.

## Neural Models for Domain-Size Invariance in Lifted CSPs

### TL;DR

We propose neural architectures to achieve domain size invariance, e.g. solving 16 x 16 sudoku by learning from 9 x 9 sudoku

### Abstract

Recent work has seen a surge in neural architectures for solving Constraint Satisfaction Problems (CSPs), such as Sudoku Puzzle, where the explicit constraints are not known, and instead learned from data. Most existing literature focuses on a fixed domain size of variables (eg, number of digits in Sudoku), during training and testing. We extend this work by studying domain-size invariance in a Lifted CSP [Joslin and Roy, 1997]: given training data of ground CSPs from one domain-size, can we learn neural models that generalize to instances of the same lifted CSP but from a different domain size? We propose two solutions for our problem on top of Recurrent Relational Networks (RRNs) [Palm et al., 2018], which model this problem in fixed domain-size setting. Our first solution converts each (multi-valued) CSP variable into multiple Boolean variables -– one for each value in the domain. Our \emph{binarized} problem is now naturally domain-size invariant, because it uses only two values, `0' and `1'. In our second solution, we operate directly on multi-valued problem, and represent each value in the domain as a separate node in the graph. We achieve domain size-invariance by training embeddings for a larger number of values than present in training problems. In both solutions, we supplement RRN with additional components -- an attention module and a loss term. Our experimental evaluation on three different lifted CSPs (Sudoku, Graph Coloring, and Futoshiki) demonstrates that both our models perform well on our novel problem, compared to a generic neural reasoner which performs significantly worse. We also compare the two models along axes of performance and scalability, and discuss their individual strengths and weaknesses.

## Geometry Enhanced Molecular Representation Learning For Property Prediction

### TL;DR

We propose a novel Geometry Enhanced Molecular representation learning method (GEM).

### Abstract

Effective molecular representation learning is of great importance to facilitate molecular property prediction, which is a fundamental task for the drug and material industry. Recent advances in graph neural networks (GNNs) have shown great promise in applying GNNs for molecular representation learning. Moreover, a few recent studies have also demonstrated successful applications of self-supervised learning methods to pre-train the GNNs to overcome the problem of insufficient labeled molecules. However, existing GNNs and pre-training strategies usually treat molecules as topological graph data without fully utilizing the molecular geometry information. Whereas, the three-dimensional (3D) spatial structure of a molecule, a.k.a molecular geometry, is one of the most critical factors for determining molecular physical, chemical, and biological properties. To this end, we propose a novel Geometry Enhanced Molecular representation learning method (GEM). At first, we design a geometry-based GNN architecture that simultaneously models atoms, bonds, and bond angles in a molecule. To be specific, we devised double graphs for a molecule: The first one encodes the atom-bond relations; The second one encodes bond-angle relations. Moreover, on top of the devised GNN architecture, we propose several novel geometry-level self-supervised learning strategies to learn spatial knowledge by utilizing the local and global molecular 3D structures. We compare GEM with various state-of-the-art (SOTA)  baselines on different molecular benchmarks and exhibit that GEM can significantly outperform all baselines in both regression and classification tasks. For example, the experimental results show an overall improvement of $8.8\%$ on average compared to SOTA baselines on the regression tasks, demonstrating the superiority of the proposed method.

## Memory-Efficient Approximation Algorithms for Max-k-Cut and Correlation Clustering

### TL;DR

We generate approximation solutions to Max-k-Cut and correlation clustering using memory linear in the size of the input graph

### Abstract

Max-k-Cut and correlation clustering are fundamental graph partitioning problems. The methods with the best approximation guarantees for Max-k-Cut and the Max-Agree variant of correlation clustering involve solving SDPs with $n^2$ variables and $n^2$ constraints (where $n$ is the number of vertices). Large-scale instances of SDPs, thus, present a memory bottleneck. In this paper, we develop a simple polynomial-time Gaussian sampling-based algorithms for these two problems that use $\mathcal{O}(n+|E|)$ memory and nearly achieve the best existing approximation guarantees. For dense graphs arriving in a stream, we eliminate the dependence on $|E|$ in the storage complexity at the cost of a slightly worse approximation ratio by combining our approach with sparsification.


## BORSCHT: Adversarial framework for molecular conformation space modeling in internal coordinates.

### TL;DR

A novel adversarial generative model for molecular conformation generation

### Abstract

In this paper, we propose a novel generative model for molecular conformation generation built upon the combination of Generative Adversarial Network and Adversarial Autoencoder. Given a molecular graph, the generator produces internal coordinates of a conformation  - an iterative representation, describing each atom by bond length and two angles relative to its predecessors. Then, the generator predicts the distance geometry of local molecular structures and makes several optimization steps to refine the conformation. The proposed model produces diverse low-energy conformations while showing geometric and coverage metrics comparable to previous models. We also introduce a simple yet meaningful conditional generation task and a corresponding benchmark - modeling of conformations with desired 3D descriptors for drug discovery problems like protein binding.

## SELFCON: Molecular Self-Contrastive Learning for Chemical Property Prediction

### TL;DR

We propose SELFCON, a self-contrastive objective, for universal molecular representation learning that learns to compare two noise versions of encoded batch samples.

### Abstract

The dominant molecular representation learning methods are based on complex self-supervised objectives for predicting graph context in a pre-training configuration. We propose SELFCON, a self-contrastive objective, for universal molecular representation learning that learns to compare two noise versions of encoded batch samples. We further execute large-scale Molecular Similarity Search on real-world dataset ZINC-15 based on pre-trained models which assigns higher scores confidently for similar pairs than traditional Morgan Fingerprint, while scoring with larger variance for random pairs. We conduct extensive experiments over 9 datasets with 3 commonly used encoders on both random and scaffold split setting. Ablation study and visualization verify the effectiveness of SELFCON.

## Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation

### TL;DR

A sequential generative model based on (graph-theoretic) flow networks, treating the root as a source and all possible generated objects as sinks.

### Abstract

This paper is about the problem of learning a stochastic policy for generating an object (like a molecular graph) from a sequence of actions, such that the probability of generating an object is proportional to a given positive reward for that object. Whereas standard return maximization tends to converge to a single return-maximizing sequence, there are cases where we would like to sample a diverse set of high-return solutions. These arise, for example, in black-box function optimization when few rounds are possible, each with large batches of queries, where the batches should be diverse, e.g., in the design of new molecules. One can also see this as a problem of approximately converting an energy function to a generative distribution. While MCMC methods can achieve that, they are expensive and generally only perform local exploration. Instead, training a generative policy amortizes the cost of search during training and yields to fast generation.  Using insights from Temporal Difference learning, we propose GFlowNet, based on a view of the generative process as a flow network, making it possible to handle the tricky case where different trajectories can yield the same final state, e.g., there are many ways to sequentially add atoms to generate some molecular graph. We cast the set of trajectories as a flow and convert the flow consistency equations into a learning objective, akin to the casting of the Bellman equations into Temporal Difference methods. We prove that any global minimum of the proposed objectives yields a policy which samples from the desired distribution, and demonstrate the improved performance and diversity of GFlowNet on a simple domain where there are many modes to the reward function, and on a molecule synthesis task.

## $\rm TVM_T$: An End-to-End Compiler Supporting both Training and Inference

### TL;DR

This paper presents $\rm TVM_T$, the first end-to-end compiler based on TVM for neural network training.

### Abstract

With the rapid development of deep learning applications, the task of searching for the best neural network models, e.g., neural architecture search (NAS) is attracting increasing attention. For NAS, the performance of neural network training is crucial, which heavily depends on both the hardware backend and the corresponding compiler optimization. To enhance the performance of neural network training, the compiler's effectiveness needs to be improved, which mainly depends on the computational graph optimization, operator-level optimization, and code generation. However, mainstream training frameworks (e.g., TensorFlow, Pytorch) provide vendor-specific operator libraries, which are obtained from manual operator design. However, a large operator-level optimization space is wasted due to the manual operator design. Therefore, TVM has been proposed. As an end-to-end compiler, TVM enables automatic operator-level optimization, and thus further enhances the performance over the existing frameworks. Moreover, TVM supports deployment of different neural network models on a wide range of hardware backends. Unfortunately, TVM only focuses on the inference performance, and does not support training of neural networks.  

This paper presents $\rm TVM_T$, the first end-to-end compiler based on TVM for neural network training. To support neural network training, the paper presents the following methods: (1) the loss function is merged into existing computational graph for supporting both forward and backward propogations; (2) the device-to-host mechanism is adopted to update the weight parameters during training process; (3) the state-of-the-art tensor program tuner is integrated to automatically generate and optimize the training program. Experimental results show 
that, compared with Pytorch, $\rm TVM_T$ improves the training performance of popular deep neural networks on Intel CPU and NVIDIA GPU by up to 4.5x, and 10x, respectively. 

## Graph Spatial Convolutional LSTM: A New Spatial-Temporal Model for Near-Surface Temperature Prediction

### TL;DR

We proposed the Graph Spatial Convolution, which can nicely unify GCN and CNN, to capture both long- and short-range spatial correlations.

### Abstract

The near-surface temperature forecast is a  classic spatial-temporal prediction problem, which can be used to prevent urban heat islands, agricultural drought, heat exposure, and other temperature crises. Many methods have been proposed. However, most of the previous approaches fail to explicitly model the long- and short-range spatial correlations simultaneously, which is critical to an accurate temperature prediction. In this paper, we propose a novel convolution operator named Graph Spatial Convolution which can explicitly model the two correlations at the same time by unifying the GCN and CNN. Upon the operator, a new near-surface temperature prediction model called GS-ConvLSTM is developed based on the novel convolution operator. Extensive experiments are conducted on two real-world datasets and the results show that our proposed method outperforms state-of-the-art techniques.

## Nara: Learning Network-Aware Resource Allocation Algorithms for Cloud Data Centres

### TL;DR

Reinforcement- and graph- learning based method to solve the NP-hard combinatorial optimisation problem associated with network aware data center resource allocation.

### Abstract

Data centres (DCs) underline many prominent future technological trends such as distributed training of large scale machine learning models and internet-of-things based platforms. DCs will soon account for over 3\% of global energy demand, so efficient use of DC resources is essential. Robust DC networks (DCNs) are essential to form the large scale systems needed to handle this demand, but can bottleneck how efficiently DC-server resources can be used when servers with insufficient connectivity between them cannot be jointly allocated to a job. However, allocating servers' resources whilst accounting for their inter-connectivity maps to an NP-hard combinatorial optimisation problem, and so is often ignored in DC resource management schemes. We present Nara, a framework based on reinforcement learning (RL) and graph neural networks (GNN) to learn network-aware allocation policies that increase the number of requests allocated over time compared to previous methods. Unique to our solution is the use of a GNN to generate representations of server-nodes in the DCN, which are then interpreted as actions by a RL policy-network which chooses from which servers resources will be allocated to incoming requests. Nara is agnostic to the topology size and shape and is trained end-to-end. The method can accept up to 33\% more requests than the best baseline when deployed on DCNs with up to the order of $10\times$ more compute nodes than the DCN seen during training and is able to maintain its policy's performance on DCNs with the order of $100\times$ more servers than seen during training. It also generalises to unseen DCN topologies with varied network structure and unseen request distributions without re-training.

## Scalable and Modular Theory-Driven Regularization of Neural-Based Models for Scene Graph Generation

### TL;DR

None

### Abstract

Despite the demonstrable effectiveness of neural-symbolic solutions for computer vision tasks, most past work has invoked symbolic theories only after the training of the neural module, as an inference-time way to enhance or regularize the neural predictions. In this work we adopt training-time regularization (TTR), and demonstrate how it can be carried out in a scalable manner that can be modularly integrated with existing neural-based models for Scene Graph Generation (SGG). Scalability is ensured by dynamically identifying the subset of the entire theory that is maximally unsatisfied w.r.t. specific training samples and regularizing the neural module using only that subset. We empirically demonstrate that our technique can effectively and efficiently improve the performance of state-of-the-art neural-based models for SGG, bringing more substantial improvements over logic-based inference-time regularization. Our concluding discussion on how our proposed technique fits naturally within a recently-proposed compositional neural-symbolic integration framework suggests that this technique could be used with theories of arbitrary syntax and semantics.

## Relative Molecule Self-Attention Transformer

### TL;DR

We made a transformer-based model with Relative Molecular Self-Attention and obtained state-of-the-art results in molecular property prediction tasks.

### Abstract

Transfer learning holds promise to revolutionize molecule property prediction - a central task to drug discovery and many more industries - by enabling data efficient learning from scarce experimental data. Motivated by the recently demonstrated potential of the Transformer architecture for molecule property prediction, we methodologically explore the design space of the self-attention mechanism tailored to molecular data.  We identify a particularly effective self-attention mechanism adapted to processing molecules, inspired by the relative self-attention layer, which involves fusing relative graph encoding and distance relations between atoms. Our main contribution is Relative Molecule Attention Transformer (R-MAT): a~novel Transformer-based model based on the developed self-attention layer that achieves state-of-the-art results across a wide range of molecule property prediction tasks.

## UniRank: Unimodal Bandit Algorithm for Generic Online Ranking

### TL;DR

None

### Abstract

We tackle, in the multiple-play bandit setting, the online ranking problem of assigning L items to predefined positions on a web page in order to maximize the number of user clicks. We propose a generic algorithm that can tackle all click models. The regret bound in O(L/Delta log(T))  of our algorithm, UniRank, is a direct consequence of the unimodality property of the bandit setting with respect to a graph where nodes are ordered sets of indistinguishable items. Experiments against state-of-the-art learning algorithms specialized or not for different click models, show that our method is as efficient as the best known specialized algorithms while giving better regret performance than generic ones on a real life dataset.

## Reinforcement Learning under a Multi-agent Predictive State Representation Model: Method and Theory

### TL;DR

None

### Abstract

This paper proposes a novel decentralized algorithm for learning the optimal policies under a multi-agent predictive state representation reinforcement learning model. Compared to the sate-of-the-art methods, the most striking feature of our algorithm is the introduction of a dynamic interaction graph which allows us to represent each agent's predictive state by considering  the behaviors of its ``neighborhood's agents. Methodologically, we develop an online algorithm that simultaneous learn the predictive state representation and agent policies. Theoretically, we provide an upper bound of the $L_2$-norm of the learned predictive state representation. Empirically, to demonstrate the efficacy of the proposed method, we provide thorough numerical results on both a MAMuJoCo robotic learning experiment and a multi-agent particle learning environment.

## Matrix factorisation and the interpretation of geodesic distance

### TL;DR

None

### Abstract

Given a graph or similarity matrix, we consider the problem of recovering a notion of true distance between the nodes, and so their true positions. Through new insights into the manifold geometry underlying a generic latent position model, we show that this can be accomplished in two steps: matrix factorisation, followed by nonlinear dimension reduction. This combination is effective because the point cloud obtained in the first step lives close to a manifold in which latent distance is encoded as geodesic distance. Hence, a nonlinear dimension reduction tool, approximating geodesic distance, can recover the latent positions, up to a simple transformation. We give a detailed account of the case where spectral embedding is used, followed by Isomap, and provide encouraging experimental evidence for other combinations of techniques.


## TENET: Tensor Embedding Network for Sparse Origin-Destination Flow Prediction

### TL;DR

We propose a simple and efficient method for spatial-temporal origin-destination flow prediction, which achieves state-of-the-art performance on several origin-destination benchmarks.

### Abstract

Origin-Destination (OD) flow reveals the latent traffic demands in urban city. Unlike general spatio-temporal prediction tasks, the Origin-Destination flow tends to be a two-dimensional matrix and highly sparse. To solve the data sparsity problem, we propose a widely applicable framework, named TENET in this paper, which leverages tensor decomposition and graph convolution blocks to achieve spatio-temporal information embedding.  Meanwhile, a bi-directional (past/future) temporal prediction module is designed to enhance the training data at each batch. In the experiment, TENET show a great improvement at both New York and Chengdu OD flow dataset than existing methods at different data sparsity levels. In addition, TENET could be integrated into different prediction methods to further improve their performances.

## VICAUSE: Simultaneous missing value imputation and causal discovery with groups

### TL;DR

None

### Abstract

Missing values constitute an important challenge in real-world machine learning for both prediction and causal discovery tasks. However, existing imputation methods are agnostic to causality, while only few methods in traditional causal discovery can handle missing data in an efficient way. In this work we propose VICAUSE, a novel approach to simultaneously tackle missing value imputation and causal discovery efficiently with deep learning. Particularly, we propose a generative model with a structured latent space and a graph neural network-based architecture, scaling to large number of variables. Moreover, our method can discover relationship between groups of variables which is useful in many real-world applications. VICAUSE shows improved performance compared to popular and recent approaches in both missing value imputation and causal discovery.

## Temporal Hyperbolic Hypergraph Network

### TL;DR

To better encode temporal/dynamic/static scale-free hypergraphs, we propose a temporal hyperbolic hypergraph convolution method that combines Euclidean & hyperbolic geometries and achieves state-of-the-art performance across numerous real-world tasks

### Abstract

Hyperbolic geometry has advanced learning representations on graphs with inherently complex geometrical and hierarchical characteristics. However, most real world networks innately comprise of higher-order relations,  dynamic behavior, and scale-free temporal characteristics with varying degrees of hyperbolicity. Combating these gaps, we propose THRONE, a temporal, inter-geometrical interaction learning-based hypergraph convolution method to capitalize on the complex, time-varying, higher-order relations and the varying hyperbolicity of network structures. Further, we enhance the hypergraph convolution by applying attention infused with hyperbolic distance information among node and hyperedge representations. THRONE incorporates hyperbolic temporal convolution layers to encode scale-free spatio-temporal information and dynamic time-evolving network structures. We extend THRONE to hypergraph-level tasks by introducing THRONE-Pool, a novel hyperbolic hypergraph pooling method to encode higher-order scale-free networks. Through a series of quantitative and exploratory analyses on ten node-level and six network-level tasks across static, spatio-temporal, and time-evolving dynamic hypergraphs, we demonstrate THRONE's practical applicability in comparison to competitive baselines. We dissect THRONE's performance contributions on a variety of benchmarks and applications spanning finance, health, traffic, wind energy, and citation networks through ablations to highlight the effectiveness of each component. Through THRONE, we take a step forward in devising a data, task, hyperbolicity and network agnostic method for learning representations.

## RLogic: Recursive Logical Rule Learning from Knowledge Graphs

### TL;DR

We investigate the problem of logical rule learning and propose a novel framework, known as RLogic, to learn logical rules directly at the schema level via sampled rule instances.

### Abstract

Logical rules are widely used to represent domain knowledge and hypothesis, which is fundamental to symbolic reasoning-based human intelligence. Very recently, it has been demonstrated that integrating logical rules into regular learning tasks can further enhance learning performance in a label-efficient manner. Many attempts have been made to learn logical rules automatically from knowledge graphs (KGs). However, all existing methods rely on the complete set of rule instances for rule evaluation and thus suffer from severe computational inefficiency. To evaluate rules in a more efficient way, we propose a novel framework RLogic to learn rules directly at the schema level. Instead of relying on all rule instances for rule evaluation, we propose to learn the plausibility of logical rules based on sampled rule instances (i.e., closed paths on KGs). In addition, RLogic incorporates one of the most significant properties of logical rules, the deductive nature, into rule learning. The introduction of the deductive nature allows RLogic to follow logical deduction to validate a rule, which is critical especially when we lack evidence to support a rule. Extensive experiments have demonstrated that RLogic is superior to existing state-of-the-art algorithms in terms of both efficiency and effectiveness. 

## Learning Representations of Partial Subgraphs by Intra- and Inter-Subgraph InfoMax

### TL;DR

We propose Intra- and Inter-Subgraph InfoMax, a model that learns a subgraph representation under partial observation.

### Abstract

Subgraphs are important substructures of graphs, but learning their representations has not been studied well. Particularly, when we have partial subgraphs, existing node- or subgraph-level message-passing is likely to produce suboptimal representations. In this paper, we propose Intra- and Inter-Subgraph InfoMax, a model that learns subgraph representations under incomplete observation. Our model employs subgraph summaries at two different levels while maximizing the mutual information between the subgraph summaries and the node representations. By doing so, we reconstruct the representation of the underlying subgraph and improve its expressiveness from different angles of the local-global structure. We conduct experiments on three real-world datasets under training and evaluation protocols designed for this problem. Experimental results show that our model outperforms baselines in all settings.

## Program-to-Circuit: Exploiting GNNs for Program Representation and Circuit Translation

### TL;DR

None

### Abstract

Comparing with software development, circuit design is much more complicated and requires extensive domain-specific expertise. One major obstacle stuck on the way to hardware agile development is the considerably time-consuming process of accurate circuit quality evaluation. To significantly expedite the circuit evaluation, we formulate it as a Program-to-Circuit problem by representing C/C++ programs as graphs, to exploit the representation power of graph neural networks (GNNs). The goal of this work is four-fold. First, we build a standard benchmark containing 40k C/C++ programs, each of which is translated to a circuit design with actual hardware quality metrics, aiming to facilitate and encourage the development of effective GNNs targeting this high-demand circuit design area. Second, 14 state-of-the-art GNN models are tested and analyzed on the Program-to-Circuit problem. We identify key design challenges of this problem, which should be carefully handled but not yet solved by existing GNNs. The goal is to provide domain-specific knowledge for designing GNNs with suitable inductive biases. Third, we discuss three sets of real-world benchmarks for GNN generalization evaluation and analyze the performance gap between the standard programs and the real-case ones. The goal is to provide more realistic benchmarking and to enable transfer learning from limited training data to real-world large-scale circuit design problems. Fourth, the Program-to-Circuit problem is one representative within the general Program-to-X framework, a set of program-based analysis problems with various downstream tasks.
An in-depth understanding of the strength and weaknesses in applying GNNs on Program-to-Circuit could largely benefit the entire family of Program-to-X. Pioneering in this direction, we expect more GNN endeavors to revolutionize this high-demand program-to-circuit problem and to enrich the expressiveness of GNNs on programs.

## Route Roulette Via Structured Probability Spaces

### TL;DR

Sampling from a learnt path distribution, using knowledge compilation diagram, with a new relaxed encoding and matrix-based algorithms to yield significant performance improvements

### Abstract

Structured probability distributions (SPD) are of increasing interest as they can capture underlying combinatorial structures of various real-world problems, such as route prediction and product recommendation. A promising line of approach to reasoning and inference over SPDs employs knowledge compilation diagrams. Despite the advantages that knowledge compilation approaches have over other techniques, scalability remains a challenge. In this work we introduce a two-fold approach, comprising of a novel relaxed encoding methodology and matrix-based algorithms, to address scalability challenges of knowledge compilation techniques. We demonstrate the power of our approach on a well-known problem of route prediction. In particular, we observed at least two orders of magnitude of improvement in the size of knowledge compilation diagram and one of magnitude in runtime performance.

## A Unified Weight Initialization Paradigm for Tensorial Convolutional Neural Networks

### TL;DR

None

### Abstract

Tensorial Convolutional Neural Networks (TCNNs) have attracted a lot of research attention due to their power in reducing the number of model parameters or enhancing the generalization ability. A significant issue for stable convergence in TCNNs is the initialization of weights. However, classical weight initialization methods such as Xavier and Kaiming initialization usually fail to generate appropriate weights for guaranteeing fast convergence when directly applied to TCNNs. Although some previous works proposed ad-hoc approaches for specific architectures, they cannot generalize to other types of TCNNs. To address these problems, we propose a unified weight initialization paradigm, which generalizes the Xavier and Kaiming methods and can be widely applicable to arbitrary TCNNs. Specifically, we first present Complementary Tensor Graph (CTG) to formally represent not only TCNN structures, but also the forward and backward propagation. Then, we build a common principle based on CTG, which is adapted to multi-node tensor networks by controlling the variance of features and gradients. Thus, through the paradigm, we can derive fan-in and fan-out initialization for various TCNNs in one scheme. We demonstrate that our paradigm can stabilize networks, leading to faster convergence and better results.

## Cultural Heritage Fragment Clustering: From Local Patch Matching to Global Linkage Prediction

### TL;DR

None

### Abstract

Clustering numerous fragmentary cultural heritage objects by their decorative patterns is an important and time-consuming work in archaeology. In this paper, we study this problem with a focus on pottery sherds from southeastern North America which are stamped with linear and curvilinear geometric patterns. Unlike common image clustering problems, pottery sherds in the same cluster are not required to have similar global appearance, but only need to be partially similar. Due to the special partial correspondence, existing clustering methods that focus on global image features are not able to solve this problem.  We formulate this task as a graph link prediction problem, and propose a local-to-global strategy to predict sherd linkage. Specifically, we first find the best matched local patches in each pair of query sherds with template matching. For each sherd, it may have different local patches that correspond to different neighboring sherds. To leverage the information of neighboring sherds, we propose Patch-GCN to model the many-to-one relationship between local patches and global sherds, which could enhance patch-level features while considering sherd-level topologies. Finally, the sherd linkage is predicted by a MLP classifier based on the GCN-enhanced pairwise patch features. Extensive experiments on 1402 high-quality pottery sherds demonstrate the superiority of our method over existing methods on the problem of sherd clustering.

## Predict-all from One using Grid-Based Multidimensional Generative Learning

### TL;DR

None

### Abstract

The recent success of generative models enabled them to cover a wide range of domain applications. However, when it comes to cross-dimensions learning, these models often struggle to deliver decent performances. Existing studies aimed to circumvent such issues by confusing cross-dimensions learning with multi-modal learning or limiting their proposed models to uni-dimensions learning. Here, we propose to solve this problem by setting a new learning technique allowing us to predict all available cross-dimension distributions from a single input cross-dimension distribution any to all and any to any. To do so, we draw inspiration from the traveling salesman problem (TSP) to propose a stochastic grid learning technique. We denote every cross-dimension distribution as a city and every generative model as a roadway. Then, we stochastically select a path, arrange its generative components sequentially, and train them. Such a method guarantees that we learn all existing possible prediction schemes within a set of available cross-dimensions. We design our learning technique in the spirit of nicely managing to handle the training of several generative models altogether. With biomedical graph learning in mind, we show that our framework generalizes, matches, and outperforms the performances of existing state-of-the-art generative methods. Code is available at https://drive.google.com/drive/folders/1UShteDs42CsdYAddXAaWCFFlNM4LtJCY?usp=sharing

## ClusterST: Clustering Spatial Temporal Networks for Traffic Forecasting 

### TL;DR

None

### Abstract

Traffic forecasting aims to capture complex spatial-temporal dependencies and non-linear dynamics, which plays an indispensable role in intelligent transportation systems and other domains like neuroscience, climate, etc. Most recent works rely on graph convolutional networks (GCN) to model the dependencies and the dynamics. However, the over-smoothing issue of GCN would produce indistinguishable features among nodes, leading to poor expressivity and weak capability of modeling complex dependencies and dynamics. To address this issue, we present a novel clustering spatial-temporal (ClusterST) unit, which incorporates unsupervised learning into GCN for extracting discriminative features. Specifically, we first exploit a neural network to learn a dynamic online cluster, i.e., learning to partition the neighbors of each node into clusters at each time step. Two probabilistic losses are proposed to improve the separability of clusters. Then, the extracted features of different clusters can be distinguished and a vanilla GCN is applied to aggregate features in each cluster. Another convolutional network is applied to fuse the extracted feature. By purely exploiting such ClusterST unit, large improvements over the state-of-the-art are achieved. Furthermore, ClusterST unit with a different number of clusters can be regarded as basic components to construct an inception-like ClusterST network for going deeper. We evaluate the framework on two real-world large-scale road network traffic datasets and observe an average improvement of $18.19\%$ and $7.62\%$ over state-of-the-art baselines, respectively. The code and models will be publicly available.

## Dynamic Trace Estimation

### TL;DR

None

### Abstract

We study a dynamic version of the implicit trace estimation problem. Given access to an oracle for computing matrix-vector multiplications with a dynamically changing matrix A, our goal is to maintain an accurate approximation to A's trace using as few multiplications as possible. We present a practical algorithm for solving this problem and prove that, in a natural setting, its complexity is quadratically better than the standard solution of repeatedly applying Hutchinson's stochastic trace estimator. We also provide an improved algorithm assuming additional common assumptions on A's dynamic updates. We support our theory with empirical results, showing significant computational improvements on three applications in machine learning and network science: tracking moments of the Hessian spectral density during neural network optimization, counting triangles and estimating natural connectivity in a dynamically changing graph.

## Self-Supervised Learning for 3D Human Pose Estimation

### TL;DR

We propose a novel representation learning method using contrastive learning which can be applied to semi-supervised and weakly-supervised learning of 3D human pose from multi-view images.

### Abstract

We propose a novel representation learning method which can be applied to semi-supervised and weakly-supervised learning of 3D human pose from multi-view images. Based on the recent network architecture which estimates of 3D canonical pose and camera rotation simultaneously, the proposed method leverages triplet loss and contrastive loss so that the network can learn meaningful representation from 2D pose datasets. Positive examples and negative examples for contrastive loss is differently designed for pose estimator and rotation estimator. After pre-training the networks, the network learns powerful representation suitable for multi-view pose learning. In addition, we propose a novel network structure using graph convolution to incorporate the structure of human skeleton. The proposed method achieves state-of-the-art performance under semi-supervised and weakly-supervised settings on Human 3.6M dataset.

## PrototypeML: Visual Design of Arbitrarily Complex Neural Networks

### TL;DR

First visual neural network development environment for PyTorch, supporting arbitrarily complex network architectures.

### Abstract

Neural network architectures are most often conceptually designed and described in visual terms, but are implemented by writing error-prone code. PrototypeML is a machine learning development environment that bridges the dichotomy between the design and development processes: it provides a highly intuitive visual neural network design interface that supports (yet abstracts) the full dynamic graph capabilities of the PyTorch deep learning framework, reduces model design and development time, makes debugging easier, and automates many framework and code writing idiosyncrasies. In this paper, we detail the deep learning development deficiencies that drove the implementation of PrototypeML, and propose a hybrid approach to resolve these issues without limiting network expressiveness or reducing code quality. We demonstrate the real-world benefits of a visual approach to neural network design for research, industry and teaching.

## Reinforcement Learning for the Strip Packing Problem

### TL;DR

None

### Abstract

The 2D packing problem is a combinatorial optimization problem often encountered in industrial production. Recently, researchers have tried to employ reinforcement learning as a new strategy to solve such problems. However, these methods lack practicality without fully considering the characteristics of the packing problem. In this article, we combine reinforcement learning with heuristic methods to solve the strip packing problem, where graph neural network and convolution neural network are adopted  to extract skyline features and select a rectangle in each step. Our network supports decimal length and, due to memoryless property, is more suitable for online situations. Experiments show that our method significantly improves the performance of the Bottom-Left heuristic and exceeds the current optimal heuristic algorithm MaxRectsBssf. Moreover, further experiments prove that our method generalizes well to large-scale and online cases.

## Learning Bipartite Markov Random Fields

### TL;DR

None

### Abstract

We present a \textit{convex} formulation to the problem of learning an undirected, weighted bipartite graph under the Gaussian Markov Random Field model, along with a scalable optimization algorithm based on the alternating direction method of multipliers that is guaranteed to find the global minima. Previous attempts to this problem relied on formulations that are either statistically motivated nonetheless nonconvex, therefore likely suboptimal, or convex but lacking statistical foundations. Motivated by practical applications where outliers or heavy-tails events are present, we extend the proposed learning scheme to the case where the data follow a Student-$t$ distribution, for which the optimization program is no longer convex, but a provably convergent iterative algorithm is proposed. The proposed estimators outperform state-of-the-art benchmarks, as evidenced by real-world data experiments.

## NeuroLKH: Combining Deep Learning Model with Lin-Kernighan-Helsgaun Heuristic for Solving the Traveling Salesman Problem

### TL;DR

We present NeuroLKH, a novel algorithm that combines deep learning with the strong traditional heuristic Lin-Kernighan-Helsgaun (LKH) for solving Traveling Salesman Problem and apply NeuroLKH to three other routing problems.

### Abstract

We present NeuroLKH, a novel algorithm that combines deep learning with the strong traditional heuristic Lin-Kernighan-Helsgaun (LKH) for solving Traveling Salesman Problem. Specifically, we train a Sparse Graph Network (SGN) with supervised learning for edge scores and unsupervised learning for node penalties, both of which are critical for improving the performance of LKH. Based on the output of SGN, NeuroLKH creates the edge candidate set and transforms edge distances to guide the searching process of LKH. Extensive experiments firmly demonstrate that, by training one model on a wide range of problem sizes, NeuroLKH significantly outperforms LKH and generalizes well to much larger sizes. Also, we show that NeuroLKH can be applied to other routing problems such as Capacitated Vehicle Routing Problem (CVRP), Pickup and Delivery Problem (PDP), and CVRP with Time Windows (CVRPTW).

## Learning from Approver Governed Slanted Supervision

### TL;DR

None

### Abstract

In this paper we address the problem of learning from approver-governed slanted supervision, i.e., an instance can enter a subsequent process in which the label is revealed if and only if it is approved to by the approver, while the labeling the rejected instances is infeasible economically, practically, or ethically. The existence of the approver makes the problem different from the traditional semi-supervised learning problem, as the labeled data slants to the one side of the approval boundary. Due to the lack of label on the rejected side of the approval boundary, learner is prone to astray. We formalize the problem of learning from approve governed slanted supervision, and propose Graph based Slantly Supervised Learning (GSSL) to tackle the problem. Experimental results show the effectiveness of our method.

## STGT: A High Efficient Traffic Prediction Model based on  Transformer

### TL;DR

None

### Abstract

  In order to model complex and dynamic spatio-temporal correlations in traffic time series, neural network architectures proposed for traffic prediction keep growing in size and complexity. This however not only increases the memory footprint and computation time, but also has a high risk of overfitting. In this paper, we propose a simple and efficient model called Spatio-Temporal Graph Transformer (STGT), which adopts a simple encoder-decoder structure accompanying with two embedding modules. Both the encoder and the decoder contain only one spatio-temporal self-attention module, and the embedding modules provide them with information of road network structure and time periodic characteristics. Experiments on two real datasets show that STGT achieves state-of-the-art performance in terms of overall accuracy, and meanwhile uses far fewer parameters than other models.

## SE(3)-Equivariant Energy-based Models for End-to-End Protein Folding

### TL;DR

We propose an end-to-end and computationally-efficient approach for protein structure optimization, powered by SE(3)-equivariant energy-based models.

### Abstract

Accurate prediction of protein structures is critical for understanding the biological function of proteins. Nevertheless, most structure optimization methods are built upon pre-defined statistical energy functions, which may be sub-optimal in formulating the conformation space. In this paper, we propose an end-to-end approach for protein structure optimization, powered by SE(3)-equivariant energy-based models. The conformation space is characterized by a SE(3)-equivariant graph neural network, with substantial modifications to embed the protein-specific domain knowledge. Furthermore, we introduce continuously-annealed Langevin dynamics as a novel sampling algorithm, and demonstrate that such process converges to native protein structures with theoretical guarantees. Extensive experiments indicate that SE(3)-Fold achieves comparable structure optimization accuracy, compared against state-of-the-art baselines, with over 1-2 orders of magnitude speed-up.

## Efficient Network Embedding based on  Commute Time Computation using Diffusion Wavelets

### TL;DR

None

### Abstract

In this paper, we propose an efficient scheme for numerical implementation of network 
embedding that preserves commute times computed with diffusion wavelets. We start by 
noting that the diffusion wavelets algorithm can be used to
compute a compressed representation of the Green function of the graph.  Therefore, 
we can compute multiscale embeddings that optimize commute times with the scaling 
functions produced at each scale. For large networks, 
the performance can be improved by replacing the orthogonalization process 
with a truncated Singular Value Decomposition (SVD)
for dimension reduction. In this case, we use singular 
vectors as embeddings, in place of the scaling functions.  
We demonstrate the efficacy of this method for data clustering and multi-label
classification on several examples, as well as its superior performance compared with  
methods in terms of speed and accuracy.

## On UMAP's True Loss Function

### TL;DR

We derive UMAP's true loss function and show that UMAP's high-dimensional similarities are not important.

### Abstract

UMAP has supplanted $t$-SNE as state-of-the-art for visualizing high-dimensional datasets in many disciplines, but the reason for its success is not well understood. In this work, we investigate UMAP's sampling based optimization scheme in detail. We derive UMAP's effective loss function in closed form and find that it differs from the published one. As a consequence, we show that UMAP does not aim to reproduce its theoretically motivated high-dimensional UMAP similarities. Instead, it tries to reproduce  similarities that only encode the shared $k$ nearest neighbor graph, thereby challenging the previous understanding of UMAP's effectiveness. Alternatively, we consider the implicit balancing of attraction and repulsion due to the negative sampling to be key to UMAP's success. We corroborate our theoretical findings on toy and single cell RNA sequencing data.

## Enabling Fast Differentially Private SGD via Just-in-Time Compilation and Vectorization

### TL;DR

We show that low-level language features can dramatically speed up differentially private stochastic gradient descent.

### Abstract

A common pain point in differentially private machine learning is the significant runtime overhead incurred when executing Differentially Private Stochastic Gradient Descent (DPSGD), which may be as large as two orders of magnitude. We thoroughly demonstrate that by exploiting powerful language primitives, including vectorization, just-in-time compilation, and static graph optimization, one can dramatically reduce these overheads, in many cases nearly matching the best non-private running times. These gains are realized in two frameworks: one is JAX, which provides rich support for these primitives through the XLA compiler. We also rebuild core parts of TensorFlow Privacy, integrating more effective vectorization as well as XLA compilation, granting significant memory and runtime improvements over previous release versions. Our proposed approaches allow us to achieve up to 50x speedups compared to the best alternatives.


## A Dependency Parsing and Graph Convolutional Networks based Approach to Keyword Extraction with Patent Application

### TL;DR

None

### Abstract

Patent data is commonly used for industrial technology analysis. However, applicants are not required to provide keywords in a patent as those in an academic paper, which poses a great obstacle in accurately extracting technical information from patent data, clustering patents with similar technical topics and analyzing development trends of certain technologies. In addition, due to the unique linguistic style and text expression of patents, the general keyword extraction methods encountered difficulties with low precision and recall rate when applied to patent texts. In this paper, we proposed an unsupervised keyword extraction method for patent data based on dependency parsing (dp) and Graph Convolutional Networks (GCN) with optimized the stopwords list according to the specific vocabulary of patent text. Through model validation on patent dataset and comparative evaluation with commonly used TextRank, Pagerank and TF-IDF methods, it is confirmed that the proposed dp-GCN method has a significant improvement over the general methods with the application for technical keyword extraction of patent information.

## Learning Networked Linear Dynamical Systems under Non-white Excitation

### TL;DR

None

### Abstract

We consider a networked linear dynamical system with $p$ agents/nodes. We study the problem of learning the underlying graph of interactions/dependencies from observations of the nodal trajectories over a time-interval $T$. We present a regularized non-casual consistent estimator for this problem and analyze its sample complexity over two regimes: (a) where the interval $T$ consists of $n$ i.i.d. observation windows of length $T/n$ (restart and record), and (b) where $T$ is one continuous observation window. Using the theory of $M$-estimators, we show that the estimator recovers the underlying interactions, in either regime, in a time-interval that is logarithmic in the system size $p$. To the best of our knowledge, this is the first work to analyze the sample complexity of learning linear dynamical systems \emph{driven by not-white wide-sense stationary (WSS) noise}.

## Learning Neural Causal Models with Active Interventions

### TL;DR

None

### Abstract

Discovering causal structures from data is a challenging inference problem of fundamental importance in all areas of science. The appealing scaling properties of neural networks have recently led to a surge of interest in differentiable neural network-based methods for learning causal structures from data. So far differentiable causal discovery has focused on static datasets of observational or interventional origin. In this work, we introduce an active intervention-targeting mechanism which enables a quick identification of the underlying causal structure of the data-generating process. Our method significantly reduces the required number of interactions compared with random exploration and is applicable for both discrete and continuous optimization formulations of learning the underlying directed acyclic graph (DAG) from data. We examine the proposed method across a wide range of settings and demonstrate superior performance on multiple benchmarks from simulated to real-world data.

## Systematic Generalization with Edge Transformers

### TL;DR

We propose a new model that is inspired by transformers and logical reasoning. The proposed model generalizes more systematically.

### Abstract

Recent research suggests that systematic generalization in  language understanding remains a challenge for state-of-the-art neural models such as Transformers and Graph Neural Networks. To tackle this challenge, we  propose the Edge Transformer, a new model that combines inspiration from  Transformers and rule-based symbolic AI. The first key idea in Edge  Transformers is to associate vector states with every edge, that is, with  every pair of input nodes as opposed to just every node as is  done in the Transformer model. The second major innovation is a triangular attention mechanism that updates edge  representations in a way  that is inspired by the concept of term unification from  logic programming. We evaluate Edge Transformer on systematic generalization  benchmarks in relational reasoning and dependency parsing. In both settings, the Edge Transformer outperforms Relation-aware  Transformer and classical Transformer baselines.

## KB-VLP: Knowledge Based Vision and Language Pretraining

### TL;DR

We introduce a novel vision-language pre-training method, Knowledge Based Vision and Language Pretraining (KB-VLP), for vision-language tasks and uses knowledge graph embeddings.

### Abstract

Transformer-based pretraining techniques have achieved impressive performance on learning cross-model representations for various multi-modality tasks. However, most off-the-shelf models do not take advantage of commonsense knowledge and logical reasoning that are crucial to many real-world tasks. To this end, we introduce a novel pretraining approach - Knowledge Based Vision and Language Pretraining (KB-VLP) - which uses knowledge graph embeddings extracted from text and detected object tags as additional input to enhance the semantic alignment learning and knowledge-aware representation of current models, and improve models' generalization, and knowledge-awareness. KB-VLP is pretrained on a large image-text corpus and automatically extracted knowledge embeddings, and then finetuned on several downstream vision-language tasks. Experiments show that KB-VLP significantly improve the performance on VQA, GQA, NLVR2 and OKVQA comparing with the baselines.

## Spatial Online Learning

### TL;DR

None

### Abstract

Online learning has been widely studied as a paradigm for sequential learning and decision making. Typical cases include time series prediction, games, and adaptive data analysis. Instead of learning the wholesome input of data at once, online learning estimates the label in a streaming, adaptive fashion. In many research areas, including geo-statistics, statistical physics, and social network mining, implicit and explicit geometry exists and 1-dimensional sequence learning became insufficient. We adapt online learning to spatially-discrete random fields (DRF for short) in this work. We presented the learn-ability guarantee of Random Graph Process (RGP for short), a subclass of directed DRF, by upper-bounding the online empirical process with structural Rademacher complexity, metric entropies, and VC-combinatorics. At last, two different regimes,  Independent  and Uniform Ergodic RGPs' learnabilities are discussed on binary online classification.

## Integer Programming to Discover Cyclic Causal Models with Latent Variables

### TL;DR

None

### Abstract

Recent advances in graphical approaches to causality have opened new opportunities to learn the causal relations among variables from observational data. In this work, we propose a new method based on integer programming for causal structure discovery that allows for both unmeasured confounders and feedback cycles. Our method takes as input observational data over a set of variables, and returns a graph in which causal relations are specified by directed edges. There are very few extant methods that consider this extremely general search space, and those that do, do not scale well. In contrast, our approach enables us to solve instances in minutes that are intractable for current state-of-the-art methods. We demonstrate our method using the US Census data from the influential study on the causal relationship between education and income by Angrist and Krueger (1991).

## Breaking Down Questions for Outside-Knowledge VQA

### TL;DR

Breaking down visual questions to better integrate external knowledge for knowledge-based VQA

### Abstract

While general Visual Question Answering (VQA) focuses on querying visual content within an image, there is a recent trend towards Knowledge-Based VQA (KB-VQA) where a system needs to link some aspects of the question to different types of knowledge beyond the image, such as commonsense concepts and factual information. To address this issue, we propose a novel approach that passes knowledge from various sources between different pieces of semantic content in the question. Questions are first segmented into several chunks, and each segment is used as a key to retrieve knowledge from ConceptNet and Wikipedia. Then, a graph neural network, taking advantage of the question's syntactic structure, integrates the knowledge for different segments to jointly predict the answer. Our experiments on the OK-VQA dataset show that our approach achieves new state-of-the-art results. 

## What does Transformer learn about source code?

### TL;DR

In the field of source code processing, We propose the aggregated attention score and graph, a method to investigate the structural information learned by the transformer.

### Abstract

In the field of source code processing, the transformer-based representation models have shown great powerfulness and have achieved state-of-the-art (SOTA) performance in many tasks. Although the transformer models process the sequential source code, pieces of evidence show that they may capture the structural information (e.g., in the syntax tree, data flow, control flow, etc.) as well. We propose the aggregated attention score, a method to investigate the structural information learned by the transformer. We also put forward the aggregated attention graph, a new way to extract program graphs from the pre-trained models automatically. We measure our methods from multiple perspectives. Furthermore, based on our empirical findings, we use the automatically extracted graphs to replace those ingenious manual-designed graphs in the Variable Misuse task. Experimental results show that the semantic graphs we extracted automatically are greatly meaningful and effective, which provide a new perspective for us to understand and use the information contained in the model.

## How does a Neural Network's Architecture Impact its Robustness to Noisy Labels?

### TL;DR

We provide a formal framework connecting the robustness of a network to the alignments between its architecture and target/noise functions. 

### Abstract

Noisy labels are inevitable in large real-world datasets.  In this work, we explore an area understudied by previous works --- how the network's architecture impacts its robustness to noisy labels. We provide a formal framework connecting the robustness of a network to the alignments between its architecture and target/noise functions. Our framework measures a network's robustness via the predictive power in its representations --- the test performance of a linear model trained on the learned representations using a small set of clean labels. We hypothesize that a network is more robust to noisy labels if its architecture is more aligned with the target function than the noise. To support our hypothesis, we provide both theoretical and empirical evidence across various neural network architectures and different domains. We also find that when the network is well-aligned with the target function, its predictive power in representations could improve upon state-of-the-art (SOTA) noisy-label-training methods in terms of test accuracy and even outperform sophisticated methods that use clean labels.

## Object DGCNN: 3D Object Detection using Dynamic Graphs

### TL;DR

None

### Abstract

3D object detection often involves complicated training and testing pipelines, which require substantial domain knowledge about individual datasets. Inspired by recent non-maximum suppression-free 2D object detection models, we propose a 3D object detection architecture on point clouds. Our method models 3D object detection as message passing on a dynamic graph, generalizing the DGCNN framework to predict a set of objects. In our construction, we remove the necessity of post-processing via object confidence aggregation or non-maximum suppression. To facilitate object detection from sparse point clouds, we also propose a set-to-set distillation approach customized to 3D detection. This approach aligns the outputs of the teacher model and the student model in a permutation-invariant fashion, significantly simplifying knowledge distillation for the 3D detection task. Our method achieves state-of-the-art performance on autonomous driving benchmarks. We also provide abundant analysis of the detection model and distillation framework. 

## Learning Semantic Representations to Verify Hardware Designs

### TL;DR

Learning hardware design representations to generate tests that brings down time from a night to few seconds and scales upto practical industrial designs

### Abstract

We introduce Design2Vec, a representation learning approach to learn semantic abstractions of hardware designs at the Register Transfer Level (RTL) for scaling the complexity of design verification. The key idea of our approach is to design a Graph convolution based neural architecture that embeds RTL syntax and semantics and pretrain it to learn the design state space. We train the model on the task of predicting coverage points in the design, given an input parameterized test. Our experimental results demonstrate that Design2Vec outperforms several baseline approaches that do not incorporate the RTL semantics and it can be used to generate instantaneous coverage predictions compared to nightly simulation times. We then present an approach to use Design2Vec  to automatically generate new inputs for unseen coverage locations in the design. Moreover, the tests generated using Design2Vec result in coverage of design points that are difficult to cover for design verification experts using the current manual practices in test generation. We demonstrate that Design2Vec is able to scale to the a contemporary industrial scale chip design.

## Learning Representations for Sub-Symbolic Reasoning

### TL;DR

None

### Abstract

Neuro-symbolic methods integrate neural architectures, knowledge representation and reasoning. However, they have been struggling at both dealing with the intrinsic uncertainty of the observations and scaling to real world applications.
This paper presents Relational Reasoning Networks (R2N), a novel end-to-end model that performs relational reasoning in the latent space of a deep learner architecture, where the representations of constants, ground atoms and their manipulations are learned in an integrated fashion.
Unlike flat architectures like Knowledge Graph Embedders, which can only represent  relations between entities, R2Ns define an additional computational structure, accounting for higher-level relations among the ground atoms.
The considered relations can be explicitly known, like the ones defined by logic formulas, or defined as unconstrained correlations among groups of ground atoms. 
R2Ns can be applied to purely symbolic tasks or as a neuro-symbolic platform to integrate learning and reasoning in heterogeneous problems with both symbolic and feature-based represented entities. The proposed model bridges the gap between previous neuro-symbolic methods that have been either limited in terms of scalability or expressivity.
The proposed methodology is shown to achieve state-of-the-art results in different experimental settings.

## What Robot do I Need? Fast Co-Adaptation of Morphology and Control using Graph Neural Networks

### TL;DR

We present a new approach for the co-adaptation of the number of joints, design parameters and behaviour of agents using a minimal amount of simulations/manufacturing cycles.

### Abstract

The co-adaptation of robot morphology and behaviour becomes increasingly important with the advent of fast 3D-manufacturing methods and efficient deep reinforcement learning algorithms. A major challenge for the application of co-adaptation methods to the real world is the simulation-to-reality-gap due to model and simulation inaccuracies. However, prior work focuses primarily on the study of evolutionary adaptation of morphologies exploiting analytical models and (differentiable) simulators with large population sizes, neglecting the existence of the simulation-to-reality-gap and the cost of manufacturing cycles in the real world. 
This paper presents a new approach combining classic high-frequency deep neural networks with computational expensive Graph Neural Networks for the data-efficient co-adaptation of agents with varying numbers of degrees-of-freedom. 
Evaluations in simulation show that the new method can co-adapt agents within such a limited number of production cycles by efficiently  combining design optimization with offline reinforcement learning, that it allows for the direct application to real-world co-adaptation tasks in future work.

## A Biologically Interpretable Graph Convolutional Network to Link Genetic Risk Pathways and Imaging Phenotypes of Disease

### TL;DR

Biologically Informed Imaging-Genetics

### Abstract

We propose a novel deep neural network for whole-genome imaging-genetics. Our genetics module uses hierarchical graph convolution and pooling operations that mimic the organization of a well-established gene ontology to embed subject-level data into a latent space. The ontology implicitly tracks the convergence of genetic risk across biological pathways, and an attention mechanism automatically identifies the salient edges in our network. We couple the imaging and genetics data using an autoencoder and predictor, which couples the latent embeddings learned for each modality. The predictor uses these embeddings for disease diagnosis, while the decoder regularizes the model. For interpretability, we implement a Bayesian feature selection strategy to extract the discriminative biomarkers of each modality. We evaluate our framework on a population study of schizophrenia that includes two functional MRI (fMRI) paradigms and gene scores derived from Single Nucleotide Polymorphism (SNP) data. Using 10-fold cross-validation, we show that our model achieves better classification performance than the baselines. In an exploratory analysis, we further show that the biomarkers identified by our model are reproducible and closely associated with deficits in schizophrenia.

## Well-classified Examples are Underestimated in  Classification with Deep Neural Networks

### TL;DR

In this paper, we theoretically and empirically show that well-classified examples are helpful for improving deep classification models.

### Abstract

The conventional wisdom behind learning deep classification models focuses on bad-classified examples and ignores well-classified examples that are far from the decision boundary. For instance, when training with cross-entropy loss, examples with higher likelihoods (i.e., well-classified examples) contribute smaller gradients in back-propagation. However, we theoretically point out that this common practice underestimates well-classified examples during learning and hinders the further improvements of models that perform well. To counteract this deficiency, we propose to reward the well-classified examples with additive bonuses to up-weight their contribution to the learning. Our work theoretically addresses the gradient vanishing issue on well-classified examples and is empirically verified by significant performance improvement on diverse tasks, including image classification, graph classification, and machine translation. Furthermore, we demonstrate that our method mitigates the model’s vulnerability to adversarial attack and benefits its classification on rare classes.

## Subgoal Search For Complex Reasoning Tasks

### TL;DR

We propose a hierarchical search method for complex reasoning domains, based on learned subgoal generator.

### Abstract

Humans excel in solving complex reasoning tasks through a mental process of moving from one idea to a related one. Inspired by this, we propose Subgoal Search (SubS) method. Its key component is a learned subgoal generator, an analog of the human intuition, that produces a diversity of subgoals that are both achievable and closer to the solution. Using subgoals reduces the search space and induces a high-level search graph suitable for efficient planning. In this paper, we implement SubS using a transformer-based subgoal module coupled with the classical best-first search framework. We show that a simple approach of generating $k$-th step ahead subgoals is surprisingly efficient on three challenging domains: two popular puzzle games, Sokoban and the Rubik's Cube, and an inequality proving benchmark INT. SubS achieves strong results including state-of-the-art on INT within a modest computational budget.

## Neuron with Steady Response Leads to Better Generalization

### TL;DR

Neuron Steadiness Regularization: an effective and efficient regularization method with extensive experimental evidence on MLPs, CNNs and GNNs.

### Abstract

Regularization can mitigate the generalization gap between training and inference by introducing inductive bias. Existing works have already proposed various inductive biases from diverse perspectives. However, to the best of our knowledge, none of them explores inductive bias from the perspective of class-dependent response distribution of individual neurons. In this paper, we conduct a substantial analysis of the characteristics of such distribution. Based on the analysis results, we articulate the Neuron Steadiness Hypothesis: the neuron with similar responses to instances of the same class leads to better generalization. Accordingly, we propose a new regularization method called Neuron Steadiness Regularization to reduce neuron intra-class response variance. We conduct extensive experiments on Multilayer Perceptron, Convolutional Neural Network, and Graph Neural Network with popular benchmark datasets of diverse domains, which show that our Neuron Steadiness Regularization consistently outperforms the vanilla version of models with significant gain and low additional overhead. 

## Detecting by Dissecting: Using Persistent Homology to catch Adversarial Examples in Deep Nets

### TL;DR

We develop a new detection method and a better understanding of adversarial examples in deep nets by tracking the path of inputs within the net.

### Abstract

As adversarial examples raise crucial safety issues in Deep Learning, the reasons for the vulnerability of neural networks (NNs), while under study, remain largely obscure. In this paper, we investigate how the problem of adversarial examples is related to over-parametrization of NNs. Dissecting the topological structure of the activation graph produced as input data goes through the different layers of a NN, we capture discriminative features based on Persistent Homology, to differentiate between clean and adversarial examples. Our contribution is twofold. First, we propose a novel kernel-based detection method that uses the persistence diagrams as features. Our method shows state-of-the-art or better detection performance on a variety of datasets and attacks. Second, we provide several theoretical and empirical results to show that under-optimized edges in the NNs have a responsibility in their vulnerability. The code is made available for reproducibility.

## Nearest Neighbor Link Prediction

### TL;DR

None

### Abstract

Link prediction is an important and challenging problem over graph structured data. For both signed and unsigned link prediction, we introduce $k$-nearest-neighbor link prediction ($k$NN-LP), an universally applicable method to improve the performance of a pre-trained link prediction (LP) model by linearly interpolating it with a $k$-nearest-neighbor ($k$NN) model. The $k$NN-LP model performs on the representation space learned from the pre-trained LP model and requires no additional training. We further propose a novel link prediction model SPR for signed graphs, which preserves the Signed Proximity and Relation information in such graphs. Experimental results demonstrate that SPR achieves the state-of-the-art performance in signed graphs, and $k$NN-LP consistently improves link prediction methods by a significant margin on both signed and unsigned link prediction tasks. Our code is available at https://github.com/kNN-LP/kNN-LP.

## Causally-Regularized Collaborative Filtering for OOD-Generalization in  Recommender System

### TL;DR

This paper propose a Causally-Regularized Collaborative Filtering  for OOD-Generalization in  recommender system

### Abstract

Recent years have witnessed the tremendous development of recommender system owing to the advanced capability of modeling user behavior. However, the extreme sparsity and noise in user-item interactions data cause the recommender system vulnerable to subtle distribution shift, especially under the setting of implicit feedback. Inspired by the recent literature, we propose to make a thorough analysis of performance from the viewpoint of out-of-distribution generalization. To improve the out-of-distribution generalization of recommender system, we incorporate the knowledge of causal graph into our framework, consisting invariant preference learning via an auxiliary DAG regularizer and anti-preference negative sampling. To this end, we integrate these two components into a unified architecture called Collaborative Filtering with Invariant Preference. Extensive experimental results from both simulation and real scenarios show that our approach surpasses the benchmark models significantly,  in both out-of-sample and selection bias cases.

## Are We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning

### TL;DR

We present a meta-review of evaluation failures across subfields of machine learning, finding surprisingly consistent failure modes.

### Abstract

Many subfields of machine learning share a common stumbling block: evaluation.  Advances in machine learning often evaporate under closer scrutiny or turn out to be less widely applicable than originally hoped.  We conduct a meta-review of 107 survey papers from natural language processing, recommender systems, computer vision, reinforcement learning, computational biology, graph learning, and more, organizing the wide range of surprisingly consistent critique into a concrete taxonomy of observed failure modes. Inspired by measurement and evaluation theory, we divide failure modes into two categories: internal and external validity. Internal validity issues pertain to evaluation on a learning problem in isolation, such as improper comparisons to baselines or overfitting from test set re-use. External validity relies on relationships between different learning problems, for instance, whether progress on a learning problem translates to progress on seemingly related tasks. Based on our taxonomy and case studies around specific examples, we provide concrete recommendations for improving evaluation practices in machine learning and introduce new areas for investigation.

## Generalized Gumbel-Softmax Gradient Estimator for Generic Discrete Random Variables

### TL;DR

This paper proposes a stochastic gradient estimator for general class of discrete distributions.

### Abstract

Estimating the gradients of stochastic nodes, which enables the gradient descent optimization on neural network parameters, is one of the crucial research questions in the deep generative modeling community. For discrete distributions, gradient estimators of Bernoulli and categorical random variables are mainly explored, for example, Gumbel-Softmax reparameterization trick. Meanwhile, other discrete distribution cases such as the Poisson, geometric, binomial, multinomial, negative binomial, etc. have not been explored. This paper proposes a general version of the Gumbel-Softmax estimator, which utilizes the truncation of discrete random variables, the Gumbel-Softmax trick, and a special form of linear transformation. The proposed estimator is able to reparameterize generic discrete distributions, not restricted to the Bernoulli and the categorical, and it enables learning on a large-scale stochastic computational graph. Our experiments consist of (1) synthetic examples and applications on VAE, which show the efficacy of our methods; and (2) topic models, which demonstrate the value of the proposed estimation in practice.

## Learning Document Graphs for Image Manipulation Detection

### TL;DR

State-of-the-art splice detection models fail when applied to a common use case, images of digital documents. We propose a graph neural network-based model for robust image manipulation detection.

### Abstract

Detecting manipulations in images is becoming increasingly important for combating misinformation and forgery. While recent advances in computer vision have lead to improved methods for detecting spliced images, most state-of-the-art methods fail when applied to images of digital documents. We propose a deep-learning method for detecting manipulations in images of documents which leverages the unique structured nature of these images in comparison with those of natural scenes. Specifically, we re-frame the classic image splice detection problem as a node classification problem, in which Optical Character Recognition (OCR) bounding boxes form nodes and edges are added according to an text-specific distance heuristic. We propose a system composed of a Variational Autoencoder (VAE)-based embedding algorithm and a graph neural network with attention, trained end-to-end for robust manipulation detection. Our proposed model outperforms both a state-of-the-art image splice detection method and a document-specific method

## A Data-Centric Optimization Framework for Machine Learning

### TL;DR

We present a flexible, interactive DNN compiler and general-purpose performance optimization that works on a variety of networks.

### Abstract

Rapid progress in deep learning is leading to a diverse set of quickly changing models, with a dramatically growing demand for compute. However, as frameworks specialize optimization to patterns in popular networks, they implicitly constrain novel and diverse models that drive progress in research. 
We empower deep learning researchers by defining a flexible and customizable Python-based pipeline for optimizing arbitrary deep neural networks. The pipeline begins with standard networks in PyTorch or ONNX and transforms computation through progressive lowering. We define four levels of general-purpose transformations, from local intra-operator optimizations to global data movement reduction. These operate on a data-centric graph intermediate representation that expresses computation and data movement at all levels of abstraction, including expanding basic operators such as convolutions to their underlying computations. 
Every part of the pipeline is extensible and can be tuned interactively. We demonstrate optimizations leading to performance improvements in YOLOv4, BERT, and never-before-seen optimizations in EfficientNet.

## Towards Sharper Generalization Bounds for Structured Prediction

### TL;DR

None

### Abstract

In this paper, we investigate the generalization performance of structured prediction learning and obtain state-of-the-art generalization bounds. Our analysis is based on factor graph decomposition of structured prediction algorithms, and we present novel margin guarantees from three different perspectives: Lipschitz continuous, smoothness, and the space capacity condition. In the Lipschitz continuous scenario, we improve the square-root dependency on the cardinality of the label set of existing bounds to a logarithmic dependency. In the smoothness scenario, we provide generalization bounds that are not only a logarithmic dependency on the cardinality of the label set but a faster convergence rate of order $\mathcal{O}(\frac{1}{n})$ w.r.t. the sample size $n$. In the space capacity scenario, generalization bounds not related to the cardinality of the label set and with faster convergence rates than $\mathcal{O}(\frac{1}{\sqrt{n}})$ are presented. In each scenario, applications are provided to suggest that these conditions are easy to be satisfied. 

## Snowflake: Scaling GNNs to high-dimensional continuous control via parameter freezing

### TL;DR

We present a parameter-freezing method for training graph neural networks, enabling strong performance and zero-shot transfer for high-dimensional locomotion control tasks, which is not possible using existing methods.

### Abstract

Recent research has shown that graph neural networks (GNNs) can learn policies for locomotion control that are as effective as a typical multi-layer perceptron (MLP), with superior transfer and multi-task performance. However, results have so far been limited to training on small agents, with the performance of GNNs deteriorating rapidly as the number of sensors and actuators grows. A key motivation for the use of GNNs in the supervised learning setting is their applicability to large graphs, but this benefit has not yet been realised for locomotion control. We show that poor scaling in GNNs is a result of increasingly unstable policy updates, caused by overfitting in parts of the network during training. To combat this, we introduce Snowflake, a GNN training method for high-dimensional continuous control that freezes parameters in selected parts of the network. Snowflake significantly boosts the performance of GNNs for locomotion control on large agents, now matching the performance of MLPs while offering superior transfer properties.

## Automatic Symmetry Discovery with Lie Algebra Convolutional Network

### TL;DR

Lie algebra simplify and unify encoding of continuous groups into neural nets, and relate them to models in physics  

### Abstract

Existing equivariant neural networks for continuous groups require discretization or group representations. All these approaches require detailed knowledge of the group parametrization and cannot learn entirely new symmetries.  We propose to work with the Lie algebra (infinitesimal generators) instead of the Lie group. Our model, the Lie algebra convolutional network (L-conv) can learn potential symmetries and does not require discretization of the group. We show that L-conv can serve as a building block to construct any group equivariant architecture. We discuss how CNNs and Graph Convolutional Networks are related to and can be expressed as L-conv with appropriate groups. We also derive the MSE loss for a single L-conv layer and find a deep relation with Lagrangians used in physics, with some of the physics aiding in defining generalization and symmetries in the loss landscape. Conversely, L-conv could be used to propose more general equivariant ansatze for scientific machine learning.

## Learning Modular Simulations for Homogeneous Networks

### TL;DR

None

### Abstract

Complex systems are often decomposed into modular subsystems for engineering tractability. Although various equation based white-box modeling techniques make use of such structure, learning based methods have yet to incorporate these ideas broadly.
We present a modular simulation framework for modeling homogeneous multibody dynamical systems, which combines ideas from graph neural networks and neural differential equations. Each module is responsible for simulating the corresponding dynamical subsystem. Full simulation of the composite system is orchestrated via spatio-temporal message passing between these modules. An arbitrary number of modules can be combined to simulate systems of a wide variety of coupling topologies. We evaluate our framework on a variety of systems and show that message passing allows coordination between multiple modules over time for accurate predictions and in certain cases, enables zero-shot generalization to new system configurations. Furthermore, we show that our models can be transferred to new system configurations with lower data requirement and training effort, compared to those trained from scratch.

## Prequential MDL for Causal Structure Learning with Neural Networks

### TL;DR

None

### Abstract

Learning the structure of Bayesian networks and causal relationships from observations is a common goal in several areas 
of science and technology. We show that the prequential minimum description length principle (MDL) can be used to derive a practical scoring function for Bayesian networks when flexible and overparametrized neural networks are used to model the conditional probability distributions between observed variables. 
MDL represents an embodiment of Occam's Razor and we obtain plausible and parsimonious graph structures without relying on sparsity inducing priors or other regularizers which must be tuned. Empirically we demonstrate competitive results on synthetic and real-world data. The score often recovers the correct structure in the presence of strongly non-linear relationships between variables; a scenario were prior approaches struggle and usually fail. 
Furthermore we discuss how the the prequential score relates to recent work that infers causal structure from the speed of adaptation when the observations come from a source undergoing distributional shift. 

## Alignment Attention by Matching Key and Query Distributions

### TL;DR

Aligning the distributions of key and query to improve the learning of attention-based models

### Abstract

The neural attention mechanism has been incorporated into deep neural networks to achieve state-of-the-art performance in various domains. Most such models use multi-head self-attention which is appealing for the ability to attend to information from different perspectives. This paper introduces alignment attention that explicitly encourages self-attention to match the distributions of the key and query within each head. The resulting alignment attention networks can be optimized as an unsupervised regularization in the existing attention framework. It is simple to convert any models with self-attention, including pre-trained ones, to the proposed alignment attention. On a variety of language understanding tasks, we show the effectiveness of our method in accuracy, uncertainty estimation, generalization across domains, and robustness to adversarial attacks. We further demonstrate the general applicability of our approach on graph attention and visual question answering, showing the great potential of incorporating our alignment method into various attention-related tasks.

## Multi-Agent Reinforcement Learning in Stochastic Networked Systems

### TL;DR

None

### Abstract

We study multi-agent reinforcement learning (MARL) in a stochastic network of agents. The objective is to find localized policies that maximize the (discounted) global reward. In general, scalability is a challenge in this setting because the size of the global state/action space can be exponential in the number of agents. Scalable algorithms are only known in cases where dependencies are static, fixed and local, e.g., between neighbors in a fixed, time-invariant underlying graph. In this work, we propose a Scalable Actor Critic framework that applies in settings where the dependencies can be non-local and stochastic, and provide a finite-time error bound that shows how the convergence rate depends on the speed of information spread in the network.  Additionally, as a byproduct of our analysis, we obtain novel finite-time convergence results for a general stochastic approximation scheme and for temporal difference learning with state aggregation, which apply beyond the setting of MARL in networked systems.

## Learning to Elect

### TL;DR

We demonstrate that set-input neural network architectures are theoretically and empirically suited to represent existing voting rules and discover new ones that maximize various notions of social welfare. 

### Abstract

Voting systems have a wide range of applications including recommender systems, web search, product design and elections. Limited by the lack of general-purpose analytical tools, it is difficult to hand-engineer desirable voting rules for each use case. For this reason, it is appealing to automatically discover voting rules geared towards each scenario. In this paper, we show that set-input neural network architectures such as Set Transformers, fully-connected graph networks and DeepSets are both theoretically and empirically well-suited for learning voting rules. In particular, we show that these network models can not only mimic a number of existing voting rules to compelling accuracy --- both position-based (such as Plurality and Borda) and comparison-based (such as Kemeny, Copeland and Maximin) --- but also discover near-optimal voting rules that maximize different social welfare functions. Furthermore, the learned voting rules generalize well to different voter utility distributions and election sizes unseen during training.

## Local $K$-means: An Efficient Optimization Algorithm And Its Generalization

### TL;DR

An efficient clustering algorithm that only considers local information

### Abstract

Until now, $k$-means is still one of the most popular clustering algorithms because of its simplicity and efficiency, although it has been proposed for a long time.
In this paper, we considered a variant of $k$-means that takes the $k$-nearest neighbor ($k$-NN) graph as input and proposed a novel clustering algorithm called Local K-Means (LKM).
We also developed a general model that unified LKM, KSUMS, and SC, and discussed the connection among them.
In addition, we proposed an efficient optimization algorithm for the unified model. 
Thus, not only LKM but also SC can be optimized with a linear time complexity with respect to the number of samples. 
Specifically, the computational overhead is $O(nk)$, where $n$ and $k$ are denote the number of samples and nearest neighbors, respectively.
Extensive experiments have been conducted on 11 synthetic and 16 benchmark datasets from the literature. 
The effectiveness, efficiency, and robustness to outliers of the proposed method have been verified by the experimental results.

## Domain Adaptation with Vision Transformer

### TL;DR

The improvement in vision Transformer for domain adaptation lies in local texture variation. We propose Frozen Patch Augmentation to fully enhance the data augmentation quality for domain adaptation. 

### Abstract

Recent studies reveal that Transformer achieves remarkable results in diverse vision tasks. However, less attention has been paid to the generalization of the vision Transformer. In this paper, we investigate the generalization ability by applying vision Transformer to domain adaptation. Surprisingly, vision Transformer gains substantial improvement over CNN-based frameworks. By investigating the intrinsic mechanism of the improvement, we find that vision Transformer is robust to the local texture variation, one major cause of the domain gap but rarely investigated in previous study. Meanwhile, we explore effective data augmentation on vision Transformer by disentangling the causal effect between the factors of data augmentation and domain characteristics on a causal graph. According to the backdoor adjustment, we propose Frozen Patch Augmentation by freezing the patch projection layer and performing domain-specific data augmentation. Experiments on various domain adaptation tasks and diverse Transformer frameworks demonstrate promising results and validate the general applicability of our method.

## Improved Dynamic Regret in Distributed Online Convex Optimization

### TL;DR

We propose a novel algorithm for distributed online convex optimization, which improves upon the existing upper bounds on the dynamic regret.

### Abstract

We consider the problem of distributed online optimization, with a group of learners connected via a dynamic communication graph. The goal of the learners is to track the global minimizer of a sum of time-varying loss functions in a distributed manner. We propose a novel algorithm, termed Distributed Online Mirror Descent with Multiple Averaging Decision and Gradient Consensus (DOMD-MADGC), which is based on mirror descent but incorporates multiple consensus averaging iterations over local gradients as well as local decisions. The key idea is to allow the local learners to collect a sufficient amount of global information, which enables them to more accurately approximation the time-varying global loss, so that they can closely track the dynamic global minimizer over time. We show that the dynamic regret of DOMD-MADGC is upper bounded by the path length, which is defined as the cumulative distance between successive minimizers. The resulting bound improves upon the bounds of existing distributed online algorithms and removes the explicit dependence on $T$.

## On the Fundamental Limits of Exact Inference in Structured Prediction

### TL;DR

None

### Abstract

Inference in structured prediction is naturally modeled with a graph, where the goal is to recover the unknown true label for each node given noisy observations corresponding to nodes and edges.
The focus of this paper is on the fundamental limits of exact recovery irrespective of computational efficiency, assuming the generative process proposed by Globerson et al. (2015).
Analyzing the information-theoretic limits is crucial for algorithm evaluation and development.
In this regard, we establish the fundamental limit bounds and show that there exists a gap between the limits and the performance of the currently existent tractable method (Bello and Honorio, 2019), implying the need for further development of algorithms for exact inference.
The fundamental limit we suggest applies to general connected graphs and involves graphical metrics such as the Cheeger constant and the maximum degree.
Finally, we reveal that the sufficient and necessary conditions derived from the limit bounds are tight up to a logarithmic factor for a wide range of graphs.

## Learning a Coreset of a Map for Path Planning

### TL;DR

We propose a new sampling approach to be used for path planning; our sampling approach is based on learning a coreset for a given map.

### Abstract

We introduce a novel approach for coreset based path planning. In our approach, first, we propose a learning algorithm to construct a coreset for a given map to first sample the feasible space and then, we construct a continuous path using only that coreset information. Unlike the traditional sampling (random or grid-based) methods, for each sample point that is saved in our learned coreset, we also learn its distance to the closest obstacle point. We summarize the entire feasible space in terms of overlapping spheres with varying radii. We explain how to utilize that computed coreset information to construct (i) a safe and (ii) short path as a graph. In our experiments, we compare our proposed coreset-based path planning algorithm to rapidly exploring random trees (RRT), its variant RRT* and probabilistic roadmap (PRM) algorithms and demonstrate its benefits over those existing algorithms over sample maps. We will release our code, upon acceptance. 

## Heuristic Pricing Using Machine Learning for Column Generation

### TL;DR

This paper presents a Machine Learning based Heuristic Pricing approach to efficiently tackle the pricing problems for reducing the solving time of column generation approach.

### Abstract

Column Generation (CG) is an effective technique for solving large-scale optimization problems. It starts by solving a subproblem with a subset of columns (i.e., variables) and gradually adds new columns that can improve the current solution to the subproblem. The new columns are generated by repeatedly solving a pricing problem, which is often a bottleneck of the CG approach due to the NP-hardness of the pricing problem. To tackle this, we propose a Machine Learning based Heuristic Pricing (MLHP) approach to efficiently generate useful columns. Our MLHP method leverages an ML model to predict the optimal solution of the pricing problem in each iteration of CG, and generates multiple high-quality solutions (i.e., columns) based on the ML prediction. We demonstrate the efficacy of our MLHP method on the graph coloring problem, and empirically show that it significantly reduces the solving time of CG as compared to other state-of-the-art exact and heuristic pricing methods.

## Self-reconstructive evidential clustering for high-dimensional data

### TL;DR

None

### Abstract

We focus on the problem of clustering high-dimensional data in this paper. Although many clustering algorithms have been proposed to tackle the curse of dimensionality, most of them require prior knowledge of the number of clusters. Besides, these algorithms create only a hard or fuzzy partition without describing the ambiguity and uncertainty in memberships of objects. To address these issues, we propose a novel self-reconstructive evidential clustering algorithm, which visually depicts the distribution of cluster centers on a graph and derives a credal partition in the framework of evidence theory. In our work, correlations between objects are learned by minimizing a loss function involving reconstruction error and related information between dimensions. Our algorithm is shown to be intuitive based on a new proposition about cluster centers and enjoy convergence guarantees during the optimization process. We show the robustness (against uninformative dimensions and outliers) and strong performance of our algorithm via synthetic experiments and real data benchmarks, respectively.

## Linear Deconfounded Score Method: Scoring DAGs with Dense Unobserved Confounding

### TL;DR

We demonstrate that sparse linear Gaussian directed acyclic graph among observed variables may be recovered approximately in the presence of certain types of unobserved confounding and propose a score-based algorithm for this problem.

### Abstract

Unobserved confounding is one of the greatest challenges for causal discovery. The case in which unobserved variables have a widespread effect on many of the observed ones is particularly difficult because most pairs of variables are conditionally dependent given any other subset, rendering the causal effect unidentifiable. In this paper we show that beyond conditional independencies, under the principle of independent mechanisms, unobserved confounding in this setting leaves a statistical footprint in the observed data distribution that allows for disentangling spurious and causal effects. Using this insight, we demonstrate that a sparse linear Gaussian directed acyclic graph among observed variables may be recovered approximately and propose an adjusted score-based causal discovery algorithm that may be implemented with general purpose solvers and scales to high-dimensional problems. We find, in addition, that despite the conditions we pose to guarantee causal recovery, performance in practice is robust to large deviations in model assumptions.

## Neural Algorithmic Reasoners are Implicit Planners

### TL;DR

We study value iteration-based implicit planning methods, discover an algorithmic bottleneck which leaves them vulnerable in low-data scenarios. By performing value iteration-style planing in the latent space, we successfully break this bottleneck.

### Abstract

Implicit planning has emerged as an elegant technique for combining learned models of the world with end-to-end model-free reinforcement learning. We study the class of implicit planners inspired by value iteration, an algorithm that is guaranteed to yield perfect policies in fully-specified tabular environments. We find that prior approaches either assume that the environment is provided in such a tabular form---which is highly restrictive---or infer "local neighbourhoods" of states to run value iteration over---for which we discover an algorithmic bottleneck effect. This effect is caused by explicitly running the planning algorithm based on scalar predictions in every state, which can be harmful to data efficiency if such scalars are improperly predicted. We propose eXecuted Latent Value Iteration Networks (XLVINs), which alleviate the above limitations. Our method performs all planning computations in a high-dimensional latent space, breaking the algorithmic bottleneck. It maintains alignment with value iteration by carefully leveraging neural graph-algorithmic reasoning and contrastive self-supervised learning. Across seven low-data settings---including classical control, navigation and Atari---XLVINs provide significant improvements to data efficiency against value iteration-based implicit planners, as well as relevant model-free baselines. Lastly, we empirically verify that XLVINs can closely align with value iteration.

## An Inductive Learning Approach for Solving Minimum Steiner Tree Problem

### TL;DR

None

### Abstract

   The minimum Steiner tree (MST) problem is a classical NP-hard problem on graphs. In this paper, we develop an inductive learning framework to generate high-quality approximate solutions to MST problems. In the proposed framework, we design a set of powerful features based on local solutions and integrate them into a node classifier to predict which nodes should be contained in the minimum Steiner tree. We construct a minimum spanning tree on the subgraph induced by the candidate nodes. To enhance the solution quality, effective and efficient refinement techniques are applied to improve the Steiner tree. Experiments confirm that our framework gains outstanding performance in fastly delivering high-quality Steiner trees. Moreover, the model scales well on larger graphs with larger terminal sets even it is trained on small graphs with small terminal sets.

## Disentangling Task Transfer Learning in Natural Language

### TL;DR

We propose a fully computational learning approach to model the structure of transfer relations between language tasks. We reveal the task taxonomy graph in latent natural language task space.

### Abstract

Contemporarily, pre-trained language models (e.g., GPT, BERT, and Electra) consistently achieve state-of-the-art results on a wide range of downstream tasks after fine-tuning, a.k.a. transfer learning. What is the reason behind the success of transfer learning in natural language? We investigate this question from two perspectives: the taxonomy in latent language task space and the language representation shift in the transfer stage, under either sparse or dense settings. First, we investigate whether natural language tasks correlated or not. For instance, whether language generation models can answer issues from the question answering task, or vice versa. Definite answers to these questions are possible, indicating the existence of a structure underneath natural language tasks.  Revealing the structure panorama has outstanding values, as it is the concept underlying transfer learning and pre-trained language models. This paper proposes a fully computational learning approach to model the structure of transfer relations between natural language tasks. It approaches by building transfer learning dependencies across a distribution of sixteen natural language tasks in a latent space. The output is a learned computational taxonomic graph for task transfer learning in natural language. Second, we observe the language representation shift in the transfer stage, and study whether the shift leads to the success of transfer learning. We propose a fully computational approach to quantify the shift in language representations and present a geometric explanation for the mechanism of how pre-trained language models work. Finally, the two perspectives unify and corroborate each other, explaining how transfer learning works and help users devise efficient pre-train and fine-tune policies for their use cases. Based on our geometric explanation, we compress ALBERT to the tiniest model size (12 layers with 6M parameters and 24 layers with 8M parameters including embedding size) and achieve more than $11\times$ inference speed up.

## Learning to Compose Visual Relations

### TL;DR

Learning to compose visual relations using energy-based models.

### Abstract

The visual world around us can be concisely described as a structured set of objects and their associated relations. An image of a room may be conjured given only the description of the underlying objects and their associated relations. While there has been significant work on designing deep neural networks which may compose individual objects together, less work has been done on composing the individual relations between objects. A principal difficulty is that while the placement of objects is mutually independent, their relations are entangled and dependent on each other. To circumvent this issue, existing works primarily compose relations by utilizing a holistic encoder, in the form of text or graph. In this work, we instead propose to represent each relation as an unnormalized density (an energy-based model), enabling us to compose separate relations in a factorized manner. We show that such a factorized decomposition allows more faithfully to both generate and edit scenes that have multiple sets of relations. We further show that decomposition enables our model to more effectively understand the underlying relational scene structure.


## Recovering Unbalanced Communities in the Stochastic Block Model With Application to Clustering with a Faulty Oracle

### TL;DR

None

### Abstract

The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. However, our understanding of SBM with unbalanced communities (arguably, more relevant in practice) is still very limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes. Under the KS-threshold conjecture, the tradeoff between the parameters in our algorithm is nearly optimal up to polylogarithmic factors for a wide range of regimes.

As a byproduct, we obtain a time-efficient algorithm with improved query complexity for a clustering problem with a faulty oracle, which improves upon a number of previous work  (Mazumdar and Saha [NIPS 2017], Larsen, Mitzenmacher and Tsourakakis [WWW 2020], Peng and Zhang [COLT 2021]). Under the KS-threshold conjecture, the query complexity of our algorithm is nearly optimal up to polylogarithmic factors. 

## Efficient Online Estimation of Causal Effects by Deciding What to Observe

### TL;DR

None

### Abstract

Researchers often face data fusion problems, where multiple data sources are available, each capturing a distinct subset of variables. While problem formulations typically take the data as given, in practice, data acquisition can be an ongoing process. In this paper, we introduce the problem of deciding, at each time, which data source to sample from. Our goal is to estimate a given functional of the parameters of a probabilistic model as efficiently as possible. We propose online moment selection (OMS), a framework in which structural assumptions are encoded as moment conditions. The optimal action at each step depends, in part, on the very moments that identify the functional of interest. Our algorithms balance exploration with choosing the best action as suggested by estimated moments. We propose two selection strategies: (1) explore-then-commit (ETC) and (2) explore-then-greedy (ETG), proving that both achieve zero asymptotic regret as assessed by MSE. We instantiate our setup for average treatment effect estimation, where structural assumptions are given by a causal graph and data sources include subsets of mediators, confounders, and instrumental variables.

## Reinforcement learning for instance segmentation with high-level priors

### TL;DR

None

### Abstract

Instance segmentation is an important computer vision problem which remains challenging despite impressive recent advances due to deep learning-based methods. Given sufficient training data, fully supervised methods can yield excellent performance, but annotation of ground-truth data remains a major bottleneck, especially for biomedical applications where it has to be performed by domain experts. The amount of labels required can be drastically reduced by using rules derived from prior knowledge to guide the segmentation. However, these rules are in general not differentiable and thus cannot be used with existing methods. Here, we relax this requirement by using stateless actor critic reinforcement learning, which enables non-differentiable rewards. We formulate the instance segmentation problem as graph partitioning and the actor critic predicts the edge weights driven by the rewards, which are based on the conformity of segmented instances to high-level priors on object shape, position or size. The experiments on toy and real datasets demonstrate that we can achieve excellent performance without any direct supervision based only on a rich set of priors.

## Learning Symbolic Interactions for Interpretable State-Space Modeling

### TL;DR

None

### Abstract

The abundance of data affords researchers to pur- sue more powerful computational tools to learn the dynamics of complex system, such as neu- ral networks, engineered systems and social net- works. Traditional machine learning approaches capture complex system dynamics either with dy- namic Bayesian networks and state space models, which is hard to scale because it is non-trivial to prescribe the dynamics with a sparse graph or a system of differential equations; or a deep neu- ral networks, where the distributed representation of the learned dynamics is hard to interpret. In this paper, we will explore the possibility of learn- ing a discrete-event simulation representation of complex system dynamics assuming multivariate normal distribution of the state variables, based on the observation that many complex system dy- namics can be decomposed into a sequence of local interactions, which individually change the system state only minimally but in sequence gen- erate complex and diverse dynamics. Our results show that the algorithm can data-efficiently cap- ture complex network dynamics in several fields with meaningful events.

## Data Augmentation with Manifold Barycenters

### TL;DR

New type of data augmentation based on optimal transport theory and Wasserstein barycenters

### Abstract

The training of Generative Adversarial Networks (GANs) requires a large amount of data, stimulating the development of new data augmentation methods to alleviate the challenge. Oftentimes, these methods either fail to produce enough new data or expand the dataset beyond the original knowledge domain. In this paper, we propose a new way of representing the available knowledge in the manifold of data barycenters. Such a representation allows performing data augmentation based on interpolation between the nearest data elements using Wasserstein distance. The proposed method finds cliques in the nearest-neighbors graph and, at each sampling iteration, randomly draws one clique to compute the Wasserstein barycenter with random uniform weights. These barycenters then become the new natural-looking elements that one could add to the dataset. We apply this approach to the problem of landmarks detection and augment the available landmarks data within the dataset. Additionally, the idea is validated on cardiac data for the task of medical segmentation. Our approach reduces the overfitting and improves the quality metrics beyond the original data outcome and beyond the result obtained with classical augmentation methods.

## Matching a Desired Causal State via Shift Interventions

### TL;DR

We propose two active learning strategies which use shift interventions to determine how to shift a causal system into a desired state.

### Abstract

Transforming a causal system from a given initial state to a desired target state is an important task permeating multiple fields including control theory, biology, and materials science. In causal models, such transformations can be achieved by performing a set of interventions. In this paper, we consider the problem of identifying a shift intervention that matches the desired mean of a system through active learning. We define the Markov equivalence class that is identifiable from shift interventions and propose two active learning strategies that are guaranteed to exactly match a desired mean. We then derive a worst-case lower bound for the number of interventions required and show that these strategies are optimal for certain classes of graphs. In particular, we show that our strategies may require exponentially fewer interventions than the previously considered approaches, which optimize for structure learning in the underlying causal graph. In line with our theoretical results, we also demonstrate experimentally that our proposed active learning strategies require fewer interventions compared to several baselines.

##  NeuroMLR: Robust & Reliable Route Recommendation on Road Networks 

### TL;DR

Predicting the most likely route in a road network via Lipschitz embedding and GCN,  achieving significantly high reachability to the destination and generalize better to unseen data.

### Abstract

Predicting the most likely route from a source location to a destination is a core functionality in mapping services. Although the problem has been studied in the literature, two key limitations remain to be addressed.  First, our study reveals that a significant portion of the routes recommended by existing methods fail to reach the destination. Second, existing techniques are transductive in nature; hence, they fail to recommend routes if unseen roads are encountered at inference time. In this paper, we address these limitations through an inductive algorithm called NeuroMLR. NeuroMLR learns a generative model from historical trajectories by conditioning on three explanatory factors: the current location, the destination, and real-time traffic conditions.  The conditional distributions are learned through a novel combination of Lipschitz embedding with Graph Convolutional Networks (GCN) on historical trajectories. Through in-depth experiments on real-world datasets, we establish that NeuroMLR imparts significant improvement in accuracy over the state of the art. More importantly, NeuroMLR generalizes dramatically better to unseen data and the recommended routes reach the destination with much higher likelihood than existing techniques.

## CANDLE: An Image Dataset for Causal Analysis in Disentangled Representations

### TL;DR

We propose a dataset and two evaluation metrics for causal analysis in disentangled representations along with a methodological improvement over existing methods.

### Abstract

Confounding effects are inevitable in real-world datasets. However, methods that identify and remove confounding are important for various downstream tasks like classification, counterfactual data augmentation, fairness, etc. Confounding analysis on image datasets is even more difficult because there is no clear way to separate causal knowledge in images. We develop an image dataset for Causal ANalysis in DisentangLed rEpresentations(CANDLE), a realistic dataset generated following a causal graph with both observed and unobserved confounders. Our dataset enables the learning of generative models that can respect causal generative factors, and thereby ask counterfactual questions on the dataset to assess causal disentanglement. We also propose two evaluation metrics to measure the level of disentanglement achieved by any model under confounding effects. We provide an improved disentangled representation learning method using weak bounding box-level supervision that is a part of CANDLE. We conduct a comprehensive suite of experiments to empirically analyze the disentanglement capabilities of existing models on CANDLE as well as other existing datasets using the proposed metrics.

## Scalable Intervention Target Estimation in Linear Models

### TL;DR

We propose a consistent and scalable algorithm that recovers intervention targets in linear structural equation models.

### Abstract

This paper considers the problem of estimating the unknown intervention targets in a causal directed acyclic graph from observational and interventional data generated under soft interventions and linear structural equation models (SEMs). Current causal structure learning methods either work with known intervention targets or use hypothesis testing to discover the unknown intervention targets even for linear SEMs, limiting their scalability and sample efficiency. This paper proposes a scalable and efficient algorithm that consistently identifies all intervention targets. The pivotal idea is to estimate the intervention sites from the difference in precision matrices, across observational and interventional datasets, by repeatedly applying them on different subsets of variables. This approach facilitates establishing sample complexity guarantees as well as scalability to larger graphs. The proposed algorithm can be used to also update a given observational Markov equivalence class into the interventional Markov equivalence class. Consistency, Markov equivalency, and sample complexity are established analytically. Finally, simulation results on both real and synthetic data demonstrate the gains of the proposed approach for scalable causal structure recovery.

## Stochastic Optimization of Areas Under Precision-Recall Curves with Provable Convergence

### TL;DR

None

### Abstract

Areas under ROC (AUROC) and precision-recall curves (AUPRC) are common metrics for evaluating classification performance for imbalanced problems. Compared with AUROC, AUPRC is a more appropriate metric for highly imbalanced datasets. While stochastic optimization of AUROC has been studied extensively, principled stochastic optimization of AUPRC has been rarely explored. In this work, we propose a principled technical method to optimize AUPRC for deep learning.  Our approach is based on maximizing the averaged precision (AP), which is an unbiased point estimator of AUPRC. We cast the objective into a sum of {\it dependent compositional functions} with inner functions dependent on random variables of the outer level. We propose efficient adaptive and non-adaptive stochastic algorithms with {\it provable convergence guarantee under mild conditions} by leveraging recent advances in stochastic compositional optimization. Extensive experimental results on image and graph datasets demonstrate that our proposed method outperforms prior methods on imbalanced problems in terms of AUPRC. To the best of our knowledge, our work represents the first attempt to optimize AUPRC with provable convergence.

## Deep Bayesian Optimization for High-Dimensional Inputs and Auxiliary Information

### TL;DR

We demonstrate Bayesian optimization using neural networks on a number of real-world scientific problems that have high-dimensional inputs and auxiliary information.

### Abstract

Bayesian optimization (BO) is a popular paradigm for global optimization of expensive black-box functions, but there are many domains where the function is not completely black-box. The data may have some known structure, e.g. symmetries, and the data generation process can yield useful intermediate or auxiliary information in addition to the value of the optimization objective. However, surrogate models traditionally employed in BO, such as Gaussian Processes (GPs), scale poorly with dataset size and struggle to incorporate known structure or auxiliary information. Instead, we propose performing BO on complex, structured problems by using Bayesian Neural Networks (BNNs), a class of scalable surrogate models that have the representation power and flexibility to handle structured data and exploit auxiliary information. We demonstrate BO on a number of realistic problems in physics and chemistry, including topology optimization of photonic crystal materials using convolutional neural networks, and chemical property optimization of molecules using graph neural networks. On these complex tasks, we show that BNNs often outperform GPs as surrogate models for BO in terms of both sampling efficiency and computational cost.

## HyperND: Nonlinear feature propagation in hypergraphs 

### TL;DR

A nonlinear diffusion method for semi-supervised learning on hypergraphs

### Abstract

Hypergraphs are a common model for multiway relationships in data, and hypergraph semi-supervised learning is the problem of assigning labels to all nodes in a hypergraph, given labels on just a few nodes. Diffusions and label spreading are classical techniques for semi-supervised learning in the graph setting, and there are some standard ways to extend them to hypergraphs. However, these methods are linear models, and do not offer an obvious way of incorporating node features for making predictions. Here, we develop a nonlinear diffusion process on hypergraphs that spreads both features and labels following the hypergraph structure, which can be interpreted as a hypergraph equilibrium network. Even though the process is nonlinear, we show global convergence to a unique limiting point for a broad class of nonlinearities, which is the global optimum of a interpretable, regularized semi-supervised learning loss function. The limiting point serves as a node embedding from which we make predictions with a linear model. Our approach is much more accurate than several hypergraph neural networks, and also takes less time to train.

## Landmark-Guided Subgoal Generation in Hierarchical Reinforcement Learning

### TL;DR

We train a high-level policy to generate a subgoal guided by landmarks, promising states to explore, in hierarchical reinforcement learning.

### Abstract

Goal-conditioned hierarchical reinforcement learning (HRL) has shown promising results for solving complex and long-horizon RL tasks. However, the action space of high-level policy in the goal-conditioned HRL is often large, so it results in poor exploration, leading to inefficiency in training. In this paper, we present HIerarchical reinforcement learning Guided by Landmarks (HIGL), a novel framework for training a high-level policy with a reduced action space guided by landmarks, i.e., promising states to explore. The key component of HIGL is twofold: (a) sampling landmarks that are informative for exploration and (b) encouraging the high level policy to generate a subgoal towards a selected landmark. For (a), we consider two criteria: coverage of the entire visited state space (i.e., dispersion of states) and novelty of states (i.e., prediction error of a state). For (b), we select a landmark as the very first landmark in the shortest path in a graph whose nodes are landmarks. Our experiments demonstrate that our framework outperforms prior-arts across a variety of control tasks, thanks to efficient exploration guided by landmarks.

## Networked Restless Multi-Armed Bandits for Mobile Interventions

### TL;DR

Motivated by a broad class of mobile intervention problems, we propose and study restless multi-arm bandits (RMAB) with network effects. 

### Abstract

Motivated by a broad class of mobile intervention problems, we propose and study restless multi-armed bandits (RMAB) with network effects. In our model, arms are partially recharging and connected through a graph, so that pulling one arm also improves the state of neighboring arms, significantly extending the previously studied setting of fully recharging bandits with no network effects. Our model is motivated by mobile intervention applications, where mobile services visit different locations to provide interventions, relevant in a variety of domains including mobile health interventions and food security, in which network effects arise due to regular population movements (such as commuting between home and work). We show that network effects in RMABs induce strong reward coupling that is not accounted for by existing solution methods. We propose a new solution approach for networked RMAB that exploits concavity structure that arises under natural assumptions on the structure of intervention effects. We show that our proposed algorithm is optimal for a large class of networked RMAB settings, and empirically demonstrate that it consistently outperforms state-of-the-art baselines.

## Distributed Machine Learning with Sparse Heterogeneous Data

### TL;DR

None

### Abstract

Motivated by distributed machine learning settings such as Federated Learning, we consider the problem of fitting a statistical model across a distributed collection of heterogeneous data sets whose similarity structure is encoded by a graph topology. Precisely, we analyse the case where each node is associated with fitting a sparse linear model, and edges join two nodes if the difference of their solutions is also sparse. We propose a method based on Basis Pursuit Denoising with a total variation penalty, and provide finite sample guarantees for sub-Gaussian  design matrices. Taking the root of the tree as a reference node, we show that if the sparsity of the differences across nodes is smaller than the sparsity at the root, then recovery is successful with fewer samples than by solving the problems independently, or by using methods that rely on a large overlap in the signal supports, such as the group Lasso. We consider both the noiseless and noisy setting, and numerically investigate the performance of distributed methods based on Distributed Alternating Direction Methods of Multipliers (ADMM) and hyperspectral unmixing.


## Causally Invariant Predictor with Shift-Robustness

### TL;DR

None

### Abstract

This paper proposes an invariant causal prediction that is robust to distribution shifts across domains and maximally reserved the transferable invariant information. 
Based on a causal factorization, we formulate the distribution shift as certain soft interventions to the system that covers a wide range of cases for shift sources. 
Instead of imposing regularizations to force the "invariance", we prove that the conditional expectation intervened by mutable variables can serve as an invariant prediction, which is based on the \emph{do}-operation in causal graph literature that naturally removes the domain-specific information and maintains the invariant dependence.
More importantly, we prove that the proposed prediction owns a desirable min-max property that minimizes the worst-case expected quadratic loss over the considered distributions set.
The empirical learning of the prediction is facilitated by a local causal discovery algorithm, where only the causal directions related to the mutable variables under soft interventions and their descendants need to be identified. 
An intuitive and flexible estimating procedure is provided which allows us to incorporate any supervised learning methods with the regenerated sample of a hypothetical distribution.

## Denoising Social Networks for Social Summarization

### TL;DR

This paper proposes a DSNSum model that  remove noise relations in social networks to improve social media summarization.

### Abstract

Social summarization aims to produce a concise summary for a huge amount of social media short texts (e.g. posts) about a specific topic. Most recent researches, motivated by social theories, use social relationships among posts to remove content redundancy caused by message propagation, while they ignore that social relations are often noisy and unreliable in real-world social networks. These noise relationships can mislead the model when calculating the salience and diversity of posts. In this paper, we propose DSNSum, a novel noise-robust model that improves social summarization from a new perspective: by removing noise relations in social networks. Since there is no golden labels of social relations, we adopt a noising-then-denosing paradigm and design two sociologically motivated noising functions to create synthetic training data by adding noises to original social networks. A denoising graph auto-encoder is then introduced to remove noises and learn reliable representations, which are then used to extract final summaries. Comprehensive experiments on two real-world social media datasets demonstrate that our model outperforms other comparison methods, validating the effectiveness of the proposed model.

## On Joint Learning for Solving Placement and Routing in Chip Design

### TL;DR

None

### Abstract

For its advantage in GPU acceleration and less dependency on human experts, machine learning has been an emerging tool for solving the placement and routing problems, as two critical steps in modern chip design flow. Being still in its early stage, there are several fundamental issues unresolved: scalability, reward design, and end-to-end learning paradigm etc. To achieve end-to-end placement learning, we first propose a joint learning method for the placement of macros and standard cells, by the integration of reinforcement learning with a gradient based optimization scheme. To further bridge the placement with the subsequent routing task, we also develop a joint learning approach via reinforcement learning. One key design in our (reinforcement) learning paradigm involves a multi-view embedding model to encode both global graph level and local node level information of the input macros. Moreover, the random network distillation is devised to encourage exploration. Experiments on public chip design benchmarks show that our method can effectively learn from experience and also provide high-quality intermediate placement for the post standard cell placement, within few hours for training.

## A Unified Analysis of Dynamic Interactive Learning

### TL;DR

We unify the framework for dynamic interactive learning, solve a gap in the bounds, and analyze efficient algorithms

### Abstract

In this paper we investigate the problem of learning combinatorial structures which may change over time.  Previous work by Emamjomeh-Zadeh et al. (2020) introduced dynamics into interactive learning as a way to model evolving user preferences in clustering problems or recommender systems. We provide many useful contributions to this problem.  First, we give a general model inspired by their Drifting Target Model, with a different definition for the transition graph. Our model provides a unified framework to study any type of concept evolution, including the Shifting Target Model and Drifting Target Model (Emamjomeh-Zadeh et al. 2020) with the same query complexity bound and running time guarantees, and we discuss several new transition models as special cases. Using this general model we solve the open problem of closing the gap between the upper and lower bounds on query complexity.  Finally, we study an efficient algorithm where the learner simply follows the feedback at each round, and we provide mistake bounds for low diameter graphs such as clique, star, and general $o(\log n)$ diameter graphs by using a Markov Chain model. 

## Symplectic Adjoint Method for Exact Gradient of Neural ODE with Minimal Memory

### TL;DR

Neural ordinary difference equation consumes large memory or takes a long time to obtain its gradient for training. The adjoint method leveraging a symplectic integrator suppresses the both bottlenecks.

### Abstract

A neural network model of a differential equation, namely, neural ODE, has enabled the learning of continuous-time dynamical systems and probabilistic distributions with high accuracy. The neural ODE uses the same network repeatedly during numerical integration. The memory consumption of the backpropagation algorithm is proportional to the number of uses times the network size. This is true even if a checkpointing scheme divides the computational graph into sub-graphs. Otherwise, the adjoint method obtains a gradient by a numerical integration backward in time. Although this method consumes memory only for a single network use, it requires high computational cost to suppress numerical errors. This study proposes the symplectic adjoint method, which is an adjoint method solved by a symplectic integrator. The symplectic adjoint method obtains the exact gradient (up to rounding error) with memory proportional to the number of uses plus the network size. The experimental results demonstrate that the symplectic adjoint method consumes much less memory than the naive backpropagation algorithm and checkpointing schemes, performs faster than the adjoint method, and is robust to rounding errors.

## Probabilistic Modeling Using Tree Linear Cascades

### TL;DR

None

### Abstract

We study low-dimensional representations of high-dimensional distributions from two perspectives. The first perspective involves a sparse tree-structured regression of a high-dimensional random vector on itself. Such a model corresponds to a particular linear structural equation model or causal model. We call them tree linear cascades. We prove the identifiability of the tree structure and solve the regression problem. The second perspective involves approximating a high-dimensional density with a simpler one that factors according to a tree. After reviewing the classical solution of Chow and Liu to this problem, we connect the two perspectives.  A tree is optimal in the regression if and only if it is optimal for the approximation in the Gaussian case. Finally, we give a result connecting both these perspectives to maximum likelihood estimation with Gaussians. We conclude with a numerical experiment demonstrating the techniques on stock market data.

## Back2Future: Leveraging Backfill Dynamics for Improving Real-time Predictions in Future

### TL;DR

We study the problem of multi-variate backfill for both features and targets and show how to leverage our insights for more general neural framework to improve both model predictions and evaluation

### Abstract

In real-time forecasting, data collection in domains such as public health is a non-trivial and demanding task. Often after data is initially released, it undergoes several revisions later (maybe due to human or technical constraints) - as a result, it may take weeks until the data reaches to a stable value. This so-called ‘backfill’ phenomenon and its effect on model performance has been barely studied in the prior literature. In this paper, we introduce the multi-variate backfill problem in real-time forecasting using COVID-19 as the motivating example. We construct a detailed dataset composed of relevant signals over the past year of the pandemic. We then systematically characterize several patterns in backfill dynamics and leverage our observations into formulating a novel problem and neural framework Back2Future that aims to refines another given model's predictions in real-time. Our extensive experiments demonstrate that our method refines the performance of top models for COVID-19 forecasting, in contrast to non-trivial baselines, yielding on average 18% improvement, enabling us obtain a new SOTA performance. In addition, we also show that our model helps in improving model evaluation as well so that policy-makers can better understand the true accuracy of forecasting models in real-time.

## Fact-Tree Reasoning for N-ary Question Answering over Knowledge Graphs

### TL;DR

None

### Abstract

In the question answering (QA) task, multi-hop reasoning framework has been extensively studied in recent years to perform more efficient and interpretable answer reasoning on the Knowledge Graph (KG). However, multi-hop reasoning is inapplicable for answering n-ary fact questions due to its linear reasoning nature. We discover that there are two feasible improvements: 1) upgrade the basic reasoning unit from entity or relation to fact; and 2) upgrade the reasoning structure from chain to tree. Based on these, we propose a novel fact-tree reasoning framework, through transforming the question into a fact tree and performing iterative fact reasoning on it to predict the correct answer. Through a comprehensive evaluation on the n-ary fact KGQA dataset introduced by this work, we demonstrate that the proposed fact-tree reasoning framework has the desired advantage of high answer prediction accuracy. In addition, we also evaluate the fact-tree reasoning framework on two binary KGQA datasets and show that our approach also has a strong reasoning ability compared with several excellent baselines. This work has direct implications for exploring complex reasoning scenarios and provides a preliminary baseline approach.

## Deep Molecular Representation Learning via Fusing Physical and Chemical Information

### TL;DR

We present a deep molecular representation method that captures and fuses physical and chemical information of molecules.

### Abstract

Molecular representation learning is the first yet vital step in combining deep learning and molecular science. To push the boundaries of molecular representation learning, we present PhysChem, a novel neural architecture that learns molecular representations via fusing physical and chemical information of molecules. PhysChem is composed of a physicist network (PhysNet) and a chemist network (ChemNet). PhysNet is a neural physical engine that learns molecular conformations through simulating molecular dynamics with parameterized forces; ChemNet implements geometry-aware deep message-passing to learn chemical / biomedical properties of molecules. Two networks specialize in their own tasks and cooperate by providing expertise to each other. By fusing physical and chemical information, PhysChem achieved state-of-the-art performances on MoleculeNet, a standard molecular machine learning benchmark. The effectiveness of PhysChem was further corroborated on cutting-edge datasets of SARS-CoV-2.

## Minimizing Polarization and Disagreement in Social Networks via Link Recommendation

### TL;DR

study the optimization problem of recommending $k$ new links to minimize the sum of polarization and disagreement in a social network with $n$ nodes and $m$ edges. 

### Abstract

Individual's opinions are fundamentally shaped  and evolved by their interactions with other people, and social phenomena such as disagreement and polarization are now tightly woven into daily life. The quantification and optimization of these concepts have been the subject of much recent research behind a wealth of high-impact data mining applications. In particular, researchers have addressed the question of how such concepts can be optimized by influencing the opinion of a small number of individuals or by designing the network from scratch.

Here, rather than  a “design-from-scratch” approach or altering the initial opinion, we study the optimization problem of recommending $k$ new links to minimize the sum of polarization and disagreement in a social network with $n$ nodes and $m$ edges.  We show that our objective function of this combinatorial optimization problem is not submodular, although it is monotone. We propose a simple greedy algorithm with a constant-factor approximation that  solves the problem in cubic running time, and we provide  theoretical analysis of the approximation guarantee for the algorithm. To overcome the computation challenge for large networks, we also provide a fast algorithm with computation complexity $\Otil (mk\eps^{-2})$ for any $\eps>0$,  where the $\Otil (\cdot)$ notation suppresses the ${\rm poly} (\log n)$ factors. Extensive experiments on real datasets demonstrate both the efficiency and effectiveness of our algorithms.

## Long-horizon planning via short-horizon contrastive representation learning

### TL;DR

None

### Abstract

While recent advances in reinforcement learning (RL) have enabled artificial agents to directly learn from high-dimensional inputs, it remains a challenge to efficiently plan in long-horizon tasks in environments with sparse rewards. In this work, we propose SoLo  ("Short Horizon Contrastive Learning for Long Horizon Planning"), a method for extracting global structure of an environment from only information that is locally available to the agent.  We construct such structure via a modified contrastive loss and clustering in the contrastive embedding space to build a graph, in order to use distances to cluster centroids as a dense intrinsic reward. In our approach, positive and negative examples for contrastive training are defined as reachable and unreachable states within a short horizon, respectively.  Additionally, we augment the contrastive loss by encouraging closeness of neighbor state embeddings and separation of clusters. In our experiments, we show that our method produces an abstract representation of the original environment that facilitates long-term planning and enables RL agents to solve sparse-reward long-horizon tasks: in challenging sparse-reward environments, only our method is able to obtain a solution policy.

## Text2Face: Text-Driven Face Generation with Geometry and Appearance Control

### TL;DR

Our method is capable of generating high-quality attribute-conditioned facial images from text, and further supports text-guided editing.

### Abstract

In recent years, various techniques have been proposed for text-based human face generation and manipulation.  Such methods, targeting at bridging the semantic gap between text visual contents, provide users with a deft hand to turn ideas to visuals via the interface of text.  However, due to the flexibility of linguistic expressiveness, the mapping from sentences to desired facial images is clearly5many-to-many, bringing about ambiguities during text-to-face generation. To alleviate these ambiguities, we introduce a local-to-global framework with two graph neural networks (one for geometry and one for appearance) embedded in to model the inter-dependency among facial parts. This is based upon our key observation that the geometry and appearance attributes among different facial components are not mutually independent, i.e.the combinations of facial components are not arbitrary. By enabling recommendations given partial descriptions of human faces, these networks are extremely suitable for our task – text-to-face generation. Our method is capable of generating high-quality attribute-conditioned facial images from text, and further supports text-guided editing. Extensive experiments have confirmed the superiority and usability of our method over the prior art.

## Fully-Asynchronous Decentralized SGD with Quantized and Local Updates

### TL;DR

We describe the first variant of decentralized SGD which combines asynchrony, non-blocking communication, local updates, and quantization, prove its convergence, and show that it provides state-of-the-art scalability in some settings.

### Abstract

Decentralized optimization is emerging as a viable alternative for scalable distributed machine learning, 
but also introduces new challenges in terms of synchronization costs.  
To this end, several communication-reduction techniques, such as non-blocking communication, quantization, and local steps, 
have been explored in the decentralized setting. 
Due to the complexity of analyzing optimization in such a relaxed setting, 
this line of work often assumes global communication rounds, which require additional synchronization. 
In this paper, we consider decentralized optimization in the simpler, but harder to analyze, asynchronous gossip model, 
in which communication occurs in discrete, randomly chosen pairings among nodes. 
Perhaps surprisingly, we show that a variant of SGD called SwarmSGD still converges in this setting, 
even if non-blocking communication, quantization, and local steps are all applied in conjunction, and even if the node data distributions and underlying graph topology are both heterogenous. 
Our analysis is based on a new connection with multi-dimensional load-balancing processes. 
We implement this algorithm and deploy it in a super-computing environment, showing that it can outperform previous decentralized methods in terms of end-to-end training time, and that it can even rival carefully-tuned large-batch SGD for certain tasks. 

## Higher Order Kernel Mean Embeddings to Capture Filtrations of Stochastic Processes

### TL;DR

By conditioning stochastic processes on their natural filtrations, we introduce the notion of higher order kernel mean embeddings and propose empirical estimators for the associated higher order maximum mean discrepancies.

### Abstract

Stochastic processes are random variables with values in some space of paths. However, reducing a stochastic process to a path-valued random variable ignores its filtration, i.e. the flow of information carried by the process through time. By conditioning the process on its filtration, we introduce a family of higher order kernel mean embeddings (KMEs) that generalizes the notion of KME to capture additional information related to the filtration. We derive empirical estimators for the associated higher order maximum mean discrepancies (MMDs) and prove consistency. We then construct a filtration-sensitive kernel two-sample test able to capture information that gets missed by the standard MMD test. In addition, leveraging our higher order MMDs we construct a family of universal kernels on stochastic processes that allows to solve real-world calibration and optimal stopping problems in quantitative finance (such as the pricing of American options) via classical kernel-based regression methods. Finally, adapting existing tests for conditional independence to the case of stochastic processes, we design a causal-discovery algorithm to recover the causal graph of structural dependencies among interacting bodies solely from observations of their multidimensional trajectories.

## Learning Spatial-Temporal Weighted Communication for Multi-agent Cooperation

### TL;DR

We propose a novel spatial-temporal cooperative weighted communication based multi-agent reinforcement learning framework to enable agents to efficiently communicate with each other in the cooperative multi-agent environment. 

### Abstract

In many real-world settings, it is crucially vital for agents to learn to communicate and cooperate. Different communication models have been proposed to represent cooperative relations among agents. However, the intensity of the cooperative relation is not paid too much attention. Especially that how it is varied with spatial-temporal information has not been studied deeply. In this paper, we propose a spatial-temporal weighted cooperative communication based multi-agent reinforcement learning framework (STWComm-Q). We design a weighted graph convolutional network to capture cooperative information among agents. On top of that, a spatial-temporal weight learning mechanism is introduced to characterize intensities of cooperations. We design a spatial feature encoder and a novel temporal convolutional network in the spatial and the temporal dimension respectively, to extract effective features for the multi-agent learning. Further, we incorporate the value decomposition into our framework to avoid individual reward design for each agent. Extensive experiments show that our method substantially outperforms existing methods on the public benchmark of micromanagement tasks in StarCraft II. Our work has made a great step forward for communication based multi-agent reinforcement learning methods.  

## GraphENS: Neighbor-Aware Ego Network Sampling for Class-Imbalanced Node Classification

### TL;DR

None

### Abstract

In many real-world node classification scenarios, nodes are highly class-imbalanced, where graph neural networks (GNNs) can be readily biased to major class instances. Albeit existing class imbalance approaches in other domains can alleviate this issue to some extent, they do not consider the impact of message passing between nodes. In this paper, we hypothesize that overfitting to the neighbor sets of minor class due to message passing is a major challenge for class-imbalanced node classification. Moreover, since these methods give large weights to the minor instance itself without considering the restricted neighbor views of minor classes, they suffer from the neighbor memorization problem. To tackle this issue, we propose GraphENS, a novel augmentation method that synthesizes the whole ego network for minor class (minor node and its one-hop neighbors) by combining two different ego networks based on their similarity. Additionally, we introduce a saliency-based node mixing method to exploit the abundant class-generic attributes of other nodes while blocking the injection of class-specific features. Our approach consistently outperforms the baselines over multiple node classification benchmark datasets and architectures.

## Coordination Among Neural Modules Through a Shared Global Workspace

### TL;DR

communication among different specialist using a shared workspace allowing higher order interactions 

### Abstract

Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorporate information from other positions; object-centric architectures make use of graph neural networks to model interactions among entities. However, pairwise interactions may not achieve global coordination or a coherent, integrated representation that can be used for downstream tasks. In cognitive science, a global workspace architecture has been proposed in which functionally specialized components share information through a common, bandwidth-limited communication channel. We explore the use of such a communication channel in the context of deep learning for modeling the structure of complex environments. The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have a rational basis in that (1) they encourage specialization and compositionality and (2) they facilitate the synchronization of otherwise independent specialists.


## Learning Directed Acyclic Graphs using Deep Q-Network

### TL;DR

This paper proposes a DQN based searching algorithm, DQN-DAG, for causal discovery problems which is general in the sense that it can work for various DAG models.

### Abstract

Directed acyclic graphs (DAGs) are widely used to model the casual relationships among random variables in many disciplines. One major class of  casual discovery methods is called `search-and-score', which searches over some DAG space and returns one with the best goodness-of-fit measure. However, the success of these methods highly relies on their own model assumptions such that their searching efficiency cannot be universally ensured and the generalization problem remains unsolved. To address these issues, this work proposes a novel Deep Q-Network based searching algorithm, DQN-DAG, to determine the optimal edge to be added at each searching step which achieves the largest scoring improvement. The proposed algorithm is more general in the sense that it works for various DAG models without making any strong assumption. A special autoencoder architecture is introduced to capture graph features and output the Q-value for each potential action by learning from historical searching trajectories. The superior performance of DQN-DAG is supported by its outerperformance over some state-of-the-art competitors including the most recent RL method in both synthetic and real examples.

## Causal Identification with Matrix Equations

### TL;DR

None

### Abstract

Causal effect identification is concerned with determining whether a causal effect is computable from a combination of qualitative assumptions about the underlying system (e.g., a causal graph) and distributions collected from this system. Many identification algorithms exclusively rely on graphical criteria made of a non-trivial combination of probability axioms, do-calculus, and refined c-factorization (e.g., Lee & Bareinboim, 2020). In a sequence of increasingly sophisticated results, it has been shown how proxy variables can be used to identify certain effects that would not be otherwise recoverable in challenging scenarios through solving matrix equations (e.g., Kuroki & Pearl, 2014; Miao et al., 2018). In this paper, we develop a new causal identification algorithm which utilizes both graphical criteria and matrix equations. Specifically, we first characterize the relationships between certain graphically-driven formulae and matrix multiplications. With such characterizations, we broaden the spectrum of proxy variable based identification conditions and further propose novel intermediary criteria based on the pseudoinverse of a matrix. Finally, we devise a causal effect identification algorithm, which accepts as input a collection of marginal, conditional, and interventional distributions, integrating enriched matrix-based criteria into a graphical identification approach.

## NOTMAD: Estimating Bayesian Networks with Sample-Specific Structures and Parameters

### TL;DR

NOTMAD enables estimation of Bayesian Networks which change parameters and structure with context.

### Abstract

Context-specific Bayesian networks (i.e. directed acyclic graphs, DAGs) identify context-dependent relationships between variables, but the non-convexity induced by  the  acyclicity  requirement  makes  it  difficult  to  share  information  between context-specific estimators (e.g. with graph generator functions). For this reason, existing methods for inferring context-specific Bayesian networks have favored breaking datasets into subsamples, limiting statistical power and resolution, and preventing the use of multi-dimensional and latent contexts.  To overcome this challenge, we propose NOTEARS-optimized Mixtures of Archetypal DAGs (NOT-MAD). NOTMAD models Bayesian network parameters as the output of a function which learns to mix archetypal networks according to sample context. The archetypal networks are estimated jointly with the context-specific networks and do not require any prior knowledge. We encode the non-convex acyclicity constraint as a smooth regularization loss which is back-propagated to the mixing function. In this way, NOTMAD generates context-specific network structures and parameters while sharing power, enabling the estimation of Bayesian network structures and parameters at even single-sample resolution. We demonstrate the utility of NOTMAD and high-resolution network inference, including estimation of patient-specific gene regulatory networks which correspond to morphological variation in cancer.

## Episodic Bandits with Stochastic Experts

### TL;DR

None

### Abstract

We study a version of the contextual bandit problem where an agent is given soft control of a node in a graph-structured environment through a set of stochastic expert policies. The agent interacts with the environment over episodes, with each episode having different context distributions; this results in the `best expert' changing across episodes. Our goal is to develop an agent that tracks the best expert over episodes. We introduce the Empirical Divergence-based UCB (ED-UCB) algorithm in this setting where the agent does not have any knowledge of the expert policies or changes in context distributions. With mild assumptions, we show that bootstrapping from $\tilde{O}(N\log(NT^2\sqrt{E}))$ samples results in a regret of  $\tilde{O}(E(N+1) + \frac{N\sqrt{E}}{T^2})$. If the expert policies are known to the agent a priori, then we can improve the regret to $\tilde{O}(EN)$ without requiring any bootstrapping. Our analysis also tightens pre-existing logarithmic regret bounds to a problem-dependent constant in the non-episodic setting when expert policies are known. We finally empirically validate our findings through simulations.

## The decomposition of the higher-order homology embedding constructed from the $k$-Laplacian

### TL;DR

None

### Abstract

The null space of the $k$-th order Laplacian $\mathbf{\mathcal L}_k$, known as the {\em $k$-th homology vector space}, encodes the non-trivial topology of a manifold or a network. Understanding the structure of the homology embedding can thus disclose geometric or topological information from the data. The study of the null space embedding of the graph Laplacian $\mathbf{\mathcal L}_0$ has spurred new research and applications, such as spectral clustering algorithms with theoretical guarantees and estimators of the Stochastic Block Model. In this work, we investigate the geometry of the $k$-th homology embedding and focus on cases reminiscent of spectral clustering. Namely, we analyze the {\em connected sum} of manifolds as a perturbation to the direct sum of their homology embeddings. We propose an algorithm to factorize the homology embedding into subspaces corresponding to a manifold's simplest topological components. The proposed framework is applied to the {\em shortest homologous loop detection} problem, a problem known to be NP-hard in general. Our spectral loop detection algorithm scales better than existing methods and is effective on diverse data such as point clouds and images. 

## $\texttt{LeadCache}$: Regret-Optimal Caching in Networks

### TL;DR

We design a new online network caching policy.

### Abstract

We consider a set-valued online prediction problem in the context of network caching. A set of users is connected to a number of caches via a bipartite network. At any time slot, each user requests some file chosen from a large catalog. A user's request is met if the requested file is cached in at least one of the caches connected to the user. The objective is to predict and optimally store the files on the caches to maximize the total number of cache-hits. We propose $\texttt{LeadCache}$ - an online caching policy based on the Follow-the-Perturbed-Leader paradigm. We show that the policy is regret-optimal up to a factor of $\tilde{O}(n^{3/8}),$ where $n$ is the number of users. We implement the policy by designing a new linear-time Pipage rounding algorithm.  With an additional Strong-Law-type assumption, we show that the total number of file fetches under $\texttt{LeadCache}$ remains almost surely finite. Finally, we derive a tight regret lower bound using results from graph coloring. Our conclusion is that the proposed learning-based caching policy decisively outperforms the classical policies both theoretically and empirically.

## Incentives and fairness in spectral clustering

### TL;DR

This paper analyze clustering through the perspective of social welfare, proposing metrics and methodology in doing so. 

### Abstract

We consider the problem of clustering when the nodes are agents connected in a graph, and study, both theoretically and experimentally, the interplay and trade-offs between three important considerations: the degree to which agents benefit from the clustering, measured by their individual utilities; the objective quality of the clustering (e.g., number of cut edges, or conductance); and how fairly different groups of agents are treated by the clustering.  We introduce a new utility called {\em closeness}, and we study it theoretically and demonstrate its advantages experimentally.  We experimentally evaluate `fair' versions of clustering (illustrated here as attaining proportional representation of the groups in spectral clustering) with respect to the three expected utilities for different groups and the whole population, in comparison to fairness-neutral spectral clustering; we find that while it can sometimes improve utility, it preserves differences between communities. Finally, we present a polynomial-time algorithm which, given a clustering and appropriate measures of fairness and quality, it will produce a more fair perturbation, which maximizes the ratio of the gain in fairness over the loss in quality --- that is to say, the next point in the fairness/quality trade-off curve.

## Adaptive Sparse Streaming Tensor-Times-Matrix Framework using GNNs

### TL;DR

None

### Abstract

Tensor decomposition is an effective tool for feature analysis in applications such as recommendation systems and social networks, Nevertheless, streaming tensors generated from continuous user behavior on the Internet pose a grand challenge to traditional static tensor operations. In this paper, we introduce an adaptive framework for streaming tensors that efficiently computes streaming sparse tensor-times-matrix (SpTTM) on heterogeneous systems. First, we design a tensor expansion compression method that aims to reduce the invalid traversal of the tensor in computation. In addition, we construct Mat-GraphConv, a graph neural networks model for matrix classification, to select an appropriate computational format for the matrixed tensor on heterogeneous systems. We perform extensive experiments with real and synthetic datasets to demonstrate the advantage and efficiency of the adaptive Stream-SpTTM framework. The experimental results show that our Mat-GraphConv approach improves the recognition accuracy by 9.2\% over the state-of-the-art method when classifying matrices with tensor expansion. Our adaptive Stream-SpTTM framework can achieve an average speedup ratio of 3.258$\times$ compared to the ParTI library. When applied to application, our method can improve the efficiency of tucker decomposition by 21.15\% on average.

## RoSearch: Search for Robust Student Architectures When Distilling Pre-trained Language Models

### TL;DR

Search for Robust Student Architectures When Distilling Pre-trained Language Models

### Abstract

Pre-trained language models achieve outstanding performance in NLP tasks. Various knowledge distillation methods have been proposed to reduce the heavy computation and storage requirements of pre-trained language models. However, from our observations, student models acquired by knowledge distillation suffer from adversarial attacks, which limits their usage in security sensitive scenarios. In order to overcome these security problems, RoSearch is proposed as a comprehensive framework to search the student models with better adversarial robustness when performing knowledge distillation. A directed acyclic graph based search space is built and an evolutionary search strategy is utilized to guide the searching approach. Each searched architecture is trained by knowledge distillation on pre-trained language model and then evaluated under a robustness-, accuracy- and efficiency-aware metric as environmental fitness. Experimental results show that RoSearch can improve robustness of student models from 7%~18% up to 45.8%~47.8% on different datasets with comparable weight compression ratio to existing distillation methods (4.6×~6.5× improvement from teacher model  BERT_BASE) and low accuracy drop. In addition, we summarize the relationship between student architecture and robustness through statistics of searched models.

## A Hybrid Causal Structure Learning Algorithm for Mixed-type Data

### TL;DR

We formulate the nonlinear causal mechanism among mixed-type data via a mixed-SEM, and propose a hybrid algorithm for causal structure learning on mixed-type data. 

### Abstract

Inferring the causal structure of a set of random variables is a crucial problem in many disciplines of science. Over the past two decades, various approaches have been proposed for causal discovery from observational data. However, most of the existing methods are designed for either purely discrete or continuous data, which limit their practical usage. In this paper, we target the problem of causal structure learning from observational mixed-type data. Although there are a few methods that are able to handle mixed-type data, they suffer from restrictions, such as linear assumption and poor scalability. To overcome these weaknesses, we formulate the causal mechanisms via mixed structure equation model and prove its identifiability under mild conditions. A novel locally consistent score, named CVMIC, is proposed for causal directed acyclic graph (DAG) structure learning. Moreover, we propose an efficient conditional independence test, named MRCIT, for mixed-type data, which is used in causal skeleton learning and final pruning to further improve the computational efficiency and precision of our model. Experimental results on both synthetic and real-world data demonstrate that our proposed hybrid model outperforms the other state-of-the-art methods.

## baller2vec: A Multi-Entity Transformer For Multi-Agent Spatiotemporal Modeling

### TL;DR

None

### Abstract

Multi-agent spatiotemporal modeling is a challenging task from both an algorithmic design and computational complexity perspective. Recent work has explored the efficacy of traditional deep sequential models in this domain, but these architectures are slow and cumbersome to train, particularly as model size increases. Further, prior attempts to model interactions between agents across time have limitations, such as imposing an order on the agents, or making assumptions about their relationships. In this paper, we introduce baller2vec, a multi-entity generalization of the standard Transformer that can, with minimal assumptions, simultaneously and efficiently integrate information across entities and time. We test the effectiveness of baller2vec for multi-agent spatiotemporal modeling by training it to perform two different basketball-related tasks: (1) simultaneously forecasting the trajectories of all players on the court and (2) forecasting the trajectory of the ball. Not only does baller2vec learn to perform these tasks well (outperforming a graph recurrent neural network with a similar number of parameters by a wide margin), it also appears to "understand" the game of basketball, encoding idiosyncratic qualities of players in its embeddings, and performing basketball-relevant functions with its attention heads.

## Scene Re-ranking for Recommendation

### TL;DR

Re-ranking based on a group of items. 

### Abstract

Re-ranking is to refine the candidate ranking list of recommended items, such that the re-ranked list attracts users to purchase or click more items than the candidate one without re-ranking. Items in the candidate list are often ranked by their relevance to users’ interests. It is thus important to exploit the mutual influence between items in the re-ranking process. Existing re-ranking models focus on only the pairwise influence between two items, and have limited capability to exploit the local mutual influence in a group of items. Users often show successive interests on a group of relevant items, e.g., mobile phone, phone covers, wireless headset, namely scene. We propose a novel re-ranking model that jointly exploits the local mutual influence in scenes and the global mutual influence between different scenes. Scene representations are learned by graph neural network and multi-head attention. In addition, matrix factorization is utilized to learn the interactive relationship between users and scenes. The final re-ranking list is generated by sorting the predicted scores of all scenes. The results on different datasets show that our method outperforms all other state-of-the-art algorithms significantly.

## Fully Hyperbolic Neural Networks

### TL;DR

We propose a fully hyperbolic neural network framework and demonstrate the advantages of our framework over previous hyperbolic neural networks by building hyperbolic Transformer, GNN and knowledge graph completion model with our framework.

### Abstract

Hyperbolic neural networks have shown great potential for modeling complex data. However, existing hyperbolic networks are not completely hyperbolic, as they encode features in a hyperbolic space yet formalize most of their operations in the tangent space (a Euclidean subspace) at the origin of the hyperbolic space. This hybrid method greatly limits the modeling ability of networks. In this paper, we propose a fully hyperbolic framework to build hyperbolic networks based on the Lorentz model by adapting the Lorentz transformations (including boost and rotation) to formalize essential operations of neural networks. Moreover, we also prove that linear transformation in tangent spaces used by existing hyperbolic networks is a relaxation of the Lorentz rotation and does not include the boost, implicitly limiting the capabilities of existing hyperbolic networks. The experimental results on four NLP tasks show that our method has better performance for building both shallow and deep networks. Our code will be released to facilitate follow-up research.

## Directed Probabilistic Watershed

### TL;DR

None

### Abstract

The Probabilistic Watershed is a semi-supervised learning algorithm applied on undirected graphs. Given a set of labeled nodes (seeds), it defines a Gibbs probability distribution over all possible spanning forests disconnecting the seeds. It calculates, for every node, the probability of sampling a forest connecting a certain seed with the considered node. We propose the "Directed Probabilistic Watershed", an extension of the Probabilistic Watershed algorithm to directed graphs. Building on the Probabilistic Watershed, we apply the Matrix Tree Theorem for directed graphs and define a Gibbs probability distribution over all incoming directed forests rooted at the seeds. Similar to the undirected case, this turns out to be equivalent to the Directed Random Walker. Furthermore, we show that in the limit case in which the Gibbs distribution has infinitely high temperature, the labeling of the Directed Probabilistic Watershed is equal to the one induced by the incoming directed forest of minimum weight.


## Continuous-Time Attentive Representation Learning on Temporal Graphs

### TL;DR

None

### Abstract

Learning continuous-time representation plays a critical role in various real-world machine learning applications. While numerous representation learning methods for static graphs have been proposed, the study of dynamic graphs is still relatively new. The evolving nature of temporal dynamic graphs requires the inductive learning ability to handle new nodes as well as the model capacity to express temporal dynamics at different scales. In this paper, we propose continuous-time attention to effectively aggregate topological neighborhood features which are compatible with temporal patterns. The basic idea is to inform the softmax attention weights that capture topological influence in the neighborhood, with the conditional intensity function of a temporal point process that captures temporal influence beyond the neighborhood. Extensive experiments demonstrate consistent and significant improvement over state-of-the-art baselines on multiple datasets, for tasks including dynamic link prediction, node classification (in both transductive and inductive settings) and top-N recommendation.

## A Law of Iterated Logarithm for Multi-Agent Reinforcement Learning

### TL;DR

None

### Abstract

In Multi-Agent Reinforcement Learning (MARL), multiple agents interact with a common environment and with each other, for solving a shared problem in sequential decision-making. Algorithms for MARL have a wealth of application in popular domains including gaming, robotics, and finance. In this work, we study a family of distributed nonlinear stochastic approximation schemes useful in MARL and derive a novel law of iterated logarithm. In particular, our result describes the convergence rate on almost every sample path where the algorithm converges. This result is the first of its kind in the distributed setup and provides deeper insights than the existing ones, which only discuss convergence rates in the expected or the CLT sense. Importantly, our result holds under significantly weaker assumptions: neither the gossip matrix needs to be doubly stochastic nor the stepsizes square summable. As an application, we show  that, for the stepsize $n^{-\gamma}$ with $\gamma \in (0, 1),$ the distributed TD(0) algorithm with linear function approximation has a convergence rate of $\mathcal{O}(\sqrt{n^{-\gamma} \log n })$ a.s.; for the $1/n$ type stepsize, it is $\mathcal{O}(\sqrt{n^{-1} \log \log n})$ a.s. These growth rates do not depend on the graph depicting the interactions among the different agents. 

## C5T5: Controllable Generation of Organic Molecules with Transformers

### TL;DR

None

### Abstract

Designing organic materials with desired properties is a data-rich problem with high potential impact across fields such as medicine, renewable energy, petrochemical engineering, and agriculture. Using generative modeling to design substances with desired properties is difficult because candidate compounds must satisfy multiple constraints, including synthetic accessibility and other metrics that are intuitive to domain experts but challenging to quantify. We propose a novel zero-shot method that enables transformers to make select-and-replace edits, altering organic substances towards desired property values. The method operates on IUPAC names---a standardized molecular representation that intuitively encodes rich structural information for organic chemists but that has been largely ignored by the ML community. Our technique requires no edited molecule pairs to train, only a rough estimate of molecular properties, and models long-range dependencies and symmetric molecular structures more easily than graph-based methods. Our method also provides a powerful interface to domain experts: it grants users fine-grained control over the generative process by selecting and replacing IUPAC name fragments,  which enables experts to reason at the level of functional groups instead of individual atoms. We demonstrate C5T5's effectiveness on four physical properties relevant for drug discovery, showing that it learns successful and chemically-intuitive strategies for altering molecules towards desired property values.

## Counterfactual Prediction under Network Effect via Representation Learning

### TL;DR

Estimate causal effect in network scenario with representation learning.

### Abstract

Counterfactual reasoning allows people to make smart decisions. In this paper, we address counterfactual prediction from observational data with the presence of network effect. For example, in the question ``would the herd immunity be better had a different subgroup people of a community gotten vaccinated,'' each individual's outcome is also affected by their contacts.
The non-i.i.d nature of networked data raises challenges for counterfactual estimation in two aspects: \textit{homophily} introduces additional confounders to causal effect beyond individual's covariates, and \textit{interference} violates the typical assumption that an individual is not affected by the treatments of his or her social contacts. To alleviate these difficulties, we propose \textbf{NetEstimator}, a novel framework based on representation learning that predicts the counterfactual outcomes from factual observations. NetEstimator uses graph neural network (GNN) to encode confounders from both individual's covariates and network's non-i.i.d nature simultaneously, and utilizes an adversarial paradigm to learn balanced representation between individuals who receive treatment and those do not. We adapt a causal effect error bound for i.i.d data to network scenario and show that the balanced representation ensures a minimized upper bound for the counterfactual error. Experimental studies on semi-synthetic dataset demonstrate the effectiveness of our framework.

## Learning Compact Representations of Neural Networks using DiscriminAtive Masking (DAM)

### TL;DR

None

### Abstract

A central goal in deep learning is to learn  compact representations of features at every layer of a neural network, which is useful for both unsupersived representation learning  and structured network pruning. While there is a growing body of work in structured pruning, current state-of-the-art methods  suffer from two key limitations: (i) instability during training, and (ii) need for an additional step of fine-tuning, which is resource-intensive. At the core of these limitations is the lack of a systematic approach that jointly prunes and refines weights during training in a single stage, and does not require any fine-tuning upon convergence to achieve state-of-the-art performance. We present a novel single-stage structured pruning method termed DiscriminAtive Masking (DAM). The key intuition behind DAM is to discriminatively prefer some of the neurons to be refined during the training process, while gradually masking out other neurons. We show that our proposed DAM appproach has remarkably good performance over various applications, including dimensionality reduction, recommendation system, graph representation learning, and structured pruning for image classification. We also theoretically show that the learning objective of DAM is directly related to minimizing the L0 norm of the masking layer. 

## Improved Guarantees for Offline Stochastic Matching via new Ordered Contention Resolution Schemes

### TL;DR

None

### Abstract

Matching is one of the most fundamental and broadly applicable problems across many domains. In these diverse real-world applications, there is often a degree of uncertainty in the input which has led to the study of stochastic matching models. Here, each edge in the graph has a known, independent probability of existing derived from some prediction. Algorithms must probe edges to determine existence and match them irrevocably if they exist. Further, each vertex may have a patience constraint denoting how many of its neighboring edges can be probed. We present new ordered contention resolution schemes yielding improved approximation guarantees for some of the foundational problems studied in this area. For stochastic matching with patience constraints in general graphs, we provide a $0.383$-competitive algorithm, significantly improving over the previous best $0.31$-competitive bound of Baveja et al. (2018). When the vertices do not have patience constraints, we describe a $0.432$-competitive random order probing algorithm with several corollaries such as an improved guarantee for the Prophet Secretary problem under Edge Arrivals. Finally, for the special case of bipartite graphs with unit patience constraints on one of the partitions, we show a $0.632$-competitive algorithm that improves on the recent $1/3$-guarantee of Hikima et al. (2021).

## Exploring Social Posterior Collapse in Variational Autoencoder for Interaction Modeling

### TL;DR

None

### Abstract

Multi-agent behavior modeling and trajectory forecasting is crucial for safe navigation of autonomous agents in interactive scenarios. Variational Autoencoder (VAE) has been widely applied in multi-agent interaction modeling for its ability in generating diverse behavior and learning a low-dimensional representation for interacting systems. However, existing literature did not formally discuss if a VAE-based model can properly encode interaction into its latent space. In this work, we argue that one of typical formulations of VAEs in multi-agent modeling suffers from an issue we refer to as social posterior collapse, i.e., the model is prone to ignore social context when predicting the future trajectory of an agent. It could cause large prediction errors and poor generalization performance. We analyze the reason behind this under-explored phenomenon and propose several measures to tackle it. Afterwards, we implement the proposed framework and experiment on real-world datasets for multi-agent trajectory prediction. In particular, we propose a novel sparse graph attention message-passing (sparse-GAMP) layer which helps us detect social posterior collapse in our experiments. In the experiments, we verify that social posterior collapse indeed occurs. Also, the proposed measures are effective in alleviating the issue. As a result, the model attains better generalization performance when social context is informative for prediction.

## MOMA: Multi-Object Multi-Actor Activity Parsing

### TL;DR

We introduce a new representation (action hypergraph), a new task (activity parsing), and a new dataset (MOMA) for complex activity recognition.

### Abstract

Complex activities often involve multiple humans utilizing different objects to complete actions (e.g., in healthcare settings, physicians, nurses, and patients interact with each other and various medical devices). The recognition of these activities poses a challenge that requires a detailed understanding of actors' roles, objects' affordances, and their associated relationships. Furthermore, these purposeful activities are composed of multiple achievable steps, including sub-activities and atomic actions, which jointly define a hierarchy of action parts. This paper introduces Activity Parsing as the overarching task of temporal segmentation and classification of activities, sub-activities, atomic actions, along with an instance-level understanding of actors, objects, and their relationships in videos. Involving multiple entities (actors and objects), we argue that traditional pair-wise relationships, often used in scene or action graphs, do not appropriately represent the dynamics between them. Hence, we introduce Action Hypergraph, a spatial-temporal graph containing hyperedges (i.e., edges with higher-order relationships), as a new representation. In addition, we introduce Multi-Object Multi-Actor (MOMA), the first benchmark and dataset dedicated to activity parsing. Lastly, to parse a video, we propose the HyperGraph Activity Parsing (HGAP) network, which outperforms several baselines, including those based on regular graphs and raw video data. 

## Adversarial Robustness through the Lens of Causality

### TL;DR

None

### Abstract

The adversarial vulnerability of deep neural networks has attracted signiﬁcant attention in machine learning. From a causal viewpoint, adversarial attacks can be considered as a specific type of distribution change on natural data. As causal reasoning has an instinct for modeling distribution change, we propose to incorporate causality into mitigating adversarial vulnerability. However, causal formulations of the intuition of adversarial attack and the development of robust DNNs are still lacking in the literature. To bridge this gap, we construct a causal graph to model the generation process of adversarial examples and define the adversarial distribution to formalize the intuition of adversarial attacks. From a causal perspective, we find that the label is spuriously correlated with the style (content-independent) information when an instance is given. The spurious correlation implies that the adversarial distribution is constructed via making the statistical conditional association between style information and labels drastically different from that in natural distribution. Thus, DNNs that fit the spurious correlation are vulnerable to the adversarial distribution. Inspired by the observation, we propose the adversarial distribution alignment method to eliminate the difference between the natural distribution and the adversarial distribution. Extensive experiments demonstrate the efficacy of the proposed method. Our method can be seen as the first attempt to leverage causality for mitigating adversarial vulnerability. 

## Sparse Quadratic Optimisation over the Stiefel Manifold with Application to Permutation Synchronisation

### TL;DR

A method for finding a globally optimal solution of a quadratic objective function over the Stiefel manifold that is sparse.

### Abstract

We address the non-convex optimisation problem of finding a sparse matrix on the Stiefel manifold (matrices with mutually orthogonal columns of unit length) that maximises (or minimises) a quadratic objective function. Optimisation problems on the Stiefel manifold occur for example in spectral relaxations of various combinatorial problems, such as graph matching, clustering, or permutation synchronisation. Although sparsity is a desirable property in such settings, it is mostly neglected in spectral formulations since existing solvers, e.g. based on eigenvalue decomposition, are unable to account for sparsity while at the same time maintaining global optimality guarantees. We fill this gap and propose a simple yet effective sparsity-promoting modification of the Orthogonal Iteration algorithm for finding the dominant eigenspace of a matrix. By doing so, we can guarantee that our method finds a Stiefel matrix that is globally optimal with respect to the quadratic objective function, while in addition being sparse. As a motivating application we consider the task of permutation synchronisation, which can be understood as a constrained clustering problem that has particular relevance for matching multiple images or 3D shapes in computer vision, computer graphics, and beyond. We demonstrate that the proposed approach outperforms previous methods in this domain.

## REMAX: Relational Representation for Multi-Agent Exploration

### TL;DR

None

### Abstract

Training a multi-agent reinforcement learning (MARL) model with a sparse reward is generally difficult because numerous combinations of interactions among agents induce a certain outcome (i.e., success or failure). Earlier studies have tried to resolve this issue by employing an intrinsic reward to induce interactions that are helpful for learning an effective policy. However, this approach requires extensive prior knowledge for designing an intrinsic reward. To train the MARL model effectively without designing the intrinsic reward, we propose a learning-based exploration strategy to generate the initial states of a game. The proposed method adopts a variational graph autoencoder to represent a game state such that (1) the state can be compactly encoded to a latent representation by considering relationships among agents, and (2) the latent representation can be used as an effective input for a coupled surrogate model to predict an exploration score. The proposed method then finds new latent representations that maximize the exploration scores and decodes these representations to generate initial states from which the MARL model starts training in the game and thus experiences novel and rewardable states. We demonstrate that our method improves the training and performance of the MARL model more than the existing exploration methods.

## Multi-modal Dependency Tree for Video Captioning

### TL;DR

None

### Abstract

Generating fluent and relevant language to describe visual content is critical for the video captioning task. Many existing methods generate captions using sequence models that predict words in a left-to-right order. In this paper, we investigate a graph-structured model for caption generation by explicitly modeling the hierarchical structure in the sentences to further improve the fluency and relevance of sentences. To this end, we propose a novel video captioning method that generates a sentence by first constructing a multi-modal dependency tree and then traversing the constructed tree, where the syntactic structure and semantic relationship in the sentence are represented by the tree topology. To take full advantage of the information from both vision and language, both the visual and textual representation features are encoded into each tree node. Different from existing dependency parsing methods that generate uni-modal dependency trees for language understanding, our method construct s multi-modal dependency trees for language generation of images and videos. We also propose a tree-structured reinforcement learning algorithm to effectively optimize the captioning model where a novel reward is designed by evaluating the semantic consistency between the generated sub-tree and the ground-truth tree. Extensive experiments on several video captioning datasets demonstrate the effectiveness of the proposed method. 

## You are AllSet: A multiset function framework for hypergraph neural networks

### TL;DR

We propose AllSet which connects learning multiset functions with hypergraph neural networks and our methods have provably more expressive power.

### Abstract

Hypergraphs are used to model higher-order interactions amongst agents and there exist many practically relevant instances of hypergraph datasets. To enable efficient processing of hypergraph-structured data, several hypergraph neural network platforms have been proposed for learning hypergraph properties and structure, with a special focus on node classification. However, almost all existing methods use heuristic propagation rules and offer suboptimal performance on many datasets. We propose AllSet, a new hypergraph neural network paradigm that represents a highly general framework for (hyper)graph neural networks and for the first time implements hypergraph neural network layers as compositions of two multiset functions that can be efficiently learned for each task and each dataset. Furthermore, AllSet draws on new connections between hypergraph neural networks and recent advances in deep learning of multiset functions. In particular, the proposed architecture utilizes Deep Sets and Set Transformer architectures that allow for significant modeling flexibility and offer high expressive power. To evaluate the performance of AllSet, we conduct the most extensive experiments to date involving ten known benchmarking datasets and three newly curated datasets that represent significant challenges for hypergraph node classification. The results demonstrate that AllSet has the unique ability to consistently either match or outperform all other hypergraph neural networks across the tested datasets.

## Neural Logic Analogy Learning

### TL;DR

None

### Abstract

Letter-string analogy is an important analogy learning task which seems to be easy for humans but very challenging for machines. The main idea behind current approaches to solving letter-string analogies is to design heuristic rules for extracting analogy structures and constructing analogy mappings. However, one key problem is that it is difficult to build a comprehensive and exhaustive set of analogy structures which can fully describe the subtlety of analogies. This problem makes current approaches unable to handle complicated letter-string analogy problems.In this paper, we proposeNeural logic analogy learning (Noan), which is a dynamic neural architecture driven by differentiable logic reasoning to solve analogy problems. Each analogy problem is converted into logical expressions consisting of logical variables and basic logical operations (AND, OR, and NOT). More specifically, Noan learns the logical variables as vector embeddings and learns each logical operation as a neural module. In this way, the model builds computational graph integrating neural network with logical reasoning to capture the internal logical structure of the input letter strings.  The analogy learning problem then becomes a True/False evaluation problem of the logical expressions. Experiments show that our machine learning-based Noan approach outperforms state-of-the-art approaches on standard letter-string analogy benchmark datasets.

## TyXe: Pyro-based Bayesian neural nets for Pytorch

### TL;DR

We make it easy to turn Pytorch neural nets into flexible BNNs powered by Pyro without having to know Pyro.

### Abstract

We introduce TyXe, a Bayesian neural network library built on top of Pytorch and Pyro. Our leading design principle is to cleanly separate architecture, prior, inference and likelihood specification, allowing for a flexible workflow where users can quickly iterate over combinations of these components. In contrast to existing packages TyXe does not implement any layer classes, and instead relies on architectures defined in generic Pytorch code. TyXe then provides modular choices for canonical priors, variational guides, inference techniques, and layer selections for a Bayesian treatment of the specified architecture. Sampling tricks for variance reduction, such as local reparameterization or flipout, are implemented as effect handlers, which can be applied independently of other specifications. We showcase the ease of use of TyXe to explore Bayesian versions of popular models from various libraries: toy regression with a pure Pytorch neural network; large-scale image classification with torchvision ResNets; graph neural networks based on DGL; and Neural Radiance Fields built on top of Pytorch3D. Finally, we provide convenient abstractions for variational continual learning. In all cases the change from a deterministic to a Bayesian neural network comes with minimal modifications to existing code, offering a broad range of researchers and practitioners alike practical access to uncertainty estimation techniques.


## TNASP: A Transformer-based NAS Predictor with a Self-evolution Framework

### TL;DR

We propose a transformer-based NAS performance predictor, associated with a Laplacian matrix based positional encoding strategy, which achieves state-of-the-art performance on NAS-Bench-101, NAS-Bench-201 and DARTS search space.

### Abstract

Predictor-based Neural Architecture Search (NAS) continues to be an important topic because it aims to mitigate the time-consuming search procedure of traditional NAS methods. A promising performance predictor determines the quality of final searched models in predictor-based NAS methods. Most existing predictor-based methodologies train model-based predictors under a proxy dataset setting, which may suffer from the accuracy decline and the generalization problem, mainly due to their poor abilities to represent spatial topology information of the graph structure data. Besides the poor encoding for spatial topology information, these works did not take advantage of the temporal information such as historical evaluations during training. Thus, we propose a transformer-based NAS performance predictor, associated with a Laplacian matrix based positional encoding strategy, which better represents topology information and achieves better performance than previous state-of-the-art methods on NAS-Bench-101, NAS-Bench-201, and DARTS search space. Furthermore, we also propose a self-evolution framework that can fully utilize temporal information as guidance. This framework iteratively involves the evaluations of previously predicted results as constraints into current optimization iteration, thus further improve the performance of our predictor. Such framework is model-agnostic, thus improving performance on various backbone structures for the prediction task. Our proposed method ranked 2nd among all teams in one recent international NAS challenge. 

## Towards Generic Interface for Human-Neural Network Knowledge Exchange

### TL;DR

We propose Human-NN-Interface, a framework using Structural Visual Concepts as a "language" for humans and NN to communicate, interact, and influence each other.

### Abstract

Neural Networks (NN) outperform humans in multiple domains, yet they suffer from a lack of transparency and interpretability, 
which hinders users from intuitive and effective interactions with them. Especially when NN makes mistakes, humans can hardly locate the reason for the error, and correcting the error is even harder. While recent advances in explainable AI has substantially improved the explainability of NNs,  effective knowledge exchange between humans and NNs is still under-explored.
To fill this gap, we propose Human-NN-Interface (HNI), a framework using a structural representation of visual concepts as a "language" for humans and NN to communicate, interact, and exchange knowledge.  Take image classification as an example, HNI visualizes the reasoning logic of a NN with class-specific Structural Concept Graph (SCG), which is human-interpretable. On the other hand, humans can effectively provide feedback and guidance to the NN by modifying the SCG and transferring the knowledge back to NN, through HNI. We demonstrate with image classification tasks on 3 different types of interactions to show the efficacy of HNI:
(1) Explaining the reasoning logic of NNs so human users can intuitively identify and locate potential errors of NN; (2) humans users correcting the errors and improve NN's performance by modifying the SCG and distilling the knowledge back to the original NN; (3) humans users intuitively guiding NN in zero-shot learning.

## Geometry of Similarity Comparisons

### TL;DR

We propose random ordinal spread variables that, in theory and practice, can reveal the underlying geometry of similarity measurements.

### Abstract

Many data analysis problems can be cast as distance geometry problems in \emph{space forms} -- Euclidean, spherical, or hyperbolic spaces. Often, absolute distance measurements are often unreliable or simply unavailable and only proxies to absolute distances in the form of similarities are available. Hence we ask the following: Given only \emph{comparisons} of similarities amongst a set of entities, what can be said about the geometry of the underlying space form? To study this question, we introduce the notions of the \textit{ordinal capacity} of a target space form and \emph{ordinal spread} of the similarity measurements. The latter is an indicator of complex patterns in the measurements, while the former quantifies the capacity of a space form to accommodate a set of measurements with a specific ordinal spread profile. We prove that the ordinal capacity of a space form is related to its dimension and the sign of its curvature. This leads to a lower bound on the Euclidean and spherical embedding dimension of what we term similarity graphs. More importantly, we show that the statistical behavior of the ordinal spread random variables defined on a similarity graph can be used to identify its underlying space form. We support our theoretical claims with experiments on weighted trees, single-cell RNA expression data and spherical cartographic measurements.

## Neural Production Systems

### TL;DR

Modelling sparse interactions among seperate entities using dynamically selected rules

### Abstract

Visual environments are structured, consisting of distinct  objects or entities. These entities have properties---visible or latent---that determine the manner in which they interact with one another. To partition images into entities, deep-learning researchers have proposed structural inductive biases such as slot-based architectures. To model interactions among entities, equivariant graph neural nets (GNNs) are used, but these are not particularly well suited to the task for two reasons. First, GNNs do not predispose interactions to be sparse, as relationships among independent entities are likely to be.  Second, GNNs do not factorize knowledge about  interactions in an entity-conditional manner. As an alternative, we take inspiration from cognitive science and resurrect a classic approach, production systems, which consist of a set of rule templates that are applied by binding placeholder  variables in the rules to specific entities. Rules are scored on their match to entities, and the best fitting rules are applied to update entity properties. In a series of experiments, we demonstrate that this architecture achieves a flexible, dynamic flow of control and serves to factorize entity-specific and rule-based information. This disentangling of knowledge achieves robust future-state prediction in rich visual environments, outperforming state-of-the-art methods using GNNs, and allows for the extrapolation from simple (few object) environments to more complex environments.


## Vitruvion: A Generative Model of Parametric CAD Sketches

### TL;DR

We build a generative model for parametric CAD sketches and use it to perform autocompletion and other tasks relevant to design.

### Abstract

Parametric computer-aided design (CAD) tools are the predominant way that engineers specify physical structures, from bicycle pedals to airplanes to printed circuit boards.
The key characteristic of parametric CAD is that design intent is encoded not only via geometric primitives, but also by parameterized constraints between the elements.
This relational specification can be viewed as the construction of a constraint program, allowing edits to coherently propagate to other parts of the design.
Machine learning offers the intriguing possibility of accelerating the design process via generative modeling of these structures, enabling new tools such as autocompletion, constraint inference, and conditional synthesis.
In this work, we present such an approach to generative modeling of parametric CAD sketches, which constitute the basic computational building blocks of modern mechanical design.
Our model, trained on real-world designs from the SketchGraphs dataset, autoregressively synthesizes sketches as sequences of primitives, with initial coordinates, and constraints that reference back to the sampled primitives.
As samples from the model match the constraint graph representation used in standard CAD software, they may be directly imported, solved, and edited according to downstream design tasks.
In addition, we condition the model on various contexts, including partial sketches (primers) and images of hand-drawn sketches.
Evaluation of the proposed approach demonstrates its ability to synthesize realistic CAD sketches and its potential to aid the mechanical design workflow.

## Weakly-supervised Learning for Matching Objects in Context

### TL;DR

None

### Abstract

Matching images or their contents plays a vital role in visual scene modelling and understanding from multiple views. This however, is often performed either on the global image level or in the local feature level. Intermediate object level matching is usually limited within video sequences, such as in the case of video object tracking. In contrast, this paper studies the problem of image-to-image object matching. Two objects are considered to be matched, if they belong to the same class and appear in a similar context. In this work, we represent the object's context using their relationships with the other surrounding objects. Learning to match objects in context has potential to abstractly model the visual concepts, linking the gap between local feature and global image level matchings.
This paper proposes a method that learns to match objects in context without requiring any direct supervision for matching. 
Instead, we only make use of the weak-supervision of objects' classes and their relationships, in addition to the proposed self-supervision technique. The matching process takes objects from two images as inputs, and performs their matching using a graph neural network. In this process, the matching is guided by, (i) one-to-one assignments; (ii) class consistency; and (iii) context consistency. The proposed method is shown to be effective on three benchmark datasets. 

## A 3D Generative Model for Structure-Based Drug Design

### TL;DR

None

### Abstract

We study a fundamental problem in structure-based drug design --- generating molecules that bind to specific protein binding sites. While we have witnessed the great success of deep generative models in drug design, the existing methods are mostly string-based or graph-based. They are limited by the lack of spatial information and thus unable to be applied to structure-based design tasks. Particularly, such models have no or little knowledge of how molecules interact with their target proteins exactly in 3D space. In this paper, we propose a 3D generative model that generates molecules given a designated 3D protein binding site. Specifically, given a binding site as the 3D context, our model estimates the probability density of atom's occurrences in 3D space --- positions that are more likely to have atoms will be assigned higher probability. To generate 3D molecules, we propose an auto-regressive sampling scheme --- atoms are sampled sequentially from the learned distribution until there is no room for new atoms. Combined with this sampling scheme, our model can generate valid and diverse molecules, which could be applicable to various structure-based molecular design tasks such as molecule sampling and linker design. Experimental results demonstrate that molecules sampled from our model exhibit high binding affinity to specific targets and good drug properties such as drug-likeness even if the model is not explicitly optimized for them.

## Calibrated Nonparametric Scan Statistics for Anomalous Pattern Detection in Graphs

### TL;DR

None

### Abstract

We propose a new approach, the calibrated nonparametric scan statistic (CNSS), for more accurate detection of anomalous patterns in large-scale, real-world graphs. Scan statistics identify connected subgraphs that are interesting or unexpected through maximization of a likelihood ratio statistic; in particular, nonparametric scan statistics (NPSS) identify subgraphs with a higher than expected proportion of individually significant nodes.  However, we show that recently proposed NPSS methods are miscalibrated, failing to account for the maximization of the statistic over the multiplicity of subgraphs.  This results in both reduced detection power for subtle signals, and low precision of the detected subgraph even for stronger signals. Thus we develop a new statistical approach to recalibrate NPSS, correctly adjusting for multiple hypothesis testing and taking the underlying graph structure into account.  While the recalibration, based on randomization testing, is computationally expensive, we propose both an efficient algorithm and new, closed-form lower bounds (on the expected maximum proportion of significant nodes for subgraphs of a given size, under the null hypothesis of no anomalous patterns). These advances, along with integration of recent core-tree decomposition methods, enable the CNSS approach to scale to large real-world graphs, with substantial improvement in the accuracy of detected subgraphs.  Extensive experiments on both semi-synthetic and real-world datasets are demonstrated to validate the effectiveness of our proposed methods, in comparison with state-of-the-art counterparts. 

## Spherical Message Passing for 3D Molecular Graphs

### TL;DR

None

### Abstract

We consider representation learning of 3D molecular graphs in which each atom is associated with a spatial position in 3D.
This is an under explored area of research, and a principled framework is currently lacking. In this work, we propose a generic framework, known as the 3D graph network (3DGN), to provide a clear interface for integrating position information in 3D molecular graphs. Built on 3DGN, we propose the spherical message passing (SMP) as a novel and specific scheme for realizing the 3DGN framework. We conduct rigorous analysis and show that the relative location of each atom in 3D molecular graphs is uniquely defined in the SMP scheme. Hence, practically, our SMP can distinguish all molecular structures in nature; formally, SMP is guaranteed to generate complete molecular representations without information loss. In addition, as it incorporates relative 3D information and affiliates a new strategy to compute torsion, SMP yields predictions naturally invariant to translation and rotation.
Based on meaningful physically-based representations of 3D information, we further propose the SphereNet for learning from 3D molecular graphs. We show that existing deep models for 3D molecules can be viewed as special cases of the SphereNet. Experimental results demonstrate that the use of complete and meaningful 3D information in SphereNet leads to significant performance improvements in prediction tasks.

## Zero Training Overhead Portfoliosfor Learning to Solve Combinatorial Problems

### TL;DR

Zero Training Overhead Portfolios (ZTop): a new model selection mechnism for  learning to solve combinatorial problems.

### Abstract

There has been an increasing interest in harnessing deep learning to tackle combinatorial optimization (CO) problems in recent years. Typical deep learning approaches for CO leverage the problem structure in the model architecture. Nevertheless, the model selection is still mainly based on the conventional machine learning setting. Due to the discrete nature of CO problems, a single model is unlikely to learn the problem entirely. We introduce Zero Training Overhead Portfolio (ZTop), a simple yet effective model selection mechanism that only requires to ensemble models at different checkpoints from the same training trajectory but performs comparably to training and combining multiple training trajectory models. Inspired by an observation that well-trained models acquired in the same training trajectory, with similar top validation performance, perform well on very different validation instances –ZTop ensembles a set of well-trained models, each providing a unique heuristic with zero training overhead, and applies them, sequentially or in parallel, to solve the test instances. We show that ZToppingsignificantly improves the performance of the state-of-the-art deep learning approaches on three prototypical CO domains, the hardest unique-solution Sudoku instances, challenging routing problems, and the graph maximum cut problem. Moreover, since ZTopping can acquire models that solve significantly different sets of instances, applying conventional model selection techniques that leverage multiple training trajectories does not provide further improvement than using ZTopping.

## Diverse Message Passing for Attribute with Heterophily

### TL;DR

None

### Abstract

Most GNNs are restricted in the \textit{uniform} message passing framework, where all attributes of one node are considered as a whole and share the uniform propagation weights along one edge,  and focus on the uniform weights learning. This is induced by the simplification of homophily and heterophily as node-level property and the ignorance of attributes difference.  Actually, different attributes possess diverse characteristics. In this paper network homophily rate defined with node label is extended to attribute homophily rate by taking attribute as weak label. Statistics show attributes possess diverse homophily.  To meet this observation, a diverse message passing (DMP) framework, which specifies each attribute propagation weight along each edge,  is proposed. Besides, two strategies are proposed to significantly reduce the model complexity of DMP to prevent overfitting issue. 
By investigating the spectral characteristics, existing spectral GNNs are just equivalent to a degeneration of diverse message passing. Theoretical analysis from numerical optimization perspective reveals that  Diverse Message Passing generates multiple groups of graph partition candidates for classifier to determine how to combine them to form the final partition, thus possesses more powerful expressive ability  compared to classic uniform message passing. And this powerful expressive ability also indicates the ability to prevent over-smoothing issue. Evaluations on real networks demonstrate the superiority of diverse message passing on preventing over-smoothing and handling network with heterophily.

## PyTorch RPC: Distributed Deep Learning Built on Tensor-Optimized Remote Procedure Calls

### TL;DR

PyTorch RPC 1) offers tensor-aware communication 2) tracks lifetime for remote data/model for distributed GC 3) extends autograd and optimizer beyond machine boundaries. 

### Abstract

Distributed training technologies have advanced rapidly in the past few years and have unlocked unprecedented scalability with increasingly complex solutions. These technologies made distributed training much more efficient and accessible; however, each technology imposes specific constraints on the training paradigm or the model structure, forcing ML practitioners to fit the problems into available tools. Moreover, since existing solutions focus more on high-level abstractions, system developers can rarely reuse building blocks when developing the next new training paradigm or domain. This has led experts in reinforcement learning, federated learning, and graph learning to rely on general-purpose RPC frameworks, although those frameworks are never optimized for deep learning. To address these problems, we propose PyTorch RPC as a generic and high-performance solution for distributed deep learning. Compared to existing RPC frameworks, PyTorch RPC natively provides essential features for implementing training applications in a distributed environment, including tensor-aware communication, remote data reference, distributed autograd engine, and distributed optimizer. Evaluations show that PyTorch RPC attains up to two orders of magnitude faster tensor communication compared to gRPC with one-tenth of user code. Case studies further demonstrate that users can easily employ PyTorch RPC to build efficient reinforcement learning applications (Mario solver), implement large language models (175B parameters), and train recommendation models (DLRM). PyTorch RPC is available at https://pytorch.org/docs/stable/rpc.html.

## MoleHD: Automated Drug Discovery using Brain-Inspired Hyperdimensional Computing

### TL;DR

This paper presents a new approach for drug discovery. 

### Abstract

Modern drug discovery is often time-consuming, complex and cost-ineffective due to the large volume of molecular data and complicated molecular properties. Recently, machine learning algorithms have shown promising results in virtual screening of automated drug discovery by predicting molecular properties. While emerging learning methods such as graph neural networks and recurrent neural networks exhibit high accuracy, they are also notoriously computation-intensive and memory-intensive with operations such as feature embeddings or deep convolutions. In this paper, we propose a viable alternative to neural network classifiers. We present MoleHD, a method based on brain-inspired hyperdimensional computing (HDC) for molecular property prediction. We first transform the SMILES presentation of molecules into feature vectors by SMILE-PE tokenizers pretrained on the ChEMBL database. Then, we develop HDC encoders to project such features into high-dimensional vectors that are used for training and inference. We perform an extensive evaluation using 30 classification tasks from 3 widely-used molecule datasets and compare MoleHD with 10 baseline methods including 6 SOTA neural network classifiers. Results show that MoleHD is able to outperform all the baseline methods on average across 30 classification tasks with significantly reduced computing cost. To the best of our knowledge, we develop the first HDC-based method for drug discovery. The promising results presented in this paper can potentially lead to a novel path in drug discovery research. 

## Neural Capacitance: A New Perspective of Neural Network Selection via Edge Dynamics

### TL;DR

This paper provides a new perspective of neural network selection by studying the edge dynamics during neural network training.

### Abstract

Efficient model selection for identifying a suitable pre-trained neural network to a downstream task is a fundamental yet challenging task in deep learning. Current practice requires expensive computational costs in model training for performance prediction. In this paper, we propose a novel framework for neural network selection by analyzing the governing dynamics over synaptic connections (edges) during training. Our framework is built on the fact that back-propagation during neural network training is equivalent to the dynamical evolution of synaptic connections. Therefore, a converged neural network is associated with an equilibrium state of a networked system composed of those edges. To this end, we construct a network mapping $\phi$, converting a neural network $G_\mathcal A$ to a directed line graph $G_\mathcal B$ that is defined on those edges in $G_\mathcal A$.
Next, we derive a \textit{neural capacitance} metric $\beta_{\rm eff}$ as a predictive measure universally capturing the generalization capability of $G_\mathcal A$ on the downstream task using only a handful of early training results.
We carried out extensive experiments using 17 popular pre-trained ImageNet models and three benchmark datasets,
including CIFAR10, Fashion-MNIST and SVHN, to evaluate the finetuning performance of our framework. 
Our neural capacitance metric is shown to be a powerful indicator for model selection based only on early training results and is more efficient than state-of-the-art methods.

## Feature-level privacy loss modelling in differentially private machine learning

### TL;DR

We extend the notion of function sensitivity in differential privacy and introduce a novel hybrid automatic differentiation system for privacy modelling

### Abstract

Differential privacy (DP) allows the quantification of privacy loss when the data of individuals is subjected to algorithmic processing such as machine learning, as well as the provision of objective privacy guarantees. However, while techniques such as individual DP allow for granular privacy accounting on the individual level, few works have investigated the impact of individual features on the individual's privacy loss. This is in part due to the reason that the analytical reasoning for such modelling requires the introduction of new tools, capable of tracking an individual's data and their features through the flow of computation. In this work, we expand the definition of sensitivity by introducing a new concept, termed partial sensitivity, which expresses the impact of specific private attributes on an individual's privacy loss. For its analysis and modelling, we propose Deuterium, a novel hybrid automatic differentiation (AD) system, which provides access to closed-form expressions for any quantity in a computational graph representing an arbitrary composition of differentiable functions. We demonstrate our approach in the setting of private database queries, where we obtain tight analytic individual DP estimates and model the sensitivity of neural networks, where we evaluate the impact of specific private attributes on overall privacy loss. We experimentally verify our findings using synthetic data to demonstrate their applicability to neural network training.

## The Complexity of Bayesian Network Learning: Revisiting the Superstructure

### TL;DR

We circumvent previously established complexity lower bounds and identify conditions under which Bayesian Network Structure Learning becomes fixed-parameter tractable.

### Abstract

We investigate the parameterized complexity of Bayesian Network Structure Learning (BNSL), a classical problem that has received significant attention in empirical but also purely theoretical studies. We follow up on previous works that have analyzed the complexity of BNSL w.r.t. the so-called superstructure of the input. While known results imply that BNSL is unlikely to be fixed-parameter tractable even when parameterized by the size of a vertex cover in the superstructure, here we show that a different kind of parameterization - notably by the size of a feedback edge set - yields fixed-parameter tractability. We proceed by showing that this result can be strengthened to a localized version of the feedback edge set, and provide corresponding lower bounds that complement previous results to provide a complexity classification of BNSL w.r.t. virtually all well-studied graph parameters.

We then analyze how the complexity of BNSL depends on the representation of the input. In particular, while the bulk of past theoretical work on the topic assumed the use of the so-called non-zero representation, here we prove that if an additive representation can be used instead then BNSL becomes fixed-parameter tractable even under significantly milder restrictions to the superstructure, notably when parameterized by the treewidth alone. Last but not least, we show how our results can be extended to the closely related problem of Polytree Learning.

## Aspis: A Robust Detection System for Distributed Learning

### TL;DR

None

### Abstract

State of the art machine learning models are routinely trained on large scale distributed clusters. Crucially, such systems can be compromised when some of the computing devices exhibit abnormal (Byzantine) behavior and return arbitrary results to the parameter server (PS). This behavior may be attributed to a plethora of reasons including system failures and orchestrated attacks. Existing work suggests robust aggregation and/or computational redundancy to alleviate the effect of distorted gradients. However, most of these schemes are ineffective when an adversary knows the task assignment and can judiciously choose the attacked workers to induce maximal damage. Our proposed method Aspis assigns gradient computations to worker nodes using a subset-based assignment which allows for multiple consistency checks on the behavior of a worker node. Examination of the calculated gradients and post-processing (clique-finding in an appropriately constructed graph) by the central node allows for efficient detection and subsequent exclusion of adversaries from the training process. We prove the Byzantine resilience and detection guarantees of Aspis under weak and strong attacks and extensively evaluate the system on various large-scale training scenarios. The main metric for our experiments is the test accuracy for which we demonstrate significant improvement of about 30% compared to many state-of-the-art approaches on the CIFAR-10 dataset. The corresponding reduction of the fraction of corrupted gradients ranges from 16% to 98%.

## Causal Markov Decision Processes: Learning Good Interventions Efficiently

### TL;DR

None

### Abstract

We introduce causal Markov Decision Processes (C-MDPs), a new formalism for sequential decision making which combines the standard MDP formulation with causal structures over state transition and reward functions. Many contemporary and emerging application areas such as digital healthcare and digital marketing can benefit from modeling with C-MDPs due to the causal mechanisms underlying the relationship between interventions and states/rewards. We propose the causal upper confidence bound value iteration (C-UCBVI) algorithm that exploits the causal structure in C-MDPs and improves the performance of standard reinforcement learning algorithms that do not take causal knowledge into account. We prove that C-UCBVI satisfies an $\tilde{O}(HS\sqrt{ZT})$ regret bound, where $T$ is the the total time steps,  $H$ is  the episodic horizon, and $S$ is the cardinality of the state space. Notably, our regret bound does not scale with the size of actions/interventions ($A$), but only scales with a causal graph dependent quantity $Z$ which can be exponentially smaller than $A$. By extending C-UCBVI to the factored MDP setting, we propose the causal factored UCBVI (CF-UCBVI) algorithm, which further reduces the regret exponentially in terms of $S$. Furthermore, we show that RL algorithms for linear MDP problems can also be incorporated in C-MDPs. We empirically show the benefit of our causal approaches in various settings to validate our algorithms and theoretical results.

## Networked Federated Multi-Task Learning

### TL;DR

we study conditions on network structure and local models (tasks) such that a robust and scalable primal-dual method learns optimal model weights

### Abstract

Many important application domains generate massive collections of local datasets. 
These local datasets are often related via an intrinsic network structure that arises 
from domain-specific notions of similarity between local datasets. Different notions 
of similarity are induced by spatio-temporal proximity, statistical dependencies or 
functional relations. We use this network structure to adaptively pool similar local 
datasets into nearly homogenous training sets for learning tailored models. Our main 
conceptual contribution is to formulate networked federated learning using the 
concept of generalized total variation (GTV) minimization as a regularizer. This formulation 
is highly flexible and can be combined with almost any parametric model including Lasso 
or deep neural networks. We unify and considerably extend some well-known approaches 
to federated multi-task learning. Our main algorithmic contribution is a novel federated  
learning algorithm which is well suited for distributed computing environments such as 
edge computing over wireless networks. This algorithm is robust against model 
misspecification and numerical errors arising from limited computational resources including 
processing time or wireless channel bandwidth. As our main theoretical contribution, 
we offer precise conditions on the local models as well on their network structure such that 
our algorithm learns nearly optimal local models. Our analysis reveals an interesting interplay 
between the (estimation-theoretic) geometry of local models and the (graph-theoretic) 
geometry of their network. 

## Scaling Gaussian Processes with Derivative Information Using Variational Inference

### TL;DR

None

### Abstract

Gaussian processes with derivative information are useful in many settings where derivative information is available, including numerous Bayesian optimization and regression tasks that arise in the natural sciences. Incorporating derivative observations, however, comes with a dominating $O(N^3D^3)$ computational cost when training on $N$ points in $D$ input dimensions. This is intractable for even moderately sized problems. While recent work has addressed this intractability in the low-$D$ setting, the high-$N$, high-$D$ setting is still unexplored and of great value, particularly as machine learning problems increasingly become high dimensional. In this paper, we introduce methods to achieve fully scalable Gaussian process regression with derivatives using variational inference. Analogous to the use of inducing values to sparsify the labels of a training set, we introduce the concept of inducing directional derivatives to sparsify the partial derivative information of the training set. This enables us to construct a variational posterior that incorporates derivative information but whose size depends neither on the full dataset size $N$ nor the full dimensionality $D$. We demonstrate the full scalability of our approach on a variety of tasks, ranging from a high dimensional Stellarator fusion regression task to training graph convolutional neural networks on PubMed using Bayesian optimization. Surprisingly, we additionally find that our approach can improve regression performance even in settings where only label data is available.

## Compositional Grammatical Learning for Vision and Language Understanding

### TL;DR

None

### Abstract

Referring expression comprehension is an emerging task in the field of vision and language understanding, which contains two workflows, i.e, two-stage methods and one-stage methods. Although the one-stage methods have attracted great interest as their intuitive framework and faster inference speed, they still have limitations on understanding complex language and capturing compositive relationships among visual entities. To address these limitations, we propose a Compositional Grammatical Learning Networks (CGLN) for one-stage referring expression comprehension, which applies multi-stage reasoning between vision and language to learn compositional relationships for better understanding. Extensive experiments on four popular benchmark datasets, i.e., RefCOCO, RefCOCO+, RefCOCOg, and ReferItGame, demonstrate that our approach obtains superior performance over the state-of-the-art one-stage methods.

## Episodic Learning, Bellman Optimality, and Lagrangian Saddle Points

### TL;DR

None

### Abstract

Episodic Learning Process (ELP) is a family of infinite-horizon Markov Decision Process (MDP) that was recently proposed to better formulate the learning environment of finite-horizon AI tasks. While being more precise in modeling real-world problems, the calibrated reward objective and the generalized $\gamma$-discounting in the ELP formalism leave open if the many pivotal theoretical properties that underlie the whole approach of value-based RL still hold in ELPs.

In this paper we developed a series of novel theoretical results to characterize the rich structures in the value function space of ELP. We proved that although the generalized Bellman optimality operator is not a contraction mapping in ELPs (in general), the operator still has a unique fixed and limiting point (and the fixed point still induces optimal policies). Different from the traditional contraction argument, the generalized Bellman optimality equation holds in ELP because of a fundamental graph property of this special class of MDPs. Moreover, we proved a minimax theorem for the nonlinear Lagrangian functions associated with the Bellman optimality operator. The strong duality property of the Lagrangian in turn entails existence of minimax-equilibrium, for which we further provided equilibrium conditions. Finally, we pointed out that there are two kinds of saddle-point value functions for the Lagrangian of ELPs, the minimax values and the maximin values. While the former were more explored in the literature, we showed that the latter (i.e. maximin value functions) have better optimality properties.

## A Markov Decision Framework for Efficient Contact Tracing and Isolation

### TL;DR

None

### Abstract

Efficient contact tracing and isolation is an effective strategy to control epidemics. It was used effectively during the Ebola epidemic and successfully implemented in several parts of the world during the ongoing COVID-19 pandemic. An important consideration while implementing contact tracing is the number of {\em contact tracers} available --- the number of such individuals is limited for economic reasons. In this paper, we present a Markov Decision Process (MDP) to formulate the problem of efficient contact tracing that reduces the size of the outbreak while using a limited number of contact tracers. We formulate each step of the MDP as a combinatorial problem, $\prob{}$. We demonstrate that $\prob{}$ is NP-Hard, so we develop two LP-based approximation algorithms. Though these algorithms directly solve \prob{}, they are often impractical in the real world due to information constraints. To this end, we develop a greedy approach based on insights from the analysis of the previous algorithms, which we show is more interpretable and implementable in practice. Finally, we carry out experiments on simulations of the MDP run on real-world networks, and show how the algorithms can help in {\em bending} the epidemic curve while limiting the number of isolated individuals. Our experimental results demonstrate that the greedy algorithm and its variant are especially effective, robust, and practical in a variety of realistic scenarios, such as when the contact graph and specific transmission probabilities are not known.

## Social Koopman Model for Multi-Agent Trajectory Prediction

### TL;DR

We extend the classic Koopman control theory to the task of multi-agent trajectory prediction.

### Abstract

Modeling multi-agent interacting systems is becoming increasingly important in a variety of fields. The main scope of this paper is the task of multi-agent trajectory prediction. The key challenges of this task include the stability of long-term prediction, the inference of inter-agent relationships and the modeling of global context etc. Most existing works mainly adopt recurrent neural networks (RNN) to extract the historical information of each agent, and use various message-passing schemes to implicitly describe the interaction between agents. However, RNN is known to be inadequate to tackle long-term time-series, and its state space lacks interpretability. In this paper, we propose a generic multi-agent trajectory prediction framework that addresses above issues, leveraging the Koopman theory to design a novel deep model. Critically, the Koopman autoencoder learns an embedding space wherein the system dynamics are linear, thus enjoying better long-term prediction performance and good theoretical interpretability. In addition, to model the interaction among agents, we explicitly infer a latent-relationship graph and fuse such information into Koopman base model. The optimization of the proposed social Koopman model is effectively driven by the joint objective of multiple losses that enforce reconstruction, linear-dynamics and precise forward prediction respectively. The proposed model is evaluated on SportVU Basketball Players Tracking data, which contains highly complex multi-player trajectories. Our experiments clearly demonstrate that the social Koopman model achieves new state-of-the-art prediction performances compared with existing trajectory-prediction methods.

## Environment Generation for Zero-Shot Compositional Reinforcement Learning

### TL;DR

Adversarial environment generation for zero-shot compositional reinforcement learning tasks.

### Abstract

Many real-world problems are compositional -- solving them requires completing interdependent sub-tasks, either in series or in parallel, that can be represented as a dependency graph. Deep reinforcement learning (RL) agents often struggle to learn such complex tasks due to the long time horizons and sparse rewards. To address this problem, we present Compositional Design of Environments (CoDE), which trains a Generator agent to automatically build a series of compositional tasks tailored to the RL agent's current skill level. This curriculum not only enables the agent to learn more complex tasks than it could have otherwise, but also selects tasks where the agent's performance is weak, enhancing its robustness and ability to generalize zero-shot to unseen tasks at test-time. We analyze why current environment generation techniques are insufficient for the problem of generating compositional tasks, and propose a new algorithm that addresses these issues. Our results assess learning and generalization across multiple compositional tasks, including the real-world problem of learning to navigate and interact with web pages. We learn how to generate environments composed of multiple pages or rooms, and train RL agents to capable of completing wide-range of complex tasks involving both manipulation and navigation across the pages and rooms. We contribute two new benchmark frameworks for generating compositional tasks, compositional MiniGrid and gMiniWoB for web navigation. CoDE yields 4x higher success rate than the strongest baseline, and demonstrates strong performance of real websites.

## You are caught stealing my winning lottery ticket! Making a lottery ticket claim its ownership

### TL;DR

A topology-based ownership verification mechanism that can prevent lottery-ticket theft under various verification schemes and attacks

### Abstract

Despite tremendous success in many application scenarios, the training and inference costs of using deep learning are also rapidly increasing over time. The lottery ticket hypothesis emerges as a promising framework to leverage a special sparse subnetwork (i.e., $\textit{winning ticket}$) instead of a full model for both training and inference, that can lower both costs without scarifying the performance. The main resource bottleneck of LTH is however the extraordinary cost to find the sparse mask of the winning ticket. That makes the found winning ticket become a valuable asset to the owners, highlighting the necessity of protecting its copyright. Our setting adds a new dimension to the recently soaring interest in protecting against the intellectual property (IP) infringement of deep models and verifying their ownerships since they take owners' resources to develop or train. While existing methods explored encrypted weights or predictions, we investigate a unique way to leverage sparse topological information to perform $\textit{lottery verification}$, by developing several graph-based signatures that can be embedded as credentials. By further combining trigger set-based methods, our proposal can work in both white-box and black-box verification scenarios. Through extensive experiments, we verify the effectiveness of lottery verification in diverse models (ResNet-20s, ResNet-18, ResNet-50) on CIFAR-10 and CIFAR-100. Specifically, our verification is shown to be robust to removal attacks such as model fine-tuning and pruning, as well as several ambiguity attacks. All codes are included in our supplement.

## Phase transitions in when feedback is useful

### TL;DR

We offer a theory of brain inference that reveals phase transitions in whether feedback provides any utility in light of energetic costs and noise constraints.

### Abstract

Sensory observations about the world are invariably ambiguous. Inference about the world's latent variables is thus an important computation for the brain. However, computational constraints limit the performance of these computations. These constraints include energetic costs for neural activity and noise for every channel. Efficient coding is a prominent theory that describes how limited resources can be used best. In one incarnation, this leads to a theory of predictive coding, where predictions are subtracted from signals, reducing the cost of sending something that is already known. This theory does not, however, account for the costs or noise associated with those predictions. Here we offer a theory that accounts for both feedforward and feedback costs, and noise in all computations. We formulate this inference problem as message-passing on a graph whereby feedback is viewed as a control signal aiming to maximize how well an inference tracks a target state while minimizing the costs of computation. We apply this formulation to the canonical problem of inferring the hidden scalar state of a linear dynamical system with Gaussian variability. Our theory predicts the gain of optimal predictive feedback and how it is incorporated into the inference computation. We show that there is a non-monotonic dependence of optimal feedback gain as a function of both the computational parameters and the world dynamics, and we reveal phase transitions in whether feedback provides any utility in optimal inference under computational costs.

## Tractable Regularization of Probabilistic Circuits

### TL;DR

We proposed tractable regularization techniques for Probabilistic Circuits.

### Abstract

Probabilistic Circuits (PCs) are a promising avenue for probabilistic modeling. They combine advantages of probabilistic graphical models (PGMs) with those of neural networks (NNs). Crucially, however, they are tractable probabilistic models, supporting efficient and exact computation of many probabilistic inference queries, such as marginals and MAP. Further, since PCs are structured computation graphs, they can take advantage of deep-learning-style parameter updates, which greatly improves their scalability. However, this innovation also makes PCs prone to overfitting, which has been observed in many standard benchmarks. Despite the existence of abundant regularization techniques for both PGMs and NNs, they are not effective enough when applied to PCs. Instead, we re-think regularization for PCs and propose two intuitive techniques, data softening and entropy regularization, that both take advantage of PCs' tractability and still have an efficient implementation as a computation graph. Specifically, data softening provides a principled way to add uncertainty in datasets in closed form, which implicitly regularizes PC parameters. 
To learn parameters from a softened dataset, PCs only need linear time by virtue of their tractability. In entropy regularization, the exact entropy of the distribution encoded by a PC can be regularized directly, which is again infeasible for most other density estimation models. We show that both methods consistently improve the generalization performance of a wide variety of PCs. Moreover, when paired with a simple PC structure, we achieved state-of-the-art results on 10 out of 20 standard discrete density estimation benchmarks.

## NAS-Bench-101 and NAS-Bench-201 are Easier than You Think: Finding Good Architectures with Simple Meta-Predictors

### TL;DR

A novel prediction-based NAS framework using simple architectural model-based meta-features and traditional regression models

### Abstract

Given the complex and time-consuming trial-and-error process of defining neural hyperparameters, Neural Architecture Search (NAS) emerged as a solution to automatize their design and select better architectures. Despite the progress of NAS methods, most of them still suffer from model complexity, high training times, and data sample inefficiency. In addition, the majority of popular NAS solutions do not use prior knowledge to improve the learning process or cut off costs. Although Meta-Learning (MtL) methods have provided good solutions to these problems, they have received less attention. MtL methods leverage prior knowledge from previous experiences to build new knowledge, tending to be faster and cheaper than other methods. Given these problems and the gaps in the literature, we propose a simplified prediction-based NAS framework called MbML-NAS, which uses traditional regression models and model-based characterization from neural architectures to estimate predictive performances and select good models. The experiments carried out compare basic models such as Linear Regression with a more complex Neural Predictor composed of a Graph Convolutional Network and an Oracle baseline. As result, we verified that our simplified approach was more efficient in finding architectures with better predictive performance than Neural Predictor on NAS-Bench-101. Furthermore, using only $0.04\%$ and $1.1\%$ from the search spaces of NAS-Bench-101 and NAS-Bench-201 for training, we achieve better or comparable predictive performance with the Oracle baseline in validation and test accuracy. The code and models used are available at https://anonymous.4open.science/r/MbML-NAS-3F9F.

## Modeling Network Interference for Individual Treatment Effect Estimation

### TL;DR

A paper that proposes a ITE estimation method under interference from networked observational data

### Abstract

With the rapid development of big data and information science, a large amount of observational data can be easily accessed from many high-impact domains, such as economics, medicine, e-commerce, and many more. An important research problem that attracts great interests from observational data is the investigation of the causal relationship between a treatment and an outcome for each individual, and the problem is often referred to as individual treatment effect (ITE) estimation. A vast majority of existing studies of ITE estimation assume that different units at play are independent and do not influence each other. However, many social science experiments have shown that there often exist different levels of interactions between units in observational data, especially in a networked environment. As a result, the treatment assignment of one unit can affect the outcome of another unit which is connected to it in the network. This phenomenon is referred to as the interference or spillover effect. In this paper, we make an initial investigation of the ITE estimation problem from networked observational data by modeling the interference between different units and provide a theoretical identification analysis to support such study. Methodologically, we propose a novel framework SPNet that first captures the influence of hidden confounders by learning two partial representations with a twin-channel graph convolution neural network, and then leverages these two partial representations for interference modeling with the aid of a masked attention mechanism. Experimental evaluations on semi-synthetic datasets corroborate the superiority of our proposed framework over the state-of-the-art ITE estimation models.

## Differentially Private Densest Subgraph

### TL;DR

None

### Abstract

Given a graph, the densest subgraph problem asks for a set of vertices such that the average degree  among these vertices is maximized. Densest subgraph has numerous applications in learning, e.g., community detection in social networks, link spam detection, correlation mining, bioinformatics, and so on. Although there are efficient algorithms that output either exact or approximate solutions  to the densest subgraph problem, existing algorithms may violate the privacy of the individuals in the network, e.g., leaking the existence/non-existence of edges.

In this paper, we study the densest subgraph problem in the framework of the differential privacy, and we derive the first upper and lower bounds for this problem. We show that there exists a linear-time epsilon-differentially private algorithm that finds a 2-approximation of the densest subgraph with an extra poly-logarithmic additive error.  Our algorithm not only reports the approximate density of the densest subgraph, but also reports the vertices that form the dense subgraph.

Our upper bound almost matches the famous 2-approximation by Charikar both in performance  and in approximation ratio, but we additionally achieve differential privacy. In comparison with Charikar's algorithm, our algorithm has an extra poly-logarithmic additive error. We partly justify the additive error with a new lower bound, showing that  for any differentially private algorithm 
that provides a constant-factor approximation, a  sub-logarithmic additive error is inherent. 

We also practically study our differentially private algorithm on real-world graphs, and we show that in practice the algorithm finds a solution which is very close to the optimal.

## Low-Rank Matrix Completion Using Ramanujan Graphs and Applications to Sample-Efficient Reinforcement Learning

### TL;DR

None

### Abstract

In a paper published in NeurIPS20 by Shah et al.,
the authors propose to learn a $Q$-function on compact state and action spaces, by (i) approximating the optimal
action-value function $Q^*$ by a low-rank function, and (ii)
learning the low-rank approximation via solving a matrix completion problem
using random sampling.
In the present paper, we follow up on this approach by making three specific
contributions:
First, we replace the random sampling approach in
Shah et al. by a deterministic approach proposed by Burnwal and Vidyasagar.
Specifically, we propose to choose the state-action pairs to be
sampled using the edge set of a Ramanujan graph.
This \textit{does not} increase the sample complexity, but makes the
conclusions deterministic.
Second, for the problem of matrix recovery under measurement noise,
we derive bounds on the vector $\ell_\infty$-norm of the residual error,
thereby obtaining error bounds that are uniform
for each state-action pair.
Previously, such bounds were available only for the Frobenius norm
of the recovery error matrix.
Third, though the theoretical results require the number of state-action
pairs to be sampled to be $O(r^3 \max\{n,m\})$ where $r$ is the rank of the
approximate $Q$ function and $n,m$ are the number of states and actions
respectively,
it is shown through numerical simulations
that in fact $O(r \max \{ n,m\})$ samples suffice.
Taken together, these three contributions make the problem of $Q$-learning
quite amenable to sample-efficient learning.


## Geometric Attention Networks for Small Point Clouds

### TL;DR

We use geometric algebra and attention to build deep learning architectures with rotation and permutation equivariance suitable for many applications in physics, chemistry, and biology.

### Abstract

Much of the success of deep learning is drawn from building architectures that properly respect underlying symmetry and structure in the data on which they operate—a set of considerations that have been united under the banner of geometric deep learning. Often problems in the physical sciences deal with relatively small sets of points in two- or three-dimensional space wherein translation, rotation, and permutation equivariance are important or even vital for models to be useful in practice. In this work, we present an architecture for deep learning on these small point clouds with rotation and permutation equivariance, composed of a set of products of terms from the geometric algebra and reductions over those products using an attention mechanism. The geometric algebra provides valuable mathematical structure by which to combine vector, scalar, and other types of geometric inputs in a systematic way to account for rotation invariance or covariance, while attention yields a powerful way to impose permutation equivariance. We demonstrate the usefulness of these architectures by training models to solve sample problems relevant to physics, chemistry, and biology.

## Explainable Molecular Analysis based on Counterfactual Generative Reasoning

### TL;DR

We present a molecular classifier and generator model that uses counterfactuals to aid in explanations and training. 

### Abstract

Molecule  generation  and  molecular  property  prediction  are  important  tasks  in molecular analysis and drug discovery.  Developing machine learning models to generate targeted molecules and predict their properties allow us to explore the otherwise impossible large space of molecular structures. However, most machine learning models for molecule generation and prediction—especially recent deep learning models—are hardly explainable,  making it difficult to understand why certain molecule structures are generated and why the model believes they present certain physicochemical properties.  Understanding the why behind the machine learning models is extremely important in science discovery such as molecular analysis, because good explanations can greatly help scientists to understand how the machine learning model works and thus to derive scientific insights,  which benefit both scientific advances and model refinement. In  this  paper,  we  present  Explainable  Molecular  Analysis  (XMOL)  based  on Counterfactual Generative Reasoning, which consists of two components:  a predictor and an explainer.  The predictor is based on Graph Neural Network which makes prediction about the property of a molecule structure, while the explainer is a domain knowledge-driven generator which generates counterfactual molecule structures  to  alter  the  prediction  result.   A  Reinforcement  Learning  agent  is jointly optimized over the predictor and the explainer to find the minimal change in molecule structure that leads to altered prediction results,  and such minimal change serves as the counterfactual explanation.  Based on domain knowledge- driven  generation,  our  model  ensures  100%  chemical  validity  of  the  generated molecules.   Through our experiments,  we show that our model is able to optimize both the accuracy in molecular property prediction and the explanation of the prediction results, which are commonly considered as two separate tasks.

## Towards a Unified Information-Theoretic Framework for Generalization

### TL;DR

We show that the CMI framework can be used to obtain optimal or near-optimal bounds for the expected excess risk for a wide range of algorithms.

### Abstract

In this work, we investigate the expressiveness of the "conditional mutual information" (CMI)  framework of Steinke and Zakynthinou (2020) and the prospect of using it to provide a unified framework for proving generalization bounds in realizable.  We first demonstrate that one can use this framework to express non-trivial (but sub-optimal) bounds for any learning algorithm that outputs hypotheses from a class of bounded VC dimension.  We then explore two directions of strengthening this bound: (i) Can the CMI framework express optimal bounds for VC classes? (ii) Can the CMI framework be used to analyze algorithms whose output hypothesis space is unrestricted (i.e. has an unbounded VC dimension)?
    
With respect to Item (i) we prove that the CMI framework yields the optimal bound on the expected risk  of Support Vector Machines (SVMs) for learning halfspaces. This result is an application of our general result showing that stable compression schemes Bousquetet al. (2020) of size $k$ have uniformly bounded CMI of order $O(k)$. We further show that an inherent limitation of proper learning of VC classes contradicts the existence of a proper learner with constant CMI, and it implies a negative resolution to an open problem of Steinke and Zakynthinou (2020).  We further study the CMI of empirical risk minimizers (ERMs) of class $H$ and show that it is possible to output all  consistent classifiers (version space) with bounded CMI if and only if $H$ has a bounded star number (Hanneke and Yang (2015)). With respect to Item (ii) we prove a general reduction showing that "leave-one-out" analysis is expressible via the CMI framework. As a corollary we investigate the CMI of the one-inclusion-graph algorithm proposed by Haussler et al. (1994). 

## Locally Invariant Explanations: Towards Causal Explanations through Local Invariant Learning

### TL;DR

Stable local explanation method to explain black-box models

### Abstract

 Locally interpretable model agnostic explanations (LIME) method is one of the most popular methods used to explain black-box models at a per example level. Although many variants have been proposed few provide a simple way to produce high fidelity explanations that are also stable and intuitive in the neighborhood of the example. In this work, we provide a novel perspective by proposing a model agnostic local explanation method inspired by the invariant risk minimization (IRM) principle -- originally proposed for (global) out-of-distribution generalization -- to provide high fidelity explanations that are robust across neighborhoods or also for near by examples. Our method is based on a game theoretic formulation 
 where we theoretically show that our approach has a strong tendency to eliminate features where the gradient of the black-box function abruptly changes sign in the locality of the example we want to explain, while in other cases it is more careful and will choose a more conservative (feature) attribution, a behavior which can be highly desirable for recourse. Empirically, we show on tabular, image and text data that the quality of our explanations with neighborhoods formed using random perturbations are much better than LIME and in some cases even comparable to other methods that use realistic neighbors sampled from the data manifold, where the latter is a popular strategy to obtain high quality explanations. This is a desirable property given that learning a manifold to either create realistic neighbors or to project explanations is typically expensive or may even be impossible. Moreover, our algorithm is simple and efficient to train, and can ascertain stable input features for local decisions of a black-box without access to side information such as a (partial) causal graph as has been seen in some recent works.

## A Rate-Distortion Approach to Domain Generalization

### TL;DR

We re-formulate domain generalization problem as optimizing the domain invariance of gradients with the constraint that they still find satisfactory solutions. This re-formulation turns out to be equivalent to the rate-distortion optimization.

### Abstract

Given data from diverse sets of distinct distributions, the goal of domain generalization is learning models that generalize to unseen distributions. The typical approach to tackle domain generalization is designing a data-driven surrogate objective to capture invariance and minimizing the empirical risk under the data-driven constraint. Although this approach is technically sound, a data-driven definition of invariance fails to capture true invariance when the number of domains is limited. In this typical case, an inaccurate data-driven objective hurts out-of-domain generalization as well as within-domain performance. In this paper, we challenge this approach and propose to optimize invariance under the constraint that empirical risk is guaranteed to be optimized. In other words, when the invariance objective and the empirical risk conflict, we trust the empirical risk since its effective sample size is the number of data points which is much higher than the number of domains. To operationalize this approach, we first conduct an analysis of SGD with biased gradients and characterize the space of gradients which guarantee convergence to the non-stationary point of the empirical risk. Furthermore, we optimize the invariance objective to find the most domain-invariant gradients within the constraint of eventual convergence. When the invariance objective is the mutual information between domains and the gradients, we show that this approach is equivalent to the rate-distortion problem from information theory. Thanks to this equivalence, we use rate-distortion theory to understand domain generalization better. We adapt the Blahut-Arimoto algorithm (Blahut, 1972; Arimoto, 1972) and develop a tractable version of it for large-scale models. We perform a diverse empirical study in the WILDS benchmark using outdoor images, satellite images, medical images, natural language, formal languages, and graph-structured data (Koh et al., 2020). The results suggest that our method outperforms prior domain generalization methods.

## A/B Testing for Recommender Systems in a Two-sided Marketplace

### TL;DR

We propose a novel A/B testing framework for seller or producer side measurements in a two-sided marketplace.

### Abstract

Two-sided marketplaces are standard business models of many online platforms (e.g., Amazon, Facebook, LinkedIn), wherein the platforms have consumers, buyers or content viewers on one side and producers, sellers or content-creators on the other. Consumer side measurement of the impact of a treatment variant can be done via simple online A/B testing. \textit{Producer side measurement is more challenging because the producer experience depends on the treatment assignment of the consumers}. Existing approaches for producer side measurement are either based on graph cluster-based randomization or on certain treatment propagation assumptions. The former approach results in low-powered experiments as the producer-consumer network density increases and the latter approach lacks a strict notion of error control. In this paper, we propose (i) a quantification of the quality of a producer side experiment, and (ii) a new experiment design mechanism that generates high quality experiments based on this quantification. Our approach, called UniCoRn ({Uni}fying {Co}unterfactual {R}a{n}kings), provides explicit control over the quality of the experiment and its computation cost. Further, we prove that our experiment design is optimal. Our approach is agnostic to the density of the producer-consumer network and does not rely on any treatment propagation assumption. Moreover, unlike the existing approaches, we do not need to know the underlying network in advance, making this widely applicable to the industrial setting where the underlying network is unknown and challenging to predict a priori due to its dynamic nature. We use simulations to thoroughly validate our approach and compare it against existing methods. We also implement UniCoRn in an edge recommendation application that serves tens of millions of members and billions of edge recommendations daily.

## Context-Hierarchy Inverse Reinforcement Learning

### TL;DR

CHIRL is a new IRL algorithm that exploits the context hierarchy to scale up IRL and learn reward functions of complex behaviors.

### Abstract

An inverse reinforcement learning (IRL) agent learns to act intelligently by observing expert demonstrations and learning the expert’s underlying reward function. Although learning the reward functions from demonstrations has achieved great success in various tasks, several other challenges are mostly ignored. Firstly, existing IRL methods try to learn the reward function from scratch without relying on any prior knowledge. Secondly, traditional IRL methods assume the reward functions are homogeneous across all the demonstrations. Some existing IRLmethods managed to extend to the heterogeneous demonstrations. However, they still assume one hidden variable that affects the behavior and learns the underlying hidden variable together with the reward from demonstrations. They ignore all the causal dependencies over different factors. To solve these issues, we present context-HierarchyIRL(CHIRL), a new IRL algorithm that exploits the context to scale up IRL and learn reward functions of complex behaviors. CHIRL models the context hierarchically as a directed acyclic graph; it represents the reward function as a corresponding modular deep neural network that associates each network module with a node of the context hierarchy. The context hierarchy and the modular reward representation enable data sharing across multiple contexts and context-specific state abstraction, significantly improving the learning performance. CHIRL has a natural connection with hierarchical task planning when the context hierarchy represents subtask decomposition. It enables to incorporate the prior knowledge of causal dependencies of subtasks and make it capable of solving large complex tasks by decoupling it into several subtasks and conquer each subtask to solve the original task. Experiments on benchmark tasks, including a large-scale autonomous driving task in the CARLA simulator, show promising results in scaling up IRL for tasks with complex reward functions.

## Coarse-to-fine Animal Pose and Shape Estimation

### TL;DR

A coarse-to-fine 3D animal pose and shape estimation, which combines the SMAL-based representation in the first stage and vertex-based representation with an encoder-decoder structured GCN in the second stage.

### Abstract

Most existing animal pose and shape estimation approaches reconstruct animal meshes with a parametric SMAL model. This is because the low-dimensional pose and shape parameters of the SMAL model makes it easier for deep networks to learn the high-dimensional animal meshes. However, the SMAL model is obtained from scans of toy animals with limited pose and shape variations, and thus may not be able to represent highly varying real animals well. This can result in ill-fittings of the estimated mesh to the 2D evidences, e.g. 2D keypoints or silhouettes.  To mitigate this problem, we propose a coarse-to-fine approach to reconstruct 3D animal mesh from a single image. The coarse estimation stage first estimates the pose, shape and translation parameters of the SMAL model. The estimated meshes are then used as a starting point by a graph convolutional network (GCN) to predict a per-vertex deformation in the refinement stage. This combination of SMAL-based and vertex-based representations benefits from both parametric and non-parametric representations. We design our mesh refinement GCN (MRGCN) as an encoder-decoder structure with hierarchical feature representation to overcome the limited receptive field of traditional GCNs.
Moreover, we observe that the global image feature used by all existing animal mesh reconstruction works is unable to capture detailed shape information for mesh refinement. We thus introduce a local feature extractor to retrieve a vertex-level feature and use it together with the global feature as the input of the MRGCN. We test our approach on the StanfordExtra dataset and achieve state-of-the-art results. Furthermore, we test the generalization capacity of our approach on the Animal Pose and BADJA datasets. Our code will be made available for research purpose upon acceptance of this paper.

## Cascade MPN: Cascade Moment Proposal Network for Video Corpus Moment Retrieval

### TL;DR

We propose video moment retrieval system referred to as Cascade Moment Proposal Network, which understands high-level sequential contextual semantics in video founded on our defined recursive algorithm.

### Abstract

This paper considers a video moment retrieval system referred to as Cascaded Moment Proposal Network (Cascaded MPN) for localizing temporal moments corresponding to natural language query in a large video corpus. Previous moment retrieval systems are largely grouped into two categories: (1) anchor-based method which presets a series of video segment proposals (e.g., via sliding window) and predicts proposal best matched with the query, and (2) anchor-free method which directly predicts frame-level start time and end time of the moment related to the query (e.g., via regression or probabilistic estimation). Although recent efforts have made great stride for moment retrieval with anchor-based or anchor-free method, still inherent weaknesses of each method remain challenging: (1) anchor-based method is vulnerable to heuristic rules of generating video proposals, which causes restrictive moment prediction in variant length; and (2) anchor-free method, as is based on frame-level interplay, suffers from insufficient understanding of long and sequential video context. To overcome aforementioned challenges, our proposed Cascaded MPN incorporates following two main properties: (1) Hierarchical Contextual Semantic (HCS) which comprehends high-level semantics under consecutive frames via building video-subtitle graph and reasoning contextual semantics; and (2) Cascaded MPN algorithm which performs recursive refinement on frame-level moment retrieval via progressively associating contextual semantics with the frame-level semantics. We also design sparsity pooling that enables the retrieval system to explore diverse candidate moments by turning locally biased moment prediction score distribution to the sporadic in every stage of Cascaded MPN. Extensive experiments on three moment retrieval benchmarks (i.e., TVR, ActivityNet, DiDeMo) show that Cascaded MPN outperforms previous state-of-the-arts, while qualitative analysis shows improved interpretability. The code will be made publicly available.

## YACC: A Framework Generalizing Tur\'anShadow for Counting Large Cliques

### TL;DR

A new algorithm for $k$-clique-counting based on the Tur\'anShadow algorithm that works for large $k$ (all existing algorithms fail)

### Abstract

Clique-counting is a fundamental problem that has application in many areas eg. dense subgraph discovery, community detection, spam detection, etc. The problem of $k$-clique-counting is difficult because as $k$ increases, the number of $k$-cliques goes up exponentially. Enumeration algorithms (even parallel ones) fail to count $k$-cliques beyond a small $k$. Approximation algorithms, like Tur\'anShadow have been shown to perform well upto $k=10$, but are inefficient for larger cliques. The recently proposed Pivoter algorithm significantly improved the state-of-the-art and was able to give exact counts of all $k$-cliques in a large number of graphs.  However, as the authors of Pivoter point out, the clique counts of some graphs like \texttt{com-lj} are still out of reach of these algorithms. We revisit the Tur\'anShadow algorithm and propose a generalized framework called YACC that leverages several insights about real-world graphs to achieve faster clique-counting. The bottleneck in Tur\'anShadow is a recursive subroutine whose stopping condition is based on a classic result from extremal combinatorics called Tur\'an's theorem. This theorem gives a lower bound for the $k$-clique density in a subgraph in terms of its edge density. However, this stopping condition is based on a worst-case graph that does not reflect the nature of real-world graphs. Using techniques for quickly discovering dense subgraphs, we relax the stopping condition in a systematic way resulting in a smaller recursion tree while still maintaining the guarantees provided by Tur\'anShadow. We further strengthen the stopping condition using wedge densities (in addition to edge densities). We deploy our algorithm on several real-world data sets and show that YACC reduces the size of the recursion tree and the running time by over an order of magnitude. Using YACC, we are able to obtain clique counts for several graphs for which clique-counting was infeasible before, including \texttt{com-lj}.


##  Neural Optimization Kernel:   Towards  Robust Deep Learning

### TL;DR

We propose a novel neural optimization kernel family and show its connection to deep neural networks. We analyze its NN architecture optimization property and  prove the generalization bounds 

### Abstract

Recent studies show a close connection between neural networks (NN) and kernel methods.  However, most of these analyses   (e.g., NTK~\citep{jacot2018neural}) focus on the influence of  (infinite) width instead of the depth of  NN models.    There remains a gap between theory and practical network designs that benefit from the depth.  This paper first proposes a novel kernel family named Neural Optimization Kernel (NOK). Our (T-layer) kernel is defined as the inner product between two $T$-step updated functionals in RKHS w.r.t. a regularized optimization problem. Theoretically,  we proved the monotonic descent property of our update rule for both convex and non-convex problems,  and a $O(1/T)$ convergence rate of our updates for convex problems.  Moreover, we propose a data-dependent structured approximation of our NOK, which builds the connection between training deep NNs and kernel methods associated with NOK.  The resultant computational graph is a ResNet-type finite width NN.  Our structured approximation preserved the monotonic descent property and  $O(1/T)$ convergence rate. Namely, a $T$-layer NN performs $T$-step monotonic descent updates. Notably,  we show our $T$-layered structured NN with ReLU maintains a   $O(1/T)$ convergence rate w.r.t. a convex regularized problem, which explains the success of ReLU on training deep NN from a NN architecture optimization perspective.   For the unsupervised learning and the shared parameter case,  we show the equivalence of training structured NN with GD and performing functional gradient descent in RKHS associated with a fixed (data-dependent) NOK  at an infinity-width regime.    For finite NOKs, we prove generalization bounds.  Remarkably, we show that overparameterized deep NN (NOK) can increase the expressive power to reduce empirical risk and reduce the generalization bound at the same time.   Empirically,   our structured approximation can serve as a simple plug-in for popular backbones.   Experiments on cifar10 and cifar100 datasets with  ResNet and DenseNet backbones verify the robustness of our structured approximated NOK blocks against two types of input noise and FGSM adversarial attack. 

## Rethinking the Spatial Route Prior in Vision-and-Language Navigation

### TL;DR

None

### Abstract

Vision-and-language navigation (VLN) is a trending topic which aims to navigate an intelligent agent to an expected position through natural language instructions. A variety of methods have been developed to tackle the problem, particularly reinforcement learning (RL) based ones. This work addresses the task of VLN from a previously-ignored aspect, namely the spatial route prior of the navigation scenes. For example, a ground-truth route is always direct-to-goal shortest-path in Room-to-Room (R2R) [3], a concatenation of multiple local shortest-paths in Room-for-Room (R4R) [15], or near-optimal route calculated by Google Map APIs in TOUCHDOWN [4]. A critically enabling innovation of this work is explicitly considering the spatial route prior under several different VLN settings. In a most information-rich case of knowing environment maps and admitting shortest-path prior, we observe that given an origin-destination node pair, the internal route can be uniquely determined. Thus, VLN can be effectively formulated as an ordinary classification problem over all possible destination nodes in the scenes. This empirically leads to more accurate predictions compared with RL-based solutions. Furthermore, we relax it to other more general VLN settings, proposing a sequential-decision variant (by abandoning the shortest-path route prior) and an explore-and-exploit scheme (for addressing the case of not knowing the environment maps) that curates a compact and informative sub-graph to exploit. As reported by [33], the performance of VLN methods has been stuck at a plateau in past two years. Even with increased model complexity, the state-of-the-art success rate on R2R validation-unseen set has stayed around 62% (for single-model) and 73% (with beam-search and ensemble). We have conducted comprehensive evaluations on both R2R and R4R, and surprisingly find that utilizing the spatial route priors may be the key of breaking above-mentioned performance ceiling. For example, on R2R validation-unseen set, when the number of discrete nodes explored is about 40, our single-model success rate reaches 73%, and increases to 78% if a Speaker model is ensembled, which significantly outstrips previous state-of-the-art VLN-BERT with ensemble and clearly re-calibrates the top records in VLN.
